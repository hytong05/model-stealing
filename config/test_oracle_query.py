#!/usr/bin/env python3
"""
Script test module truy v·∫•n target model.

Script n√†y:
1. Load features v√† ground truth labels t·ª´ parquet file
2. Truy v·∫•n target model ƒë·ªÉ l·∫•y predictions
3. So s√°nh predictions v·ªõi ground truth
4. T√≠nh c√°c metrics (accuracy, precision, recall, F1, confusion matrix)
5. L∆∞u k·∫øt qu·∫£ v√†o file
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.targets.oracle_client import LocalOracleClient, create_oracle_from_name
from sklearn.preprocessing import RobustScaler
from pathlib import Path


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """L·∫•y danh s√°ch feature columns t·ª´ parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_data_with_labels(
    parquet_path: str,
    feature_cols: list,
    label_col: str = "Label",
    max_samples: Optional[int] = None,
    batch_size: int = 10000,
) -> tuple[np.ndarray, np.ndarray]:
    """
    Load features v√† ground truth labels t·ª´ parquet file.
    Lo·∫°i b·ªè c√°c samples c√≥ label = -1 (unlabeled).
    """
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    all_y = []
    rows_processed = 0

    print(f"üìÇ ƒêang load d·ªØ li·ªáu t·ª´ {parquet_path}...")
    for batch in pq_file.iter_batches(batch_size=batch_size, columns=feature_cols + [label_col]):
        if max_samples and rows_processed >= max_samples:
            break

        batch_df = batch.to_pandas()

        # L·∫•y labels
        if label_col in batch_df.columns:
            labels = batch_df[label_col].values
        else:
            alt_cols = [col for col in batch_df.columns if col.lower() == label_col.lower()]
            if alt_cols:
                labels = batch_df[alt_cols[0]].values
            else:
                raise KeyError(f"Label column '{label_col}' kh√¥ng t·ªìn t·∫°i trong batch.")

        # Lo·∫°i b·ªè label -1 (unlabeled)
        valid_mask = labels != -1
        if not np.any(valid_mask):
            continue

        batch_X = batch_df[feature_cols].values.astype(np.float32)[valid_mask]
        batch_y = labels[valid_mask].astype(np.int32)

        # X·ª≠ l√Ω NaN/Inf
        batch_X = np.nan_to_num(batch_X, nan=0.0, posinf=0.0, neginf=0.0)

        all_X.append(batch_X)
        all_y.append(batch_y)
        rows_processed += len(batch_y)

        if max_samples and rows_processed >= max_samples:
            # C·∫Øt b·ªõt batch cu·ªëi n·∫øu c·∫ßn
            excess = rows_processed - max_samples
            if excess > 0:
                all_X[-1] = all_X[-1][:-excess]
                all_y[-1] = all_y[-1][:-excess]
            break

    X = np.vstack(all_X) if all_X else np.array([]).reshape(0, len(feature_cols))
    y = np.concatenate(all_y) if all_y else np.array([], dtype=np.int32)

    print(f"‚úÖ ƒê√£ load {len(y):,} samples (ƒë√£ lo·∫°i b·ªè label -1)")
    print(f"   Features shape: {X.shape}")
    print(f"   Labels distribution: {dict(zip(*np.unique(y, return_counts=True)))}")

    return X, y


def load_normalization_stats(normalization_stats_path: Optional[str], model_path: Path) -> tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Load normalization stats t·ª´ file .npz.
    T·ª± ƒë·ªông t√¨m file n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.
    """
    if normalization_stats_path:
        stats_path = Path(normalization_stats_path).expanduser().resolve()
    else:
        # T·ª± ƒë·ªông t√¨m file normalization_stats.npz trong c√πng th∆∞ m·ª•c v·ªõi model
        model_dir = model_path.parent
        model_name = model_path.stem
        possible_paths = [
            model_dir / f"{model_name}.npz",
            model_dir / f"{model_name}_normalization_stats.npz",
            model_dir / "normalization_stats.npz",
        ]
        stats_path = None
        for path in possible_paths:
            if path.exists():
                stats_path = path
                break
    
    if stats_path is None or not stats_path.exists():
        return None, None
    
    print(f"   üìÇ ƒêang load normalization stats t·ª´ {stats_path.name}...")
    stats = np.load(stats_path, allow_pickle=True)
    feature_means = stats.get("feature_means")
    feature_stds = stats.get("feature_stds")
    
    if feature_means is None or feature_stds is None:
        print(f"   ‚ö†Ô∏è  File kh√¥ng ch·ª©a feature_means ho·∫∑c feature_stds")
        return None, None
    
    print(f"   ‚úÖ ƒê√£ load normalization stats: {feature_means.shape[0]} features")
    return feature_means, feature_stds


def normalize_features(X: np.ndarray, feature_means: np.ndarray, feature_stds: np.ndarray) -> np.ndarray:
    """
    Normalize features gi·ªëng nh∆∞ trong notebook CEE.ipynb:
    X = (X - feature_means) / feature_stds
    """
    # ƒê·∫£m b·∫£o feature_means v√† feature_stds c√≥ c√πng s·ªë features v·ªõi X
    if feature_means.shape[0] > X.shape[1]:
        # C·∫Øt b·ªè features th·ª´a
        feature_means = feature_means[:X.shape[1]]
        feature_stds = feature_stds[:X.shape[1]]
    elif feature_means.shape[0] < X.shape[1]:
        # C·∫Øt b·ªè features th·ª´a t·ª´ X
        X = X[:, :feature_means.shape[0]]
    
    # Normalize: (X - mean) / std
    X_normalized = (X - feature_means) / feature_stds
    
    # X·ª≠ l√Ω NaN/Inf gi·ªëng nh∆∞ trong notebook
    X_normalized = np.nan_to_num(X_normalized, nan=0.0, posinf=0.0, neginf=0.0)
    
    return X_normalized


def query_oracle(
    oracle_client: LocalOracleClient,
    X: np.ndarray,
    model_type: str,
    model_path: Path,
    normalization_stats_path: Optional[str] = None,
    batch_size: int = 1024,
) -> np.ndarray:
    """
    Truy v·∫•n oracle ƒë·ªÉ l·∫•y predictions.
    
    QUAN TR·ªåNG: Oracle client ƒë√£ t·ª± ƒë·ªông x·ª≠ l√Ω normalization v√† feature alignment.
    Attacker ch·ªâ c·∫ßn g·ª≠i raw features, oracle s·∫Ω t·ª± ƒë·ªông:
    - Normalize features (n·∫øu c√≥ normalization stats)
    - Align feature dimensions
    - Tr·∫£ v·ªÅ binary predictions (0 ho·∫∑c 1)
    """
    print(f"\nüöÄ ƒêang truy v·∫•n oracle module...")
    print(f"   ‚ÑπÔ∏è  Oracle client s·∫Ω t·ª± ƒë·ªông x·ª≠ l√Ω normalization v√† feature alignment")

    # Query oracle theo batch - oracle client t·ª± ƒë·ªông x·ª≠ l√Ω m·ªçi th·ª©
    num_samples = X.shape[0]
    predictions = np.zeros(num_samples, dtype=np.int32)

    for start in range(0, num_samples, batch_size):
        end = min(start + batch_size, num_samples)
        batch = X[start:end]
        predictions[start:end] = oracle_client.predict(batch)
        if (start // batch_size) % 50 == 0 or end == num_samples:
            print(f"   ‚Ä¶ processed {end:,}/{num_samples:,} ({end/num_samples*100:.1f}%)")

    return predictions


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """T√≠nh c√°c metrics ƒë√°nh gi√°."""
    accuracy = accuracy_score(y_true, y_pred)
    balanced_acc = balanced_accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)

    # T√≠nh AUC n·∫øu c√≥ th·ªÉ (c·∫ßn probabilities, nh∆∞ng oracle ch·ªâ tr·∫£ v·ªÅ binary)
    # V·ªõi binary predictions, kh√¥ng th·ªÉ t√≠nh AUC ch√≠nh x√°c
    try:
        # Th·ª≠ l·∫•y probabilities n·∫øu oracle h·ªó tr·ª£
        auc = None
    except:
        auc = None

    # Chuy·ªÉn ƒë·ªïi class distribution sang Python native types
    true_dist = np.unique(y_true, return_counts=True)
    pred_dist = np.unique(y_pred, return_counts=True)
    class_dist_true = {int(k): int(v) for k, v in zip(true_dist[0], true_dist[1])}
    class_dist_pred = {int(k): int(v) for k, v in zip(pred_dist[0], pred_dist[1])}

    metrics = {
        "accuracy": float(accuracy),
        "balanced_accuracy": float(balanced_acc),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": {
            "true_negative": int(tn),
            "false_positive": int(fp),
            "false_negative": int(fn),
            "true_positive": int(tp),
        },
        "total_samples": int(len(y_true)),
        "class_distribution_true": class_dist_true,
        "class_distribution_pred": class_dist_pred,
    }

    return metrics


def main():
    parser = argparse.ArgumentParser(
        description="Test module truy v·∫•n target model v√† so s√°nh v·ªõi ground truth"
    )
    parser.add_argument(
        "--parquet-path",
        type=str,
        required=True,
        help="ƒê∆∞·ªùng d·∫´n t·ªõi parquet file c√≥ features v√† labels",
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="ƒê∆∞·ªùng d·∫´n t·ªõi target model (h5 ho·∫∑c lgb). N·∫øu kh√¥ng cung c·∫•p, s·∫Ω d√πng --model-name.",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default=None,
        help="T√™n model (CEE, LEE, CSE, LSE). N·∫øu cung c·∫•p, s·∫Ω t·ª± ƒë·ªông t√¨m model v√† detect type.",
    )
    parser.add_argument(
        "--model-type",
        type=str,
        choices=["h5", "lgb"],
        default=None,
        help="Lo·∫°i model: 'h5' (Keras) ho·∫∑c 'lgb' (LightGBM). Ch·ªâ c·∫ßn n·∫øu d√πng --model-path.",
    )
    parser.add_argument(
        "--label-col",
        type=str,
        default="Label",
        help="T√™n c·ªôt label trong parquet (m·∫∑c ƒë·ªãnh: 'Label')",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Gi·ªõi h·∫°n s·ªë samples ƒë·ªÉ test (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£)",
    )
    parser.add_argument(
        "--normalization-stats-path",
        type=str,
        default=None,
        help="ƒê∆∞·ªùng d·∫´n t·ªõi file normalization_stats.npz (cho Keras/LightGBM). N·∫øu None, s·∫Ω t·ª± ƒë·ªông t√¨m trong c√πng th∆∞ m·ª•c v·ªõi model.",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Threshold binary cho oracle (m·∫∑c ƒë·ªãnh: 0.5)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh: output/test_oracle/)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1024,
        help="Batch size cho query oracle (m·∫∑c ƒë·ªãnh: 1024)",
    )

    args = parser.parse_args()

    # Validate arguments
    if args.model_name is None and args.model_path is None:
        raise ValueError("‚ùå Ph·∫£i cung c·∫•p --model-name ho·∫∑c --model-path")
    
    if args.model_name is not None and args.model_path is not None:
        raise ValueError("‚ùå Ch·ªâ cung c·∫•p --model-name HO·∫∂C --model-path, kh√¥ng ph·∫£i c·∫£ hai")
    
    if args.model_path is not None and args.model_type is None:
        raise ValueError("‚ùå Khi d√πng --model-path, ph·∫£i cung c·∫•p --model-type")

    # Resolve paths
    parquet_path = Path(args.parquet_path).expanduser().resolve()
    if not parquet_path.exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y parquet file: {parquet_path}")

    # X·ª≠ l√Ω model path/name
    use_model_name = args.model_name is not None
    if use_model_name:
        model_name = args.model_name.upper().strip()
        model_path = None  # S·∫Ω ƒë∆∞·ª£c t·∫°o t·ª´ t√™n
        model_type = None  # S·∫Ω ƒë∆∞·ª£c auto-detect
    else:
    model_path = Path(args.model_path).expanduser().resolve()
    if not model_path.exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y model file: {model_path}")
        model_type = args.model_type
        model_name = None

    # Output directory
    if args.output_dir:
        output_dir = Path(args.output_dir).expanduser()
    else:
        output_dir = PROJECT_ROOT / "output" / "test_oracle"
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("TEST MODULE TRUY V·∫§N TARGET MODEL")
    print("=" * 80)
    print(f"\nüìã C·∫•u h√¨nh:")
    print(f"   Parquet file: {parquet_path}")
    if use_model_name:
        print(f"   Model name: {model_name} (t·ª± ƒë·ªông detect type v√† path)")
    else:
    print(f"   Model file: {model_path}")
        print(f"   Model type: {model_type.upper()}")
    print(f"   Label column: {args.label_col}")
    if args.max_samples:
        print(f"   Max samples: {args.max_samples:,}")
    print(f"   Output directory: {output_dir}")

    # Load feature columns
    print(f"\nüìÇ ƒêang ƒë·ªçc feature columns...")
    feature_cols = get_feature_columns(str(parquet_path), args.label_col)
    print(f"‚úÖ T√¨m th·∫•y {len(feature_cols)} feature columns")

    # Load data v·ªõi ground truth labels
    X, y_true = load_data_with_labels(
        str(parquet_path),
        feature_cols,
        args.label_col,
        max_samples=args.max_samples,
    )

    if len(y_true) == 0:
        raise ValueError("‚ùå Kh√¥ng c√≥ samples h·ª£p l·ªá (t·∫•t c·∫£ ƒë·ªÅu c√≥ label -1)")

    # Kh·ªüi t·∫°o oracle client
    print(f"\nüîÑ ƒêang kh·ªüi t·∫°o oracle client...")
    
    if use_model_name:
        # S·ª≠ d·ª•ng t√™n model - t·ª± ƒë·ªông detect m·ªçi th·ª©
        oracle_client = create_oracle_from_name(
            model_name=model_name,
            threshold=args.threshold,
            feature_dim=X.shape[1],
        )
        # L·∫•y model_path v√† model_type t·ª´ oracle client ƒë·ªÉ hi·ªÉn th·ªã
        model_path = Path(oracle_client.model_path)
        model_type = oracle_client.model_type
    else:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n th·ªß c√¥ng
        normalization_stats_path = args.normalization_stats_path
        if normalization_stats_path is None:
            # T·ª± ƒë·ªông t√¨m file normalization_stats.npz trong c√πng th∆∞ m·ª•c v·ªõi model
            model_dir = model_path.parent
            model_name_from_path = model_path.stem
            possible_paths = [
                model_dir / f"{model_name_from_path}.npz",
                model_dir / f"{model_name_from_path}_normalization_stats.npz",
                model_dir / "normalization_stats.npz",
            ]
            for path in possible_paths:
                if path.exists():
                    normalization_stats_path = str(path)
                    print(f"üìÇ T·ª± ƒë·ªông t√¨m th·∫•y normalization stats: {path.name}")
                    break
            
            # Ch·ªâ raise error n·∫øu l√† LightGBM (b·∫Øt bu·ªôc)
            if normalization_stats_path is None and model_type == "lgb":
                raise FileNotFoundError(
                    f"‚ùå Kh√¥ng t√¨m th·∫•y file normalization stats. "
                    f"LightGBM model c·∫ßn normalization stats. "
                    f"Vui l√≤ng cung c·∫•p --normalization-stats-path ho·∫∑c ƒë·∫∑t file trong: {model_dir}"
                )
        
    oracle_client = LocalOracleClient(
            model_type=model_type,
        model_path=str(model_path),
            normalization_stats_path=normalization_stats_path,
        threshold=args.threshold,
        feature_dim=X.shape[1],
    )
    
    print(f"‚úÖ Oracle client ƒë√£ s·∫µn s√†ng")
    print(f"   Model type: {model_type.upper()}")
    print(f"   Model path: {model_path}")

    # Query oracle ƒë·ªÉ l·∫•y predictions
    y_pred = query_oracle(
        oracle_client, 
        X, 
        model_type, 
        model_path,
        normalization_stats_path=None,  # ƒê√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong oracle client
        batch_size=args.batch_size
    )

    # T√≠nh metrics
    print(f"\nüìä ƒêang t√≠nh metrics...")
    metrics = calculate_metrics(y_true, y_pred)

    # In k·∫øt qu·∫£
    print("\n" + "=" * 80)
    print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
    print("=" * 80)
    print(f"\nüìà Metrics:")
    print(f"   Accuracy:           {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"   Balanced Accuracy:  {metrics['balanced_accuracy']:.4f} ({metrics['balanced_accuracy']*100:.2f}%)")
    print(f"   Precision:          {metrics['precision']:.4f}")
    print(f"   Recall:             {metrics['recall']:.4f}")
    print(f"   F1 Score:           {metrics['f1_score']:.4f}")

    print(f"\nüìä Confusion Matrix:")
    cm = metrics["confusion_matrix"]
    print(f"   True Negative (TN):  {cm['true_negative']:,}")
    print(f"   False Positive (FP): {cm['false_positive']:,}")
    print(f"   False Negative (FN): {cm['false_negative']:,}")
    print(f"   True Positive (TP):  {cm['true_positive']:,}")

    print(f"\nüìä Class Distribution:")
    print(f"   Ground Truth: {metrics['class_distribution_true']}")
    print(f"   Predictions:  {metrics['class_distribution_pred']}")

    # Classification report
    print(f"\nüìã Classification Report:")
    print(classification_report(y_true, y_pred, target_names=["Benign", "Malware"]))

    # L∆∞u k·∫øt qu·∫£
    output_json = output_dir / "test_results.json"
    output_txt = output_dir / "test_results.txt"

    # L∆∞u JSON
    results = {
        "config": {
            "parquet_path": str(parquet_path),
            "model_path": str(model_path),
            "model_name": model_name if use_model_name else None,
            "model_type": model_type,
            "label_col": args.label_col,
            "max_samples": args.max_samples,
            "threshold": args.threshold,
        },
        "metrics": metrics,
    }

    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    # L∆∞u text report
    with open(output_txt, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("TEST MODULE TRUY V·∫§N TARGET MODEL - K·∫æT QU·∫¢\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"üìã C·∫•u h√¨nh:\n")
        f.write(f"   Parquet file: {parquet_path}\n")
        if use_model_name:
            f.write(f"   Model name: {model_name} (t·ª± ƒë·ªông detect type v√† path)\n")
        else:
        f.write(f"   Model file: {model_path}\n")
        f.write(f"   Model type: {model_type.upper()}\n")
        f.write(f"   Label column: {args.label_col}\n")
        if args.max_samples:
            f.write(f"   Max samples: {args.max_samples:,}\n")
        f.write(f"   Threshold: {args.threshold}\n")
        f.write(f"\nüìà Metrics:\n")
        f.write(f"   Accuracy:           {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\n")
        f.write(f"   Balanced Accuracy:  {metrics['balanced_accuracy']:.4f} ({metrics['balanced_accuracy']*100:.2f}%)\n")
        f.write(f"   Precision:          {metrics['precision']:.4f}\n")
        f.write(f"   Recall:             {metrics['recall']:.4f}\n")
        f.write(f"   F1 Score:           {metrics['f1_score']:.4f}\n")
        f.write(f"\nüìä Confusion Matrix:\n")
        f.write(f"   True Negative (TN):  {cm['true_negative']:,}\n")
        f.write(f"   False Positive (FP): {cm['false_positive']:,}\n")
        f.write(f"   False Negative (FN): {cm['false_negative']:,}\n")
        f.write(f"   True Positive (TP):  {cm['true_positive']:,}\n")
        f.write(f"\nüìä Class Distribution:\n")
        f.write(f"   Ground Truth: {metrics['class_distribution_true']}\n")
        f.write(f"   Predictions:  {metrics['class_distribution_pred']}\n")
        f.write(f"\nüìã Classification Report:\n")
        f.write(classification_report(y_true, y_pred, target_names=["Benign", "Malware"]))

    print(f"\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£:")
    print(f"   JSON: {output_json}")
    print(f"   Text: {output_txt}")


if __name__ == "__main__":
    main()

