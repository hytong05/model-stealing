"""
Script ƒë·ªÉ ch·∫°y extraction v·ªõi c√°c s·ªë l∆∞·ª£ng queries kh√°c nhau v√† t·∫°o report

H·ªó tr·ª£ c·∫£ target model .h5 (Keras) v√† .lgb (LightGBM)
"""
import json
import os
import sys
import argparse
from pathlib import Path
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


def _resolve_path(path_str: str) -> Path:
    """Resolve path (relative to PROJECT_ROOT n·∫øu c·∫ßn)."""
    path_obj = Path(path_str)
    if not path_obj.is_absolute():
        path_obj = PROJECT_ROOT / path_obj
    return path_obj.resolve()


def _format_template(template: str, context: dict, template_name: str) -> str:
    """Helper ƒë·ªÉ format template v√† b√°o l·ªói r√µ r√†ng n·∫øu placeholder sai."""
    try:
        return template.format(**context)
    except KeyError as exc:
        missing = exc.args[0]
        available = ", ".join(sorted(context.keys()))
        raise ValueError(
            f"Placeholder {{{missing}}} trong {template_name} kh√¥ng kh·∫£ d·ª•ng. "
            f"C√°c placeholder h·ª£p l·ªá: {available}"
        ) from exc

from scripts.attacks.extract_final_model import run_extraction

# Known compatibility matrix: model_name -> list of compatible datasets
MODEL_DATASET_COMPATIBILITY = {
    "CEE": ["ember"],
    "CSE": ["ember"],
    "LEE": ["ember"],
    "LSE": ["ember"],
    "CNN": ["ember"],
    "KNN": ["ember"],
    "XGBOOST": ["ember"],
    "XGBOOST-EMBER": ["ember"],
    "DUALFFNN": ["ember"],
    "DUALFFNN-EMBER": ["ember"],
    "TABNET": ["ember"],
    "TABNET-EMBER": ["ember"],
    # Add new models here: "LEE_SOMLAP": ["somlap"],
}

def validate_model_dataset_compatibility(model_name: str, dataset: str):
    """
    Validate compatibility between model and dataset before running attack.
    Ch·ªâ warning, kh√¥ng block v√¨ padding/truncate s·∫Ω t·ª± ƒë·ªông x·ª≠ l√Ω feature mismatch.
    
    Args:
        model_name: Name of target model (e.g., "LEE", "CEE")
        dataset: Name of attack dataset (e.g., "ember", "somlap")
    """
    model_name_upper = model_name.upper().strip()
    dataset_lower = dataset.lower().strip()
    
    if model_name_upper in MODEL_DATASET_COMPATIBILITY:
        compatible_datasets = MODEL_DATASET_COMPATIBILITY[model_name_upper]
        if dataset_lower not in [d.lower() for d in compatible_datasets]:
            compatible_str = ", ".join(compatible_datasets)
            print(f"\n‚ö†Ô∏è  WARNING: Model '{model_name}' th∆∞·ªùng ƒë∆∞·ª£c train tr√™n: {compatible_str}")
            print(f"   ƒêang s·ª≠ d·ª•ng dataset '{dataset}' - c√≥ th·ªÉ c√≥ feature mismatch")
            print(f"   üìå Padding/Truncate s·∫Ω t·ª± ƒë·ªông x·ª≠ l√Ω n·∫øu c√≥ s·ª± kh√°c bi·ªát v·ªÅ s·ªë features")
            print(f"   üí° N·∫øu mu·ªën t·ªëi ∆∞u, n√™n s·ª≠ d·ª•ng: --dataset {compatible_str}\n")
    # Unknown model - will be validated later by extract_final_model based on feature dimensions


def _create_individual_report(output_dir: Path, result: dict, config: dict):
    """
    T·∫°o report ri√™ng cho t·ª´ng config trong folder output c·ªßa config ƒë√≥
    
    Args:
        output_dir: Th∆∞ m·ª•c output c·ªßa config
        result: K·∫øt qu·∫£ c·ªßa config (ch·ª©a metrics)
        config: C·∫•u h√¨nh config (ch·ª©a description, query_batch, etc.)
    """
    # Report paths trong folder output c·ªßa config
    report_txt_path = output_dir / "extraction_report.txt"
    report_md_path = output_dir / "extraction_report.md"
    report_json_path = output_dir / "extraction_report.json"
    
    # T·∫°o report text
    with open(report_txt_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write(f"B√ÅO C√ÅO EXTRACTION ATTACK\n")
        f.write(f"C·∫•u h√¨nh: {result['config_name']}\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("TH√îNG TIN C·∫§U H√åNH:\n")
        f.write("-" * 80 + "\n")
        f.write(f"M√¥ t·∫£: {config.get('description', result.get('description', 'N/A'))}\n")
        f.write(f"Query batch: {result['query_batch']:,}\n")
        f.write(f"S·ªë rounds: {result['num_rounds']}\n")
        f.write(f"Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
        f.write(f"Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
        if result.get("query_gap_reason"):
            f.write(f"Ghi ch√∫ queries: {result['query_gap_reason']}\n")
        f.write(f"T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n\n")
        
        f.write("K·∫æT QU·∫¢ METRICS:\n")
        f.write("-" * 80 + "\n")
        f.write(f"Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
        f.write(f"Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
        f.write(f"F1-score: {result['final_f1']:.4f}\n")
        f.write(f"Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
        f.write(f"Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
        if not pd.isna(result.get('final_auc', float('nan'))):
            f.write(f"AUC: {result['final_auc']:.4f}\n")
        f.write(f"Precision: {result['final_precision']:.4f}\n")
        f.write(f"Recall: {result['final_recall']:.4f}\n\n")
        
        f.write("FILES:\n")
        f.write("-" * 80 + "\n")
        f.write(f"Metrics CSV: {result.get('metrics_csv', 'N/A')}\n")
        f.write(f"Surrogate model: {result.get('surrogate_model_path', 'N/A')}\n")
        f.write(f"Output directory: {result['output_dir']}\n")
    
    # T·∫°o report Markdown
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write(f"# B√°o C√°o Extraction Attack: {result['config_name']}\n\n")
        
        f.write("## Th√¥ng Tin C·∫•u H√¨nh\n\n")
        f.write(f"- **M√¥ t·∫£:** {config.get('description', result.get('description', 'N/A'))}\n")
        f.write(f"- **Query batch:** {result['query_batch']:,}\n")
        f.write(f"- **S·ªë rounds:** {result['num_rounds']}\n")
        f.write(f"- **Queries d·ª± ki·∫øn:** {result['total_queries']:,}\n")
        f.write(f"- **Queries th·ª±c t·∫ø:** {result.get('actual_queries_used', result['total_queries']):,}\n")
        if result.get("query_gap_reason"):
            f.write(f"- **Ghi ch√∫ queries:** {result['query_gap_reason']}\n")
        f.write(f"- **T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val):** {result['total_labels_used']:,}\n\n")
        
        f.write("## K·∫øt Qu·∫£ Metrics\n\n")
        f.write(f"- **Accuracy:** {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
        f.write(f"- **Balanced Accuracy:** {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
        f.write(f"- **F1-score:** {result['final_f1']:.4f}\n")
        f.write(f"- **Optimal Threshold:** {result.get('optimal_threshold', 0.5):.4f}\n")
        f.write(f"- **Agreement:** {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
        if not pd.isna(result.get('final_auc', float('nan'))):
            f.write(f"- **AUC:** {result['final_auc']:.4f}\n")
        f.write(f"- **Precision:** {result['final_precision']:.4f}\n")
        f.write(f"- **Recall:** {result['final_recall']:.4f}\n\n")
        
        f.write("## Files\n\n")
        f.write(f"- **Metrics CSV:** `{result.get('metrics_csv', 'N/A')}`\n")
        f.write(f"- **Surrogate model:** `{result.get('surrogate_model_path', 'N/A')}`\n")
        f.write(f"- **Output directory:** `{result['output_dir']}`\n")
    
    # T·∫°o report JSON
    with open(report_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "config": result,
            "description": config.get('description', result.get('description', 'N/A'))
        }, f, indent=2, ensure_ascii=False)
    
    print(f"   üìÑ ƒê√£ l∆∞u report trong folder output:")
    print(f"      - Text: {report_txt_path.name}")
    print(f"      - Markdown: {report_md_path.name}")
    print(f"      - JSON: {report_json_path.name}")


def main():
    parser = argparse.ArgumentParser(description="Ch·∫°y model extraction v·ªõi nhi·ªÅu c·∫•u h√¨nh")
    parser.add_argument("--model_name", type=str, default=None,
                       help="T√™n model (CEE, LEE, CSE, LSE, CNN, KNN, XGBOOST, DUALFFNN, TABNET). ∆Øu ti√™n h∆°n --model_path. S·∫Ω t·ª± ƒë·ªông detect type v√† t√¨m normalization stats.")
    parser.add_argument("--model_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file model (.h5, .lgb, .json, .pt, ho·∫∑c .zip). Ch·ªâ d√πng n·∫øu kh√¥ng c√≥ --model_name")
    parser.add_argument("--model_type", type=str, choices=["h5", "lgb", "xgboost", "pytorch", "tabnet"], default=None,
                       help="Lo·∫°i model: 'h5' (Keras), 'lgb' (LightGBM), 'xgboost' (XGBoost), 'pytorch' (PyTorch/dualFFNN), ho·∫∑c 'tabnet' (TabNet). Ch·ªâ c·∫ßn n·∫øu d√πng --model_path")
    parser.add_argument("--normalization_stats_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file normalization_stats.npz. C·∫ßn cho model_type='lgb', 'xgboost', 'pytorch', ho·∫∑c 'tabnet'")
    parser.add_argument("--attacker_type", type=str, choices=["keras", "lgb", "dual", "cnn", "knn", "xgb", "tabnet"], required=True,
                       help="Lo·∫°i surrogate model: 'keras' (DNN), 'lgb' (LightGBM), 'dual' (dualDNN), 'cnn' (CNN), 'knn' (KNN), 'xgb' (XGBoost), ho·∫∑c 'tabnet' (TabNet). B·∫ÆT BU·ªòC ph·∫£i ch·ªâ ƒë·ªãnh.")
    parser.add_argument("--dataset", type=str, choices=["ember", "somlap"], default="ember",
                       help="Dataset ƒë·ªÉ t·∫•n c√¥ng: 'ember' (m·∫∑c ƒë·ªãnh) ho·∫∑c 'somlap'")
    parser.add_argument("--threshold_optimization_metric", type=str, choices=["f1", "accuracy", "balanced_accuracy"], default="f1",
                       help="Metric ƒë·ªÉ t·ªëi ∆∞u threshold cho dualDNN: 'f1' (m·∫∑c ƒë·ªãnh), 'accuracy', ho·∫∑c 'balanced_accuracy'")
    parser.add_argument("--fixed_threshold", type=float, default=None,
                       help="S·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh thay v√¨ t·ªëi ∆∞u (v√≠ d·ª•: 0.5). Ch·ªâ √°p d·ª•ng cho dualDNN.")
    parser.add_argument("--auto_create_stats", action="store_true", default=False,
                       help="T·ª± ƒë·ªông t·∫°o file normalization stats n·∫øu kh√¥ng t√¨m th·∫•y (ch·ªâ cho model .lgb)")
    parser.add_argument("--surrogate_dir_template", type=str, default=None,
                       help="Template th∆∞ m·ª•c l∆∞u surrogate. C√≥ th·ªÉ d√πng {config}, {attacker}, {model}, {model_type}. M·∫∑c ƒë·ªãnh: output/<config>")
    parser.add_argument("--surrogate_name_template", type=str, default=None,
                       help="Template t√™n file surrogate (kh√¥ng extension). H·ªó tr·ª£ {config}, {attacker}, {model}, {model_type}. M·∫∑c ƒë·ªãnh: surrogate_model")
    args = parser.parse_args()
    
    # Validate arguments
    if args.model_name is None and args.model_path is None:
    # T·ª± ƒë·ªông t√¨m model file n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        pass
    elif args.model_name is not None and args.model_path is not None:
        raise ValueError("‚ùå Ch·ªâ cung c·∫•p --model-name HO·∫∂C --model-path, kh√¥ng ph·∫£i c·∫£ hai")
    
    # attacker_type ƒë√£ ƒë∆∞·ª£c argparse validate (required=True)
    
    # X·ª≠ l√Ω model_name ho·∫∑c model_path
    model_name = args.model_name.upper().strip() if args.model_name else None
    weights_path = None
    
    # Validate model-dataset compatibility early (before loading data)
    if model_name is not None:
        validate_model_dataset_compatibility(model_name, args.dataset)
        print(f"‚úÖ Model-dataset compatibility check passed: {model_name} <-> {args.dataset}")
    
    if model_name is not None:
        # S·ª≠ d·ª•ng model_name - s·∫Ω t·ª± ƒë·ªông detect m·ªçi th·ª©
        print(f"‚úÖ S·ª≠ d·ª•ng model name: {model_name}")
        print(f"   S·∫Ω t·ª± ƒë·ªông detect model type v√† t√¨m normalization stats")
        # weights_path v√† model_type s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω trong run_extraction
    elif args.model_path is None:
        possible_models = [
            PROJECT_ROOT / "artifacts" / "targets" / "CEE.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "CSE.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "LEE.lgb",
            PROJECT_ROOT / "artifacts" / "targets" / "LSE.lgb",
            PROJECT_ROOT / "artifacts" / "targets" / "CNN.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "KNN.pkl",
            PROJECT_ROOT / "artifacts" / "targets" / "xgboost_ember.json",
            PROJECT_ROOT / "artifacts" / "targets" / "dualffnn_ember_full.pt",
            PROJECT_ROOT / "artifacts" / "targets" / "tabnet_ember.zip",
            PROJECT_ROOT / "artifacts" / "targets" / "final_model.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "final_model.lgb",
        ]
        
        for model_path in possible_models:
            if model_path.exists():
                weights_path = str(model_path.resolve())
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y model: {weights_path}")
                break
        
        if weights_path is None:
            raise FileNotFoundError(
                f"Kh√¥ng t√¨m th·∫•y file model n√†o. Vui l√≤ng ch·ªâ ƒë·ªãnh b·∫±ng --model_path. "
                f"ƒê√£ t√¨m t·∫°i: {[str(p) for p in possible_models]}"
            )
    else:
        weights_path_obj = Path(args.model_path)
        if not weights_path_obj.is_absolute():
            weights_path = str((PROJECT_ROOT / args.model_path).resolve())
        else:
            weights_path = str(weights_path_obj.resolve())
        
        if not Path(weights_path).exists():
            raise FileNotFoundError(
                f"‚ùå Model file kh√¥ng t·ªìn t·∫°i: {weights_path}\n"
                f"   ƒê√£ th·ª≠ resolve t·ª´: {args.model_path}"
            )
    
    if model_name is None:
        # X·ª≠ l√Ω model_path (c√°ch c≈©)
        if args.model_type is None:
            model_path_obj = Path(weights_path)
            suffix_lower = model_path_obj.suffix.lower()
            
            if suffix_lower in ['.lgb', '.txt', '.d5']:
                args.model_type = "lgb"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: LGB (t·ª´ extension {model_path_obj.suffix})")
            elif suffix_lower == '.json':
                # XGBoost model
                args.model_type = "xgboost"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: XGBoost (t·ª´ extension {model_path_obj.suffix})")
            elif suffix_lower == '.pt':
                # PyTorch model (dualFFNN)
                args.model_type = "pytorch"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: PyTorch (t·ª´ extension {model_path_obj.suffix})")
            elif suffix_lower == '.zip':
                # TabNet model
                args.model_type = "tabnet"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: TabNet (t·ª´ extension {model_path_obj.suffix})")
            elif suffix_lower == '.pkl':
                # .pkl c√≥ th·ªÉ l√† LightGBM ho·∫∑c sklearn - s·∫Ω ƒë∆∞·ª£c auto-detect trong create_oracle_from_name
                # T·∫°m th·ªùi ƒë·ªÉ None ƒë·ªÉ auto-detect
                args.model_type = None
                print(f"‚úÖ File .pkl - s·∫Ω t·ª± ƒë·ªông detect model type (LightGBM ho·∫∑c sklearn) trong create_oracle_from_name")
            elif suffix_lower in ['.h5', '.hdf5']:
                args.model_type = "h5"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: H5 (t·ª´ extension {model_path_obj.suffix})")
            else:
                args.model_type = "h5"
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ph√°t hi·ªán model type t·ª´ extension, m·∫∑c ƒë·ªãnh: H5")
    
    # Ch·ªâ resolve weights_path n·∫øu kh√¥ng d√πng model_name
    if model_name is None:
        weights_path_abs = str(Path(weights_path).resolve())
        weights_path = weights_path_abs
        
        if not Path(weights_path).exists():
            raise FileNotFoundError(f"‚ùå Model file kh√¥ng t·ªìn t·∫°i: {weights_path}")
        
        # Get model info for verification
        model_path_obj = Path(weights_path)
        model_name_from_path = model_path_obj.name
        model_size = model_path_obj.stat().st_size / (1024 * 1024)  # MB
        
        print(f"\n‚úÖ ƒê√£ x√°c nh·∫≠n target model:")
        print(f"   Path (absolute): {weights_path}")
        print(f"   File name: {model_name_from_path}")
        print(f"   File size: {model_size:.2f} MB")
    else:
        # S·ª≠ d·ª•ng model_name - kh√¥ng c·∫ßn x·ª≠ l√Ω paths ·ªü ƒë√¢y
        model_path_obj = None
        model_name_from_path = model_name
        model_size = None
        args.model_type = None  # S·∫Ω ƒë∆∞·ª£c auto-detect

    model_identifier = model_name_from_path if model_name_from_path else "UNKNOWN_TARGET"
    model_type_label = (args.model_type.upper() if args.model_type else "AUTO")
    template_context_base = {
        "attacker": args.attacker_type,
        "model": model_identifier,
        "model_type": model_type_label,
    }
    
    normalization_stats_path = None
    # C√°c model types c·∫ßn normalization stats: lgb, sklearn, xgboost, pytorch (dualFFNN), tabnet
    needs_normalization = args.model_type in ["lgb", "sklearn", "xgboost", "pytorch", "tabnet"]
    if model_name is None and needs_normalization and args.normalization_stats_path is None:
        model_name_without_ext = model_path_obj.stem
        # X·ª≠ l√Ω special cases: dualffnn_ember_full.pt -> dualffnn_ember
        if model_name_without_ext.endswith("_full"):
            model_name_without_ext = model_name_without_ext[:-5]  # Remove "_full"
        
        possible_stats_paths = [
            model_path_obj.parent / f"{model_name_without_ext}.npz",
            model_path_obj.parent / f"{model_name_without_ext}_normalization_stats.npz",
            model_path_obj.parent / "normalization_stats.npz",
            PROJECT_ROOT / "artifacts" / "targets" / "normalization_stats.npz",
        ]
        
        for stats_path in possible_stats_paths:
            if stats_path.exists():
                normalization_stats_path = str(stats_path.resolve())
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y normalization stats: {normalization_stats_path}")
                print(f"   Stats file: {Path(normalization_stats_path).name}")
                break
        
        if normalization_stats_path is None and args.auto_create_stats:
            print(f"\n‚ö†Ô∏è  KH√îNG T√åM TH·∫§Y file normalization stats!")
            print(f"   üîÑ ƒêang t·ª± ƒë·ªông t·∫°o file normalization stats...")
            try:
                from scripts.data.create_normalization_stats import (
                    get_feature_columns,
                    compute_normalization_stats,
                )

                # Th·ª≠ d√πng file m·ªõi trong ember_2018_v2 tr∆∞·ªõc
                train_parquet_new = PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other.parquet"
                train_parquet_old = PROJECT_ROOT / "data" / "train_ember_2018_v2_features_label_other.parquet"
                if train_parquet_new.exists():
                    train_parquet = train_parquet_new
                elif train_parquet_old.exists():
                    train_parquet = train_parquet_old
                else:
                    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y training data t·∫°i: {train_parquet_new} ho·∫∑c {train_parquet_old}")

                output_stats_path = model_path_obj.parent / f"{model_name_without_ext}_normalization_stats.npz"
                label_col = "Label"

                print(f"   üìä ƒêang ƒë·ªçc features t·ª´ {train_parquet}...")
                feature_cols = get_feature_columns(str(train_parquet), label_col)

                print(f"   üìä ƒêang t√≠nh normalization stats...")
                feature_means, feature_stds = compute_normalization_stats(
                    str(train_parquet), feature_cols, label_col, sample_size=50000, batch_size=2048
                )

                print(f"   üíæ ƒêang l∆∞u v√†o {output_stats_path}...")
                import numpy as np

                np.savez(
                    str(output_stats_path),
                    feature_means=feature_means,
                    feature_stds=feature_stds,
                    feature_cols=np.array(feature_cols, dtype=object),
                )

                normalization_stats_path = str(output_stats_path.resolve())
                print(f"   ‚úÖ ƒê√£ t·∫°o file normalization stats: {normalization_stats_path}")
                print(f"   Stats file: {Path(normalization_stats_path).name} (cho model {model_name})")

            except Exception as e:
                print(f"   ‚ùå L·ªói khi t·∫°o normalization stats: {e}")
                import traceback

                traceback.print_exc()
                print(f"\n   üí° Vui l√≤ng t·∫°o th·ªß c√¥ng b·∫±ng:")
                print(f"      python scripts/data/create_normalization_stats.py \\")
                print(f"          --output_path {model_path_obj.parent / f'{model_name_without_ext}_normalization_stats.npz'}")
                print(f"   ho·∫∑c ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë√£ c√≥ s·∫µn qua --normalization_stats_path")
                raise
    else:
        if args.normalization_stats_path is not None:
            stats_path_obj = Path(args.normalization_stats_path)
            if not stats_path_obj.is_absolute():
                normalization_stats_path = str((PROJECT_ROOT / args.normalization_stats_path).resolve())
            else:
                normalization_stats_path = str(stats_path_obj.resolve())
            
            if not Path(normalization_stats_path).exists():
                raise FileNotFoundError(
                    f"‚ùå Normalization stats file kh√¥ng t·ªìn t·∫°i: {normalization_stats_path}\n"
                    f"   ƒê√£ th·ª≠ resolve t·ª´: {args.normalization_stats_path}"
                )
        else:
            normalization_stats_path = None
    
    base_output_dir = PROJECT_ROOT / "output"
    
    # ƒê∆∞·ªùng d·∫´n data files - ƒë·ªÉ None ƒë·ªÉ run_extraction() t·ª± x·ª≠ l√Ω d·ª±a tr√™n dataset parameter
    # run_extraction() s·∫Ω t·ª± ƒë·ªông load ƒë√∫ng file d·ª±a tr√™n dataset (ember ho·∫∑c somlap)
    train_parquet = None  # S·∫Ω ƒë∆∞·ª£c set b·ªüi run_extraction() d·ª±a tr√™n dataset parameter
    test_parquet = None   # S·∫Ω ƒë∆∞·ª£c set b·ªüi run_extraction() d·ª±a tr√™n dataset parameter
    
    # X√°c ƒë·ªãnh t√™n model target ƒë·ªÉ d√πng trong output folder name
    if model_name:
        target_model_name = model_name.upper()
    else:
        # N·∫øu kh√¥ng c√≥ model_name, l·∫•y t·ª´ file name (b·ªè extension)
        if weights_path:
            target_model_name = Path(weights_path).stem.upper()
        else:
            target_model_name = "UNKNOWN"
    
    # Format attacker type cho output folder name
    attacker_name_map = {
        "keras": "DNN",
        "lgb": "LGB",
        "dual": "dualDNN",
        "cnn": "CNN",
        "knn": "KNN",
        "xgb": "XGB",
        "tabnet": "TabNet"
    }
    attacker_name_display = attacker_name_map.get(args.attacker_type.lower(), args.attacker_type.upper())
    
    # Dataset name (lowercase cho folder name theo y√™u c·∫ßu)
    dataset_name = args.dataset.lower()
    
    # Helper function ƒë·ªÉ t·∫°o output folder name theo format: [targetmodel]-[dataset]-[surrogate]-[queries]
    def create_output_folder_name(target_model: str, dataset: str, attacker: str, total_queries: int) -> str:
        """T·∫°o t√™n folder output theo format: TARGETMODEL-dataset-ATTACKER-queries"""
        return f"{target_model}-{dataset}-{attacker}-{total_queries}"
    
    # C√°c c·∫•u h√¨nh kh√°c nhau
    # L∆∞u √Ω: total_budget = seed_size (10%) + val_size (20%) + AL_queries (70%)
    # AL_queries = query_batch √ó num_rounds (ch·ªâ t√≠nh s·ªë queries trong active learning rounds)
    # Labels s·ª≠ d·ª•ng = seed_size + val_size + AL_queries = total_budget
    # TEST MODE: S·ª≠ d·ª•ng config nh·ªè ƒë·ªÉ test nhanh
    test_mode = os.environ.get("EXTRACTION_TEST_MODE", "false").lower() == "true"
    
    if test_mode:
        total_queries = 100
        config_name = create_output_folder_name(target_model_name, dataset_name, attacker_name_display, total_queries)
        configurations = [
            {
                "name": config_name,
                "total_budget": total_queries,  # 100 queries total (seed + val + AL)
                "description": "TEST: T·ªïng 100 queries (seed + val + AL queries)"
            }
        ]
    else:
        # Config 1: 200 queries
        total_queries_1 = 200
        config_name_1 = create_output_folder_name(target_model_name, dataset_name, attacker_name_display, total_queries_1)
        # Config 2: 1000 queries
        total_queries_2 = 1000
        config_name_2 = create_output_folder_name(target_model_name, dataset_name, attacker_name_display, total_queries_2)
        # Config 3: 5000 queries
        total_queries_3 = 5000
        config_name_3 = create_output_folder_name(target_model_name, dataset_name, attacker_name_display, total_queries_3)
        
        configurations = [
            {
                "name": config_name_1,
                "total_budget": total_queries_1,  # 200 queries total (seed + val + AL)
                "description": "T·ªïng 200 queries (seed + val + AL queries)"
            },
            {
                "name": config_name_2,
                "total_budget": total_queries_2,  # 1000 queries total (seed + val + AL)
                "description": "T·ªïng 1,000 queries (seed + val + AL queries)"
            },
            {
                "name": config_name_3,
                "total_budget": total_queries_3,  # 5000 queries total (seed + val + AL)
                "description": "T·ªïng 5,000 queries (seed + val + AL queries)"
            }
        ]
    
    results = []
    
    print("=" * 80)
    print("B·∫ÆT ƒê·∫¶U CH·∫†Y EXTRACTION V·ªöI C√ÅC C·∫§U H√åNH KH√ÅC NHAU")
    print("=" * 80)
    print(f"\nüìã C·∫•u h√¨nh chung cho T·∫§T C·∫¢ configs:")
    print(f"   ‚úÖ Target model: {target_model_name}")
    if model_name:
        print(f"      (t·ª± ƒë·ªông detect type v√† path)")
    else:
        if weights_path:
            print(f"      File: {Path(weights_path).name}")
            print(f"      Path (absolute): {weights_path}")
            if args.model_type:
                print(f"      Model type: {args.model_type.upper()}")
    print(f"   ‚úÖ Dataset: {args.dataset.upper()}")
    if normalization_stats_path:
        print(f"   ‚úÖ Normalization stats: {Path(normalization_stats_path).name}")
        print(f"      Path (absolute): {normalization_stats_path}")
    elif model_name:
        print(f"   ‚ÑπÔ∏è  Normalization stats: T·ª± ƒë·ªông t√¨m (n·∫øu c√≥)")
    else:
        print(f"   ‚ÑπÔ∏è  Normalization stats: Kh√¥ng s·ª≠ d·ª•ng (Keras model)")
    print(f"   ‚úÖ Attacker type (surrogate model): {attacker_name_display}")
    print(f"   üìÅ Output folder format: {target_model_name}-{dataset_name}-{attacker_name_display}-[queries]")
    print("=" * 80)
    target_model_display = model_name if model_name else (Path(weights_path).name if weights_path else "Unknown")
    print(f"\n‚ö†Ô∏è  L∆ØU √ù: T·∫•t c·∫£ c√°c configs s·∫Ω t·∫•n c√¥ng C√ôNG M·ªòT target model: {target_model_display}")
    print("=" * 80)
    
    for config in configurations:
        print(f"\n{'='*80}")
        print(f"üî¨ C·∫§U H√åNH: {config['name']}")
        print(f"   {config['description']}")
        print(f"{'='*80}\n")
        
        output_dir = base_output_dir / config["name"]
        output_dir.mkdir(parents=True, exist_ok=True)
        
        template_context = dict(template_context_base)
        template_context["config"] = config["name"]
        surrogate_dir_override = None
        surrogate_name_override = None
        if args.surrogate_dir_template:
            formatted_dir = _format_template(
                args.surrogate_dir_template,
                template_context,
                "surrogate_dir_template"
            )
            surrogate_dir_override = str(_resolve_path(formatted_dir))
        if args.surrogate_name_template:
            surrogate_name_override = _format_template(
                args.surrogate_name_template,
                template_context,
                "surrogate_name_template"
            )

        if model_name is None:
            if not Path(weights_path).exists():
                raise FileNotFoundError(
                    f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Target model kh√¥ng t·ªìn t·∫°i khi ch·∫°y config {config['name']}!\n"
                    f"   Model path: {weights_path}\n"
                    f"   C√≥ th·ªÉ model ƒë√£ b·ªã x√≥a ho·∫∑c di chuy·ªÉn trong qu√° tr√¨nh ch·∫°y."
                )
        
        print(f"\nüîç X√°c nh·∫≠n target model cho config {config['name']}:")
        if model_name:
            print(f"   ‚úÖ Model name: {model_name} (s·∫Ω t·ª± ƒë·ªông detect)")
        else:
            print(f"   ‚úÖ Model file: {Path(weights_path).name}")
            print(f"   ‚úÖ Path: {weights_path}")
        if normalization_stats_path:
            if not Path(normalization_stats_path).exists():
                raise FileNotFoundError(
                    f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Normalization stats kh√¥ng t·ªìn t·∫°i!\n"
                    f"   Stats path: {normalization_stats_path}"
                )
            print(f"   ‚úÖ Normalization stats: {Path(normalization_stats_path).name}")
        if surrogate_dir_override:
            print(f"   üìÅ Surrogate dir override: {surrogate_dir_override}")
        if surrogate_name_override:
            print(f"   üìÑ Surrogate name override: {surrogate_name_override}")
        
        try:
            summary = run_extraction(
                output_dir=output_dir,
                train_parquet=train_parquet,
                test_parquet=test_parquet,
                dataset=args.dataset,  # Dataset ƒë·ªÉ t·∫•n c√¥ng: "ember" ho·∫∑c "somlap"
                seed=42,
                eval_size=4000,
                total_budget=config["total_budget"],  # T·ªïng query budget (seed + val + AL queries)
                num_epochs=100,  # Theo nghi√™n c·ª©u: 100 epochs v·ªõi early_stopping=30 (ch·ªâ d√πng cho Keras)
                model_type=args.model_type,
                normalization_stats_path=normalization_stats_path,  # ƒê·∫£m b·∫£o l√† absolute path
                attacker_type=args.attacker_type,
                weights_path=weights_path if model_name is None else None,
                model_name=model_name,
                threshold_optimization_metric=args.threshold_optimization_metric,
                fixed_threshold=args.fixed_threshold,
                surrogate_dir=surrogate_dir_override,
                surrogate_name=surrogate_name_override,
            )
            
            oracle_source = summary.get("oracle_source")
            if oracle_source is None:
                raise ValueError("Summary kh√¥ng ch·ª©a oracle_source ƒë·ªÉ verify.")
            summary_model_path = Path(oracle_source)
            summary_model_name = summary.get("model_file_name", summary_model_path.name)
            
            if model_name is None:
                # V·ªõi weights_path, verify c·∫£ path v√† name
                expected_model_name = Path(weights_path).name
                if weights_path and summary_model_path.resolve() != Path(weights_path).resolve():
                    print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Summary model path ({summary_model_path}) != Model path ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh ({weights_path})")
                    print(f"   Tuy nhi√™n s·∫Ω ti·∫øp t·ª•c v√¨ c√≥ th·ªÉ do resolve path.")
                
                if summary_model_name != expected_model_name:
                    raise ValueError(
                        f"Model file name kh√¥ng kh·ªõp: summary c√≥ {summary_model_name} nh∆∞ng expected l√† {expected_model_name}."
                    )
            else:
                # V·ªõi model_name, ch·ªâ c·∫ßn verify t√™n model kh·ªõp
                # X√°c ƒë·ªãnh extension d·ª±a tr√™n model_name ho·∫∑c summary_model_name
                model_name_upper = model_name.upper()
                summary_name_lower = summary_model_name.lower()
                
                # Map model names to expected extensions
                expected_extensions = {
                    "XGBOOST": ".json",
                    "DUALFFNN": ".pt",
                    "TABNET": ".zip",
                }
                
                # T√¨m extension t·ª´ summary ho·∫∑c d√πng default
                if any(ext in summary_name_lower for ext in [".lgb", ".txt", ".d5"]):
                    expected_ext = ".lgb"
                elif ".json" in summary_name_lower:
                    expected_ext = ".json"
                elif ".pt" in summary_name_lower:
                    expected_ext = ".pt"
                elif ".zip" in summary_name_lower:
                    expected_ext = ".zip"
                elif ".h5" in summary_name_lower or ".hdf5" in summary_name_lower:
                    expected_ext = ".h5"
                elif model_name_upper in expected_extensions:
                    expected_ext = expected_extensions[model_name_upper]
                else:
                    expected_ext = ".h5"  # Default
                
                expected_model_name = f"{model_name}{expected_ext}".lower()
                if summary_model_name.lower() != expected_model_name:
                    print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Summary model name ({summary_model_name}) != Expected model name ({expected_model_name})")
                    print(f"   Tuy nhi√™n s·∫Ω ti·∫øp t·ª•c v√¨ c√≥ th·ªÉ do extension kh√°c nhau ho·∫∑c naming convention.")
                if not summary_model_name.upper().startswith(model_name.upper()):
                    print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Summary model name ({summary_model_name}) kh√¥ng b·∫Øt ƒë·∫ßu v·ªõi model name ({model_name})")
                    print(f"   Tuy nhi√™n s·∫Ω ti·∫øp t·ª•c v√¨ c√≥ th·ªÉ do naming convention.")
            
            print(f"   ‚úÖ Verified: Model trong summary kh·ªõp ({summary_model_name})")
            
            # L·∫•y metrics cu·ªëi c√πng
            final_metrics = summary["metrics"][-1] if summary["metrics"] else {}
            
            # L·∫•y s·ªë queries th·ª±c t·∫ø t·ª´ metrics (kh√¥ng t√≠nh seed v√† val)
            # queries_used trong metrics ch·ªâ t√≠nh AL queries, kh√¥ng t√≠nh seed v√† val
            # N·∫øu kh√¥ng c√≥ trong metrics, t√≠nh t·ª´ summary ho·∫∑c config
            if "queries_used" in final_metrics:
                actual_queries_used = final_metrics["queries_used"]
            else:
                # Fallback: T√≠nh AL queries t·ª´ summary ho·∫∑c config
                query_batch_from_summary = summary.get("query_batch", 0)
                num_rounds_from_summary = summary.get("num_rounds", 0)
                if query_batch_from_summary > 0 and num_rounds_from_summary > 0:
                    actual_queries_used = query_batch_from_summary * num_rounds_from_summary
                else:
                    # T√≠nh t·ª´ total_budget: AL_queries = total_budget - seed - val
                    seed_size_from_summary = summary.get("seed_size", 0)
                    val_size_from_summary = summary.get("val_size", 0)
                    if seed_size_from_summary > 0 and val_size_from_summary > 0:
                        actual_queries_used = config["total_budget"] - seed_size_from_summary - val_size_from_summary
                    else:
                        # Fallback cu·ªëi c√πng: d√πng total_budget (sai nh∆∞ng t·ªët h∆°n l√† crash)
                        actual_queries_used = config["total_budget"]
            
            result = {
                "config_name": config["name"],
                "description": config["description"],
                "total_queries": config["total_budget"],  # Total query budget (seed + val + AL)
                "actual_queries_used": summary.get("total_queries_actual", actual_queries_used),
                "query_batch": summary.get("query_batch", 0),  # L·∫•y t·ª´ summary
                "num_rounds": summary.get("num_rounds", 0),  # L·∫•y t·ª´ summary
                "seed_size": summary.get("seed_size", 0),
                "val_size": summary.get("val_size", 0),
                "total_labels_used": final_metrics.get("labels_used", 0),
                "optimal_threshold": final_metrics.get("optimal_threshold", 0.5),
                "final_accuracy": final_metrics.get("surrogate_acc", 0.0),
                "final_balanced_accuracy": final_metrics.get("surrogate_balanced_acc", 0.0),  # Quan tr·ªçng v·ªõi class imbalance
                "final_auc": final_metrics.get("surrogate_auc", float("nan")),
                "final_precision": final_metrics.get("surrogate_precision", 0.0),
                "final_recall": final_metrics.get("surrogate_recall", 0.0),
                "final_f1": final_metrics.get("surrogate_f1", 0.0),
                "final_agreement": final_metrics.get("agreement_with_target", 0.0),
                "output_dir": str(output_dir),
                "metrics_csv": summary.get("metrics_csv", ""),
                "surrogate_model_path": summary.get("surrogate_model_path", ""),
                "query_gap_reason": summary.get("query_gap_reason"),
            }

            #region agent log
            try:
                import json as _json, time as _time
                _log_payload = {
                    "sessionId": "debug-session",
                    "runId": "pre-fix",
                    "hypothesisId": "H1",
                    "location": "run_multiple_extractions.py:result_build",
                    "message": "summary to result mapping",
                    "data": {
                        "config": config["name"],
                        "summary_last_metrics": summary.get("metrics", [])[-1] if summary.get("metrics") else None,
                        "final_accuracy": result["final_accuracy"],
                        "final_agreement": result["final_agreement"],
                        "optimal_threshold": result["optimal_threshold"],
                        "metrics_csv": result["metrics_csv"],
                        "surrogate_model_path": result["surrogate_model_path"],
                        "total_queries_actual": summary.get("total_queries_actual"),
                        "query_batch": summary.get("query_batch"),
                        "num_rounds": summary.get("num_rounds"),
                    },
                    "timestamp": int(_time.time() * 1000),
                }
                with open("/home/hytong/Documents/model_extraction_malware/.cursor/debug.log", "a", encoding="utf-8") as _f:
                    _f.write(_json.dumps(_log_payload, ensure_ascii=False) + "\n")
            except Exception:
                pass
            #endregion
            
            results.append(result)
            
            # T·∫°o report ri√™ng cho t·ª´ng config trong folder output c·ªßa config ƒë√≥
            _create_individual_report(output_dir, result, config)
            
            print(f"\n{'='*80}")
            print(f"‚úÖ Ho√†n th√†nh {config['name']}")
            print(f"{'='*80}")
            print(f"   Accuracy: {result['final_accuracy']:.4f}")
            print(f"   Balanced Accuracy: {result['final_balanced_accuracy']:.4f} (quan tr·ªçng v·ªõi class imbalance)")
            print(f"   F1-score: {result['final_f1']:.4f}")
            print(f"   Agreement: {result['final_agreement']:.4f}")
            print(f"   Optimal Threshold: {result['optimal_threshold']:.4f}")
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi ch·∫°y {config['name']}: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "config_name": config["name"],
                "description": config["description"],
                "error": str(e)
            })
    
    # T·∫°o report
    print(f"\n{'='*80}")
    print("üìä T·∫†O REPORT")
    print(f"{'='*80}\n")
    
    #region agent log
    try:
        import json as _json, time as _time
        _log_payload = {
            "sessionId": "debug-session",
            "runId": "pre-fix",
            "hypothesisId": "H3",
            "location": "run_multiple_extractions.py:before_report",
            "message": "results collected before report",
            "data": {
                "results_count": len(results),
                "configs": [r.get("config_name") for r in results],
                "final_metrics_list": [
                    {
                        "config": r.get("config_name"),
                        "final_accuracy": r.get("final_accuracy"),
                        "final_agreement": r.get("final_agreement"),
                        "optimal_threshold": r.get("optimal_threshold"),
                        "metrics_csv": r.get("metrics_csv"),
                    }
                    for r in results
                    if "error" not in r
                ],
            },
            "timestamp": int(_time.time() * 1000),
        }
        with open("/home/hytong/Documents/model_extraction_malware/.cursor/debug.log", "a", encoding="utf-8") as _f:
            _f.write(_json.dumps(_log_payload, ensure_ascii=False) + "\n")
    except Exception:
        pass
    #endregion

    report_path = base_output_dir / "extraction_comparison_report.txt"
    report_md_path = base_output_dir / "extraction_comparison_report.md"
    
    # T·∫°o report text
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO SO S√ÅNH C√ÅC SURROGATE MODELS\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("T√ìM T·∫ÆT:\n")
        f.write("-" * 80 + "\n")
        f.write(f"ƒê√£ ch·∫°y extraction v·ªõi {len(configurations)} c·∫•u h√¨nh kh√°c nhau:\n\n")
        
        for result in results:
            if "error" in result:
                f.write(f"‚ùå {result['config_name']}: L·ªñI - {result['error']}\n")
            else:
                f.write(f"‚úÖ {result['config_name']}:\n")
                f.write(f"   - Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"   - Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                if result.get("query_gap_reason"):
                    f.write(f"   - Ghi ch√∫ queries: {result['query_gap_reason']}\n")
                f.write(f"   - Labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n")
                f.write(f"   - Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"   - Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"   - F1-score: {result['final_f1']:.4f}\n")
                f.write(f"   - Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                f.write(f"   - Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"   - AUC: {result['final_auc']:.4f}\n")
                f.write(f"   - Precision: {result['final_precision']:.4f}\n")
                f.write(f"   - Recall: {result['final_recall']:.4f}\n")
                f.write(f"   - Output: {result['output_dir']}\n")
                f.write("\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("CHI TI·∫æT T·ª™NG C·∫§U H√åNH:\n")
        f.write("=" * 80 + "\n\n")
        
        for result in results:
            if "error" not in result:
                f.write(f"\n{result['config_name'].upper().replace('_', ' ')}:\n")
                f.write("-" * 80 + "\n")
                f.write(f"M√¥ t·∫£: {result['description']}\n")
                f.write(f"Query batch: {result['query_batch']:,}\n")
                f.write(f"S·ªë rounds: {result['num_rounds']}\n")
                f.write(f"Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                if result.get("query_gap_reason"):
                    f.write(f"L√Ω do ch√™nh l·ªách queries: {result['query_gap_reason']}\n")
                f.write(f"T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n\n")
                
                f.write("Metrics cu·ªëi c√πng:\n")
                f.write(f"  - Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"  - Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"  - F1-score: {result['final_f1']:.4f}\n")
                f.write(f"  - Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                f.write(f"  - Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"  - AUC: {result['final_auc']:.4f}\n")
                f.write(f"  - Precision: {result['final_precision']:.4f}\n")
                f.write(f"  - Recall: {result['final_recall']:.4f}\n")
                f.write(f"\nFiles:\n")
                f.write(f"  - Metrics CSV: {result['metrics_csv']}\n")
                f.write(f"  - Surrogate model: {result['surrogate_model_path']}\n")
                f.write(f"  - Output directory: {result['output_dir']}\n")
    
    # T·∫°o report Markdown
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o So S√°nh C√°c Surrogate Models\n\n")
        f.write("## T√≥m T·∫Øt\n\n")
        f.write(f"ƒê√£ ch·∫°y extraction v·ªõi {len(configurations)} c·∫•u h√¨nh kh√°c nhau v·ªÅ s·ªë l∆∞·ª£ng queries.\n\n")
        
        f.write("## B·∫£ng So S√°nh\n\n")
        f.write("| C·∫•u h√¨nh | Queries | Labels | Accuracy | Balanced Acc | F1 | Agreement | Threshold | AUC |\n")
        f.write("|----------|---------|--------|----------|--------------|----|-----------|-----------|-----|\n")
        
        for result in results:
            if "error" not in result:
                auc_str = f"{result['final_auc']:.4f}" if not pd.isna(result['final_auc']) else "N/A"
                actual_queries = result.get('actual_queries_used', result['total_queries'])
                balanced_acc = result.get('final_balanced_accuracy', 0.0)
                threshold = result.get('optimal_threshold', 0.5)
                f.write(f"| {result['config_name']} | {actual_queries:,} | {result['total_labels_used']:,} | "
                       f"{result['final_accuracy']:.4f} | {balanced_acc:.4f} | {result['final_f1']:.4f} | "
                       f"{result['final_agreement']:.4f} | {threshold:.3f} | {auc_str} |\n")
            else:
                f.write(f"| {result['config_name']} | ERROR | - | - | - | - | - | - | - |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng C·∫•u H√¨nh\n\n")
        
        for result in results:
            if "error" not in result:
                f.write(f"### {result['config_name'].replace('_', ' ').title()}\n\n")
                f.write(f"**M√¥ t·∫£:** {result['description']}\n\n")
                f.write(f"- Query batch: {result['query_batch']:,}\n")
                f.write(f"- S·ªë rounds: {result['num_rounds']}\n")
                f.write(f"- Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"- Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                if result.get("query_gap_reason"):
                    f.write(f"- Ghi ch√∫ queries: {result['query_gap_reason']}\n")
                f.write(f"- T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n\n")
                
                f.write("**Metrics cu·ªëi c√πng:**\n\n")
                f.write(f"- Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"- Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"- F1-score: {result['final_f1']:.4f}\n")
                f.write(f"- Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                f.write(f"- Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"- AUC: {result['final_auc']:.4f}\n")
                f.write(f"- Precision: {result['final_precision']:.4f}\n")
                f.write(f"- Recall: {result['final_recall']:.4f}\n\n")
                
                f.write("**Files:**\n\n")
                f.write(f"- Metrics CSV: `{result['metrics_csv']}`\n")
                f.write(f"- Surrogate model: `{result['surrogate_model_path']}`\n")
                f.write(f"- Output directory: `{result['output_dir']}`\n\n")
    
    # L∆∞u JSON summary
    json_path = base_output_dir / "extraction_comparison_summary.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON summary: {json_path}")
    
    # In t√≥m t·∫Øt ra console
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    df_results = pd.DataFrame([r for r in results if "error" not in r])
    if not df_results.empty:
        df_display = df_results.copy()
        if 'actual_queries_used' not in df_display.columns:
            df_display['actual_queries_used'] = df_display['total_queries']
        print(df_display[["config_name", "actual_queries_used", "total_labels_used", 
                          "final_accuracy", "final_balanced_accuracy", "final_f1", 
                          "final_agreement", "optimal_threshold"]].to_string(index=False))
    
    return results


if __name__ == "__main__":
    main()

