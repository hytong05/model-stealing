"""
Script ƒë·ªÉ so s√°nh 3 ki·∫øn tr√∫c dualFFNN: dualFFNN, dualFFNN-1 (deeper), dualFFNN-2 (narrower)
C·ªë ƒë·ªãnh s·ªë queries t·∫•n c√¥ng l√† 1000 v√† so s√°nh k·∫øt qu·∫£ gi·ªØa 3 ki·∫øn tr√∫c.
"""
import json
import os
import sys
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import tensorflow as tf

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.models.dnn import create_dnn2, create_dnn2_deeper, create_dnn2_narrower
from src.attackers import AbstractAttacker


class KerasDualAttackerOriginal(AbstractAttacker):
    """KerasDualAttacker v·ªõi ki·∫øn tr√∫c dualFFNN g·ªëc (create_dnn2)"""
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381,)):
        self.model = create_dnn2(seed=seed, mc=mc, input_shape=input_shape)
        self.checkpoint_filepath = '/tmp/checkpoint2_original.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)
        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, y_true, X_val, y_val, y_val_true, num_epochs):
        self.model.fit((X, y_true), y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=((X_val, y_val_true), y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])  
        self.model.load_weights(self.checkpoint_filepath)          

    def __call__(self, X, y_true):
        return self.model.predict((X, y_true), verbose=0)

    def save_model(self, path):
        self.model.save(path+".h5")


class KerasDualAttackerDeeper(AbstractAttacker):
    """KerasDualAttacker v·ªõi ki·∫øn tr√∫c dualFFNN-1 (deeper - create_dnn2_deeper)"""
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381,)):
        self.model = create_dnn2_deeper(seed=seed, mc=mc, input_shape=input_shape)
        self.checkpoint_filepath = '/tmp/checkpoint2_deeper.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)
        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, y_true, X_val, y_val, y_val_true, num_epochs):
        self.model.fit((X, y_true), y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=((X_val, y_val_true), y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])  
        self.model.load_weights(self.checkpoint_filepath)          

    def __call__(self, X, y_true):
        return self.model.predict((X, y_true), verbose=0)

    def save_model(self, path):
        self.model.save(path+".h5")


class KerasDualAttackerNarrower(AbstractAttacker):
    """KerasDualAttacker v·ªõi ki·∫øn tr√∫c dualFFNN-2 (narrower - create_dnn2_narrower)"""
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381,)):
        self.model = create_dnn2_narrower(seed=seed, mc=mc, input_shape=input_shape)
        self.checkpoint_filepath = '/tmp/checkpoint2_narrower.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)
        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, y_true, X_val, y_val, y_val_true, num_epochs):
        self.model.fit((X, y_true), y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=((X_val, y_val_true), y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])  
        self.model.load_weights(self.checkpoint_filepath)          

    def __call__(self, X, y_true):
        return self.model.predict((X, y_true), verbose=0)

    def save_model(self, path):
        self.model.save(path+".h5")


def _resolve_path(path_str: str) -> Path:
    """Resolve path (relative to PROJECT_ROOT n·∫øu c·∫ßn)."""
    path_obj = Path(path_str)
    if not path_obj.is_absolute():
        path_obj = PROJECT_ROOT / path_obj
    return path_obj.resolve()


def run_extraction_with_architecture(
    architecture_name: str,
    attacker_class,
    output_dir: Path,
    train_parquet=None,
    test_parquet=None,
    dataset: str = "ember",
    seed: int = 42,
    eval_size: int = 4000,
    total_budget: int = 1000,
    num_epochs: int = 100,
    model_type: str = None,
    normalization_stats_path: str = None,
    weights_path: str | None = None,
    model_name: str = None,
    threshold_optimization_metric: str = "f1",
    fixed_threshold: float | None = None,
    surrogate_name: str | None = None,
):
    """
    Ch·∫°y extraction v·ªõi attacker class t√πy ch·ªânh.
    
    Args:
        architecture_name: T√™n ki·∫øn tr√∫c (dualFFNN, dualFFNN-1, dualFFNN-2)
        attacker_class: Class c·ªßa attacker (KerasDualAttackerOriginal, KerasDualAttackerDeeper, KerasDualAttackerNarrower)
        ... (c√°c tham s·ªë kh√°c gi·ªëng run_extraction)
    """
    # Monkey-patch KerasDualAttacker trong c·∫£ src.attackers v√† extract_final_model
    import src.attackers as attackers_module
    original_attacker_class = attackers_module.KerasDualAttacker
    attackers_module.KerasDualAttacker = attacker_class
    
    # Import v√† patch trong extract_final_model module
    import scripts.attacks.extract_final_model as extract_module
    original_extract_attacker = extract_module.KerasDualAttacker
    extract_module.KerasDualAttacker = attacker_class
    
    from scripts.attacks.extract_final_model import run_extraction
    
    try:
        # G·ªçi run_extraction v·ªõi attacker_type="dual"
        summary = run_extraction(
            output_dir=output_dir,
            train_parquet=train_parquet,
            test_parquet=test_parquet,
            dataset=dataset,
            seed=seed,
            eval_size=eval_size,
            total_budget=total_budget,
            num_epochs=num_epochs,
            model_type=model_type,
            normalization_stats_path=normalization_stats_path,
            attacker_type="dual",
            weights_path=weights_path,
            model_name=model_name,
            threshold_optimization_metric=threshold_optimization_metric,
            fixed_threshold=fixed_threshold,
            surrogate_name=surrogate_name,
        )
        
        summary['architecture_name'] = architecture_name
        return summary
    finally:
        # Kh√¥i ph·ª•c class g·ªëc
        attackers_module.KerasDualAttacker = original_attacker_class
        extract_module.KerasDualAttacker = original_extract_attacker


def main():
    parser = argparse.ArgumentParser(
        description="So s√°nh 3 ki·∫øn tr√∫c dualFFNN v·ªõi s·ªë queries c·ªë ƒë·ªãnh 1000"
    )
    parser.add_argument("--model_name", type=str, default=None,
                       help="T√™n model (CEE, LEE, CSE, LSE). ∆Øu ti√™n h∆°n --model_path.")
    parser.add_argument("--model_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file model (.h5 ho·∫∑c .lgb). Ch·ªâ d√πng n·∫øu kh√¥ng c√≥ --model_name")
    parser.add_argument("--model_type", type=str, choices=["h5", "lgb"], default=None,
                       help="Lo·∫°i model: 'h5' (Keras) ho·∫∑c 'lgb' (LightGBM). Ch·ªâ c·∫ßn n·∫øu d√πng --model_path")
    parser.add_argument("--normalization_stats_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file normalization_stats.npz. Ch·ªâ c·∫ßn n·∫øu d√πng --model_path v·ªõi model_type='lgb'")
    parser.add_argument("--dataset", type=str, choices=["ember", "somlap"], default="ember",
                       help="Dataset ƒë·ªÉ t·∫•n c√¥ng: 'ember' (m·∫∑c ƒë·ªãnh) ho·∫∑c 'somlap'")
    parser.add_argument("--threshold_optimization_metric", type=str, choices=["f1", "accuracy", "balanced_accuracy"], default="f1",
                       help="Metric ƒë·ªÉ t·ªëi ∆∞u threshold: 'f1' (m·∫∑c ƒë·ªãnh), 'accuracy', ho·∫∑c 'balanced_accuracy'")
    parser.add_argument("--fixed_threshold", type=float, default=None,
                       help="S·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh thay v√¨ t·ªëi ∆∞u (v√≠ d·ª•: 0.5)")
    parser.add_argument("--total_queries", type=int, default=1000,
                       help="T·ªïng s·ªë queries t·∫•n c√¥ng (m·∫∑c ƒë·ªãnh: 1000)")
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.model_name is None and args.model_path is None:
        # T·ª± ƒë·ªông t√¨m model file n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        pass
    elif args.model_name is not None and args.model_path is not None:
        raise ValueError("‚ùå Ch·ªâ cung c·∫•p --model_name HO·∫∂C --model_path, kh√¥ng ph·∫£i c·∫£ hai")
    
    # X·ª≠ l√Ω model_name ho·∫∑c model_path
    model_name = args.model_name.upper().strip() if args.model_name else None
    weights_path = None
    
    if model_name is not None:
        print(f"‚úÖ S·ª≠ d·ª•ng model name: {model_name}")
        print(f"   S·∫Ω t·ª± ƒë·ªông detect model type v√† t√¨m normalization stats")
    elif args.model_path is None:
        # T·ª± ƒë·ªông t√¨m model
        possible_models = [
            PROJECT_ROOT / "artifacts" / "targets" / "CEE.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "CSE.h5",
            PROJECT_ROOT / "artifacts" / "targets" / "LEE.lgb",
            PROJECT_ROOT / "artifacts" / "targets" / "LSE.lgb",
        ]
        
        for model_path in possible_models:
            if model_path.exists():
                weights_path = str(model_path.resolve())
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y model: {weights_path}")
                break
        
        if weights_path is None:
            raise FileNotFoundError(
                f"Kh√¥ng t√¨m th·∫•y file model n√†o. Vui l√≤ng ch·ªâ ƒë·ªãnh b·∫±ng --model_path. "
                f"ƒê√£ t√¨m t·∫°i: {[str(p) for p in possible_models]}"
            )
    else:
        weights_path_obj = Path(args.model_path)
        if not weights_path_obj.is_absolute():
            weights_path = str((PROJECT_ROOT / args.model_path).resolve())
        else:
            weights_path = str(weights_path_obj.resolve())
        
        if not Path(weights_path).exists():
            raise FileNotFoundError(
                f"‚ùå Model file kh√¥ng t·ªìn t·∫°i: {weights_path}\n"
                f"   ƒê√£ th·ª≠ resolve t·ª´: {args.model_path}"
            )
    
    if model_name is None:
        # X·ª≠ l√Ω model_type n·∫øu c·∫ßn
        if args.model_type is None:
            model_path_obj = Path(weights_path)
            if model_path_obj.suffix.lower() in ['.lgb', '.txt', '.d5']:
                args.model_type = "lgb"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: LGB (t·ª´ extension {model_path_obj.suffix})")
            elif model_path_obj.suffix.lower() in ['.h5', '.hdf5']:
                args.model_type = "h5"
                print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: H5 (t·ª´ extension {model_path_obj.suffix})")
            else:
                args.model_type = "h5"
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ph√°t hi·ªán model type t·ª´ extension, m·∫∑c ƒë·ªãnh: H5")
    
    # X·ª≠ l√Ω normalization_stats_path n·∫øu c·∫ßn
    normalization_stats_path = None
    if model_name is None and args.model_type in ["lgb", "sklearn"] and args.normalization_stats_path is None:
        model_path_obj = Path(weights_path)
        model_name_without_ext = model_path_obj.stem
        possible_stats_paths = [
            model_path_obj.parent / f"{model_name_without_ext}.npz",
            model_path_obj.parent / f"{model_name_without_ext}_normalization_stats.npz",
            model_path_obj.parent / "normalization_stats.npz",
            PROJECT_ROOT / "artifacts" / "targets" / "normalization_stats.npz",
        ]
        
        for stats_path in possible_stats_paths:
            if stats_path.exists():
                normalization_stats_path = str(stats_path.resolve())
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y normalization stats: {normalization_stats_path}")
                break
    elif args.normalization_stats_path is not None:
        stats_path_obj = Path(args.normalization_stats_path)
        if not stats_path_obj.is_absolute():
            normalization_stats_path = str((PROJECT_ROOT / args.normalization_stats_path).resolve())
        else:
            normalization_stats_path = str(stats_path_obj.resolve())
        
        if not Path(normalization_stats_path).exists():
            raise FileNotFoundError(
                f"‚ùå Normalization stats file kh√¥ng t·ªìn t·∫°i: {normalization_stats_path}"
            )
    
    # X√°c ƒë·ªãnh t√™n model target
    if model_name:
        target_model_name = model_name.upper()
    else:
        if weights_path:
            target_model_name = Path(weights_path).stem.upper()
        else:
            target_model_name = "UNKNOWN"
    
    dataset_name = args.dataset.lower()
    total_queries = args.total_queries
    
    # C·∫•u h√¨nh 3 ki·∫øn tr√∫c
    architectures = [
        {
            "name": "dualFFNN",
            "description": "dualFFNN - Ki·∫øn tr√∫c g·ªëc",
            "attacker_class": KerasDualAttackerOriginal,
        },
        {
            "name": "dualFFNN-1",
            "description": "dualFFNN-1 - Deeper Network (2382‚Üí2382‚Üí1024‚Üí512‚Üí128‚Üí64‚Üí32‚Üí1)",
            "attacker_class": KerasDualAttackerDeeper,
        },
        {
            "name": "dualFFNN-2",
            "description": "dualFFNN-2 - Narrower Network (2382‚Üí1024‚Üí512‚Üí256‚Üí64‚Üí1)",
            "attacker_class": KerasDualAttackerNarrower,
        },
    ]
    
    base_output_dir = PROJECT_ROOT / "output"
    
    # T·∫°o th∆∞ m·ª•c output ch√≠nh cho comparison
    comparison_output_dir = base_output_dir / f"dualFFNN_comparison_{target_model_name}_{dataset_name}_{total_queries}"
    comparison_output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("B·∫ÆT ƒê·∫¶U SO S√ÅNH 3 KI·∫æN TR√öC DUALFFNN")
    print("=" * 80)
    print(f"\nüìã C·∫•u h√¨nh:")
    print(f"   ‚úÖ Target model: {target_model_name}")
    print(f"   ‚úÖ Dataset: {args.dataset.upper()}")
    print(f"   ‚úÖ T·ªïng queries: {total_queries:,}")
    print(f"   ‚úÖ S·ªë ki·∫øn tr√∫c: {len(architectures)}")
    print(f"   ‚úÖ Output directory: {comparison_output_dir}")
    print("=" * 80)
    
    results = []
    train_parquet = None
    test_parquet = None
    
    for arch_config in architectures:
        arch_name = arch_config["name"]
        arch_description = arch_config["description"]
        attacker_class = arch_config["attacker_class"]
        
        print(f"\n{'='*80}")
        print(f"üî¨ KI·∫æN TR√öC: {arch_name}")
        print(f"   {arch_description}")
        print(f"{'='*80}\n")
        
        # T·∫°o th∆∞ m·ª•c output cho t·ª´ng ki·∫øn tr√∫c
        arch_output_dir = comparison_output_dir / arch_name
        arch_output_dir.mkdir(parents=True, exist_ok=True)
        
        surrogate_name = f"surrogate_{target_model_name}_{arch_name}"
        
        try:
            summary = run_extraction_with_architecture(
                architecture_name=arch_name,
                attacker_class=attacker_class,
                output_dir=arch_output_dir,
                train_parquet=train_parquet,
                test_parquet=test_parquet,
                dataset=args.dataset,
                seed=42,
                eval_size=4000,
                total_budget=total_queries,
                num_epochs=100,
                model_type=args.model_type,
                normalization_stats_path=normalization_stats_path,
                weights_path=weights_path if model_name is None else None,
                model_name=model_name,
                threshold_optimization_metric=args.threshold_optimization_metric,
                fixed_threshold=args.fixed_threshold,
                surrogate_name=surrogate_name,
            )
            
            # L·∫•y metrics cu·ªëi c√πng
            final_metrics = summary["metrics"][-1] if summary["metrics"] else {}
            
            result = {
                "architecture": arch_name,
                "description": arch_description,
                "total_queries": total_queries,
                "actual_queries_used": summary.get("total_queries_actual", total_queries),
                "query_batch": summary.get("query_batch", 0),
                "num_rounds": summary.get("num_rounds", 0),
                "seed_size": summary.get("seed_size", 0),
                "val_size": summary.get("val_size", 0),
                "total_labels_used": final_metrics.get("labels_used", 0),
                "optimal_threshold": final_metrics.get("optimal_threshold", 0.5),
                "final_accuracy": final_metrics.get("surrogate_acc", 0.0),
                "final_balanced_accuracy": final_metrics.get("surrogate_balanced_acc", 0.0),
                "final_auc": final_metrics.get("surrogate_auc", float("nan")),
                "final_precision": final_metrics.get("surrogate_precision", 0.0),
                "final_recall": final_metrics.get("surrogate_recall", 0.0),
                "final_f1": final_metrics.get("surrogate_f1", 0.0),
                "final_agreement": final_metrics.get("agreement_with_target", 0.0),
                "output_dir": str(arch_output_dir),
                "metrics_csv": summary.get("metrics_csv", ""),
                "surrogate_model_path": summary.get("surrogate_model_path", ""),
            }
            
            results.append(result)
            
            print(f"\n{'='*80}")
            print(f"‚úÖ Ho√†n th√†nh {arch_name}")
            print(f"{'='*80}")
            print(f"   Accuracy: {result['final_accuracy']:.4f}")
            print(f"   Balanced Accuracy: {result['final_balanced_accuracy']:.4f}")
            print(f"   F1-score: {result['final_f1']:.4f}")
            print(f"   Agreement: {result['final_agreement']:.4f}")
            print(f"   Optimal Threshold: {result['optimal_threshold']:.4f}")
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi ch·∫°y {arch_name}: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "architecture": arch_name,
                "description": arch_description,
                "error": str(e)
            })
    
    # T·∫°o report so s√°nh
    print(f"\n{'='*80}")
    print("üìä T·∫†O REPORT SO S√ÅNH")
    print(f"{'='*80}\n")
    
    report_path = comparison_output_dir / "comparison_report.txt"
    report_md_path = comparison_output_dir / "comparison_report.md"
    
    # T·∫°o report text
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO SO S√ÅNH 3 KI·∫æN TR√öC DUALFFNN\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("T√ìM T·∫ÆT:\n")
        f.write("-" * 80 + "\n")
        f.write(f"Target model: {target_model_name}\n")
        f.write(f"Dataset: {args.dataset.upper()}\n")
        f.write(f"T·ªïng queries: {total_queries:,}\n")
        f.write(f"S·ªë ki·∫øn tr√∫c: {len(architectures)}\n\n")
        
        f.write("K·∫æT QU·∫¢ SO S√ÅNH:\n")
        f.write("-" * 80 + "\n\n")
        
        for result in results:
            if "error" in result:
                f.write(f"‚ùå {result['architecture']}: L·ªñI - {result['error']}\n\n")
            else:
                f.write(f"‚úÖ {result['architecture']} ({result['description']}):\n")
                f.write(f"   - Queries th·ª±c t·∫ø: {result.get('actual_queries_used', total_queries):,}\n")
                f.write(f"   - Labels s·ª≠ d·ª•ng: {result['total_labels_used']:,}\n")
                f.write(f"   - Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"   - Balanced Accuracy: {result['final_balanced_accuracy']:.4f} ({result['final_balanced_accuracy']*100:.2f}%)\n")
                f.write(f"   - F1-score: {result['final_f1']:.4f}\n")
                f.write(f"   - Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                f.write(f"   - Optimal Threshold: {result['optimal_threshold']:.4f}\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"   - AUC: {result['final_auc']:.4f}\n")
                f.write(f"   - Precision: {result['final_precision']:.4f}\n")
                f.write(f"   - Recall: {result['final_recall']:.4f}\n")
                f.write(f"   - Output: {result['output_dir']}\n\n")
        
        # T√¨m ki·∫øn tr√∫c t·ªët nh·∫•t cho t·ª´ng metric
        if all("error" not in r for r in results):
            f.write("\n" + "=" * 80 + "\n")
            f.write("KI·∫æN TR√öC T·ªêT NH·∫§T:\n")
            f.write("=" * 80 + "\n\n")
            
            metrics_to_compare = [
                ("final_accuracy", "Accuracy"),
                ("final_balanced_accuracy", "Balanced Accuracy"),
                ("final_f1", "F1-score"),
                ("final_agreement", "Agreement"),
            ]
            
            for metric_key, metric_name in metrics_to_compare:
                best_result = max(results, key=lambda x: x.get(metric_key, 0))
                f.write(f"{metric_name}: {best_result['architecture']} ({best_result[metric_key]:.4f})\n")
    
    # T·∫°o report Markdown
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o So S√°nh 3 Ki·∫øn Tr√∫c dualFFNN\n\n")
        f.write("## T√≥m T·∫Øt\n\n")
        f.write(f"- **Target model:** {target_model_name}\n")
        f.write(f"- **Dataset:** {args.dataset.upper()}\n")
        f.write(f"- **T·ªïng queries:** {total_queries:,}\n")
        f.write(f"- **S·ªë ki·∫øn tr√∫c:** {len(architectures)}\n\n")
        
        f.write("## B·∫£ng So S√°nh\n\n")
        f.write("| Ki·∫øn tr√∫c | Queries | Labels | Accuracy | Balanced Acc | F1 | Agreement | Threshold | AUC |\n")
        f.write("|-----------|---------|--------|----------|--------------|----|-----------|-----------|-----|\n")
        
        for result in results:
            if "error" not in result:
                auc_str = f"{result['final_auc']:.4f}" if not pd.isna(result['final_auc']) else "N/A"
                actual_queries = result.get('actual_queries_used', total_queries)
                balanced_acc = result.get('final_balanced_accuracy', 0.0)
                threshold = result.get('optimal_threshold', 0.5)
                f.write(f"| {result['architecture']} | {actual_queries:,} | {result['total_labels_used']:,} | "
                       f"{result['final_accuracy']:.4f} | {balanced_acc:.4f} | {result['final_f1']:.4f} | "
                       f"{result['final_agreement']:.4f} | {threshold:.3f} | {auc_str} |\n")
            else:
                f.write(f"| {result['architecture']} | ERROR | - | - | - | - | - | - | - |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng Ki·∫øn Tr√∫c\n\n")
        
        for result in results:
            if "error" not in result:
                f.write(f"### {result['architecture']}\n\n")
                f.write(f"**M√¥ t·∫£:** {result['description']}\n\n")
                f.write(f"- Query batch: {result['query_batch']:,}\n")
                f.write(f"- S·ªë rounds: {result['num_rounds']}\n")
                f.write(f"- Queries th·ª±c t·∫ø: {result.get('actual_queries_used', total_queries):,}\n")
                f.write(f"- T·ªïng labels s·ª≠ d·ª•ng: {result['total_labels_used']:,}\n\n")
                
                f.write("**Metrics:**\n\n")
                f.write(f"- Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"- Balanced Accuracy: {result['final_balanced_accuracy']:.4f} ({result['final_balanced_accuracy']*100:.2f}%)\n")
                f.write(f"- F1-score: {result['final_f1']:.4f}\n")
                f.write(f"- Optimal Threshold: {result['optimal_threshold']:.4f}\n")
                f.write(f"- Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"- AUC: {result['final_auc']:.4f}\n")
                f.write(f"- Precision: {result['final_precision']:.4f}\n")
                f.write(f"- Recall: {result['final_recall']:.4f}\n\n")
                
                f.write("**Files:**\n\n")
                f.write(f"- Metrics CSV: `{result['metrics_csv']}`\n")
                f.write(f"- Surrogate model: `{result['surrogate_model_path']}`\n")
                f.write(f"- Output directory: `{result['output_dir']}`\n\n")
    
    # L∆∞u JSON summary
    json_path = comparison_output_dir / "comparison_summary.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON summary: {json_path}")
    
    # In t√≥m t·∫Øt ra console
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    df_results = pd.DataFrame([r for r in results if "error" not in r])
    if not df_results.empty:
        df_display = df_results[["architecture", "final_accuracy", "final_balanced_accuracy", 
                                 "final_f1", "final_agreement", "optimal_threshold"]].copy()
        print(df_display.to_string(index=False))
    
    return results


if __name__ == "__main__":
    main()
