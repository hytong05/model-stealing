import json
import os
import sys
from pathlib import Path
import gc

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, balanced_accuracy_score
from sklearn.preprocessing import RobustScaler

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.attackers import KerasAttacker, LGBAttacker, KerasDualAttacker
from src.targets.oracle_client import LocalOracleClient, create_oracle_from_name
from src.sampling import entropy_sampling
from sklearn_extra.cluster import KMedoids


def _clip_scale(scaler: RobustScaler, X: np.ndarray) -> np.ndarray:
    """Scale data v·ªõi RobustScaler v√† clip v·ªÅ [-5, 5] gi·ªëng pipeline g·ªëc."""
    transformed = scaler.transform(X)
    return np.clip(transformed, -5, 5)


def _resolve_optional_path(path_str: str | None) -> Path | None:
    if path_str is None:
        return None
    path_obj = Path(path_str)
    if not path_obj.is_absolute():
        path_obj = PROJECT_ROOT / path_obj
    return path_obj.resolve()


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """L·∫•y danh s√°ch feature columns t·ª´ parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_data_from_parquet_stratified(
    parquet_path_label_0: str,
    parquet_path_label_1: str,
    feature_cols: list,
    label_col: str,
    take_rows: int = None,
    shuffle: bool = False,
    batch_size: int = 10000,
    seed: int = None,
) -> tuple:
    """
    Load d·ªØ li·ªáu t·ª´ 2 file ƒë√£ chia s·∫µn theo label (label_0 v√† label_1) v·ªõi stratified sampling.
    ƒê·∫£m b·∫£o c√¢n b·∫±ng class (50/50).
    """
    print(f"  üîÑ Loading t·ª´ 2 file ƒë√£ chia s·∫µn theo label (stratified)...")
    print(f"     Class 0: {parquet_path_label_0}")
    print(f"     Class 1: {parquet_path_label_1}")
    
    # Load t·ª´ m·ªói file
    X_0, y_0 = load_data_from_parquet(
        parquet_path_label_0, feature_cols, label_col, skip_rows=0, take_rows=None, shuffle=False, batch_size=batch_size, seed=None
    )
    X_1, y_1 = load_data_from_parquet(
        parquet_path_label_1, feature_cols, label_col, skip_rows=0, take_rows=None, shuffle=False, batch_size=batch_size, seed=None
    )
    
    print(f"  ‚úÖ Loaded: {len(X_0)} samples class 0, {len(X_1)} samples class 1")
    
    # Stratified sampling: L·∫•y 50% t·ª´ m·ªói class
    if take_rows is not None:
        samples_per_class = take_rows // 2
        rng = np.random.default_rng(seed)
        
        # Shuffle m·ªói class
        indices_0 = np.arange(len(X_0))
        indices_1 = np.arange(len(X_1))
        rng.shuffle(indices_0)
        rng.shuffle(indices_1)
        
        # L·∫•y samples_per_class t·ª´ m·ªói class
        selected_0 = indices_0[:min(samples_per_class, len(X_0))]
        selected_1 = indices_1[:min(samples_per_class, len(X_1))]
        
        # N·∫øu kh√¥ng ƒë·ªß t·ª´ m·ªôt class, l·∫•y th√™m t·ª´ class kia
        if len(selected_0) < samples_per_class:
            needed = samples_per_class - len(selected_0)
            selected_1 = indices_1[:min(samples_per_class + needed, len(X_1))]
        elif len(selected_1) < samples_per_class:
            needed = samples_per_class - len(selected_1)
            selected_0 = indices_0[:min(samples_per_class + needed, len(X_0))]
        
        X_0 = X_0[selected_0]
        y_0 = y_0[selected_0]
        X_1 = X_1[selected_1]
        y_1 = y_1[selected_1]
        
        print(f"  ‚úÖ Selected: {len(X_0)} samples class 0, {len(X_1)} samples class 1")
    
    # K·∫øt h·ª£p
    X_all = np.vstack([X_0, X_1])
    y_all = np.concatenate([y_0, y_1])
    
    # Shuffle n·∫øu c·∫ßn
    if shuffle:
        print(f"  üîÑ ƒêang shuffle {len(X_all):,} samples...")
        if seed is not None:
            rng = np.random.default_rng(seed)
            indices = rng.permutation(len(X_all))
        else:
            indices = np.random.permutation(len(X_all))
        X_all = X_all[indices]
        y_all = y_all[indices]
    
    return X_all, y_all


def load_data_from_parquet(
    parquet_path: str,
    feature_cols: list,
    label_col: str,
    skip_rows: int = 0,
    take_rows: int = None,
    shuffle: bool = False,
    batch_size: int = 10000,
    seed: int = None,
) -> tuple:
    """
    Load d·ªØ li·ªáu t·ª´ parquet file, lo·∫°i b·ªè label -1 v√† tr·∫£ v·ªÅ X, y.
    Gi·ªëng logic trong final_model.ipynb nh∆∞ng kh√¥ng normalize (s·∫Ω normalize sau).
    
    Args:
        seed: Random seed cho shuffle. N·∫øu None th√¨ d√πng np.random kh√¥ng c√≥ seed (kh√¥ng reproducible).
    """
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    all_y = []
    rows_seen = 0
    emitted = 0
    removed_total = 0
    batch_count = 0

    try:
        total_batches = (pq_file.metadata.num_rows + batch_size - 1) // batch_size

        for batch in pq_file.iter_batches(batch_size=batch_size, columns=feature_cols + [label_col]):
            batch_count += 1
            batch_len = len(batch)
            batch_start = rows_seen
            rows_seen += batch_len

            if rows_seen <= skip_rows:
                if batch_count % 50 == 0:
                    print(f"  ‚è≥ ƒê√£ x·ª≠ l√Ω {batch_count}/{total_batches} batches (ƒëang skip)...")
                continue

            batch_df = batch.to_pandas()

            if skip_rows > batch_start:
                start_idx = skip_rows - batch_start
                batch_df = batch_df.iloc[start_idx:]

            if label_col in batch_df.columns:
                label_series = batch_df[label_col]
            else:
                alt_cols = [col for col in batch_df.columns if col.lower() == label_col.lower()]
                if alt_cols:
                    label_series = batch_df[alt_cols[0]]
                else:
                    raise KeyError(
                        f"Label column '{label_col}' kh√¥ng t·ªìn t·∫°i. C√°c c·ªôt: {list(batch_df.columns)[:5]}..."
                    )

            # Lo·∫°i b·ªè label -1 (unlabeled)
            valid_mask = label_series != -1
            if not np.any(valid_mask):
                removed_total += len(label_series)
                del batch_df, label_series
                gc.collect()
                continue

            removed_total += int(np.sum(~valid_mask))
            batch_df = batch_df[valid_mask]
            label_series = label_series[valid_mask]

            if take_rows is not None:
                remaining = take_rows - emitted
                if remaining <= 0:
                    break
                if len(batch_df) > remaining:
                    batch_df = batch_df.iloc[:remaining]
                    label_series = label_series.iloc[:remaining]

            if batch_df.empty:
                del batch_df, label_series
                gc.collect()
                continue

            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            y = label_series.values.astype(np.int32)

            all_X.append(X)
            all_y.append(y)
            emitted += len(X)

            if batch_count % 20 == 0:
                del batch_df, label_series
                gc.collect()
                if take_rows is None:
                    print(f"  ‚è≥ ƒê√£ x·ª≠ l√Ω {batch_count}/{total_batches} batches, loaded {emitted:,} samples...")
                else:
                    print(
                        f"  ‚è≥ ƒê√£ x·ª≠ l√Ω {batch_count}/{total_batches} batches, loaded {emitted:,}/{take_rows:,} samples..."
                    )
            else:
                del batch_df, label_series

            if take_rows is not None and emitted >= take_rows:
                break

        if all_X:
            X_concat = np.concatenate(all_X, axis=0)
            y_concat = np.concatenate(all_y, axis=0)
            del all_X, all_y
            gc.collect()
        else:
            X_concat = np.empty((0, len(feature_cols)), dtype=np.float32)
            y_concat = np.empty((0,), dtype=np.int32)

        if shuffle and len(X_concat) > 0:
            print(f"  üîÑ ƒêang shuffle {len(X_concat):,} samples...")
            if seed is not None:
                rng = np.random.default_rng(seed)
                indices = rng.permutation(len(X_concat))
            else:
                indices = np.random.permutation(len(X_concat))
            X_concat = X_concat[indices]
            y_concat = y_concat[indices]

        if removed_total > 0:
            print(f"  ‚ö†Ô∏è  ƒê√£ lo·∫°i b·ªè {removed_total:,} samples c√≥ label -1 (unlabeled)")

        return X_concat, y_concat
    finally:
        del pq_file
        gc.collect()


def run_extraction(
    output_dir: Path,
    train_parquet: str = None,
    test_parquet: str = None,
    dataset: str = "ember",  # "ember" ho·∫∑c "somlap" - dataset ƒë·ªÉ t·∫•n c√¥ng
    seed: int = 42,
    feature_dim: int = 2381,
    seed_size: int = 2000,
    val_size: int = 2000,
    eval_size: int = 4000,
    query_batch: int = 2000,
    num_rounds: int = 5,
    num_epochs: int = 5,
    model_type: str = "h5",  # "h5" ho·∫∑c "lgb" - ch·ªâ c·∫ßn n·∫øu d√πng weights_path
    normalization_stats_path: str = None,  # Ch·ªâ c·∫ßn n·∫øu d√πng weights_path v·ªõi model_type="lgb"
    attacker_type: str = None,  # "keras", "lgb", ho·∫∑c "dual" (dualDNN), None ƒë·ªÉ t·ª± ƒë·ªông ch·ªçn theo model_type
    weights_path: str | None = None,
    model_name: str = None,  # T√™n model (CEE, LEE, CSE, LSE) - ∆∞u ti√™n h∆°n weights_path
    threshold_optimization_metric: str = "f1",  # "f1", "accuracy", "balanced_accuracy" - metric ƒë·ªÉ t·ªëi ∆∞u threshold
    fixed_threshold: float | None = None,  # N·∫øu kh√¥ng None, s·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh thay v√¨ t·ªëi ∆∞u
    surrogate_dir: str | None = None,  # Cho ph√©p override th∆∞ m·ª•c l∆∞u surrogate
    surrogate_name: str | None = None,  # Cho ph√©p override t√™n file surrogate (kh√¥ng extension)
) -> dict:
    output_dir = Path(output_dir)
    rng = np.random.default_rng(seed)
    pool_exhausted_flag = False
    over_budget_flag = False

    # Ch·ªâ set TF environment variables n·∫øu d√πng Keras model
    if model_type == "h5" or attacker_type in ["keras", "dual"]:
        os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
        os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")

    # X√°c ƒë·ªãnh label column d·ª±a tr√™n dataset
    dataset = dataset.lower()
    if dataset == "ember":
        label_col = "Label"
    elif dataset == "somlap":
        label_col = "class"
    else:
        raise ValueError(f"Dataset kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {dataset}. Ch·ªçn 'ember' ho·∫∑c 'somlap'")
    
    # Auto-detect attacker_type n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if attacker_type is None:
        attacker_type = "keras" if model_type == "h5" else "lgb"

    # Debug: Log gi√° tr·ªã train_parquet v√† test_parquet tr∆∞·ªõc khi x·ª≠ l√Ω
    print(f"\nüîç DEBUG: dataset={dataset}, train_parquet={train_parquet}, test_parquet={test_parquet}")

    # Load d·ªØ li·ªáu t·ª´ parquet files (EMBER ho·∫∑c SOMLAP)
    # QUAN TR·ªåNG: N·∫øu train_parquet ho·∫∑c test_parquet ƒë√£ ƒë∆∞·ª£c set (kh√¥ng ph·∫£i None),
    # c·∫ßn ƒë·∫£m b·∫£o ch√∫ng ph√π h·ª£p v·ªõi dataset ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if train_parquet is not None or test_parquet is not None:
        # N·∫øu ƒë√£ ƒë∆∞·ª£c set, c·∫ßn validate xem c√≥ kh·ªõp v·ªõi dataset kh√¥ng
        if train_parquet is not None:
            train_path = Path(train_parquet)
            # Ki·ªÉm tra xem file c√≥ ph·∫£i l√† EMBER file khi dataset l√† somlap kh√¥ng
            if dataset == "somlap" and ("ember" in str(train_path).lower() or "ember_2018" in str(train_path)):
                print(f"‚ö†Ô∏è  WARNING: train_parquet ƒë∆∞·ª£c set l√† EMBER file nh∆∞ng dataset l√† SOMLAP!")
                print(f"   ‚ö†Ô∏è  ƒêang b·ªè qua train_parquet v√† s·ª≠ d·ª•ng dataset parameter ƒë·ªÉ ch·ªçn file ƒë√∫ng")
                train_parquet = None
            elif dataset == "ember" and "somlap" in str(train_path).lower():
                print(f"‚ö†Ô∏è  WARNING: train_parquet ƒë∆∞·ª£c set l√† SOMLAP file nh∆∞ng dataset l√† EMBER!")
                print(f"   ‚ö†Ô∏è  ƒêang b·ªè qua train_parquet v√† s·ª≠ d·ª•ng dataset parameter ƒë·ªÉ ch·ªçn file ƒë√∫ng")
                train_parquet = None
        
        if test_parquet is not None:
            test_path = Path(test_parquet)
            # Ki·ªÉm tra xem file c√≥ ph·∫£i l√† EMBER file khi dataset l√† somlap kh√¥ng
            if dataset == "somlap" and ("ember" in str(test_path).lower() or "ember_2018" in str(test_path)):
                print(f"‚ö†Ô∏è  WARNING: test_parquet ƒë∆∞·ª£c set l√† EMBER file nh∆∞ng dataset l√† SOMLAP!")
                print(f"   ‚ö†Ô∏è  ƒêang b·ªè qua test_parquet v√† s·ª≠ d·ª•ng dataset parameter ƒë·ªÉ ch·ªçn file ƒë√∫ng")
                test_parquet = None
            elif dataset == "ember" and "somlap" in str(test_path).lower():
                print(f"‚ö†Ô∏è  WARNING: test_parquet ƒë∆∞·ª£c set l√† SOMLAP file nh∆∞ng dataset l√† EMBER!")
                print(f"   ‚ö†Ô∏è  ƒêang b·ªè qua test_parquet v√† s·ª≠ d·ª•ng dataset parameter ƒë·ªÉ ch·ªçn file ƒë√∫ng")
                test_parquet = None
    
    if train_parquet is None:
        if dataset == "ember":
            # EMBER dataset: Th·ª≠ d√πng file ƒë√£ chia s·∫µn theo label tr∆∞·ªõc
            train_parquet_label_0 = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other_label_0.parquet")
            train_parquet_label_1 = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other_label_1.parquet")
            # Fallback v·ªÅ file c≈© n·∫øu kh√¥ng c√≥ file m·ªõi
            train_parquet_old = str(PROJECT_ROOT / "data" / "train_ember_2018_v2_features_label_other.parquet")
            if Path(train_parquet_label_0).exists() and Path(train_parquet_label_1).exists():
                train_parquet = None  # S·∫Ω d√πng stratified load t·ª´ 2 file
            elif Path(train_parquet_old).exists():
                train_parquet = train_parquet_old
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y EMBER train data t·∫°i: {train_parquet_label_0} ho·∫∑c {train_parquet_old}")
        elif dataset == "somlap":
            # SOMLAP dataset
            train_parquet_path = PROJECT_ROOT / "data" / "SOMLAP" / "SOMLAP DATASET_train.parquet"
            if train_parquet_path.exists():
                train_parquet = str(train_parquet_path)
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y SOMLAP train data t·∫°i: {train_parquet_path}")
    
    if test_parquet is None:
        if dataset == "ember":
            test_parquet_new = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "test" / "test_ember_2018_v2_features_label_other.parquet")
            test_parquet_old = str(PROJECT_ROOT / "data" / "test_ember_2018_v2_features_label_other.parquet")
            if Path(test_parquet_new).exists():
                test_parquet = test_parquet_new
            elif Path(test_parquet_old).exists():
                test_parquet = test_parquet_old
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y EMBER test data t·∫°i: {test_parquet_new} ho·∫∑c {test_parquet_old}")
        elif dataset == "somlap":
            test_parquet_path = PROJECT_ROOT / "data" / "SOMLAP" / "SOMLAP DATASET_test.parquet"
            if test_parquet_path.exists():
                test_parquet = str(test_parquet_path)
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y SOMLAP test data t·∫°i: {test_parquet_path}")

    print("=" * 60)
    print(f"üìä ƒêang load d·ªØ li·ªáu {dataset.upper()}...")
    print("=" * 60)
    print(f"Dataset: {dataset.upper()}")
    print(f"Label column: {label_col}")
    print(f"Train file: {train_parquet if train_parquet else '(s·∫Ω t·ª± ƒë·ªông ch·ªçn d·ª±a tr√™n dataset)'}")
    print(f"Test file: {test_parquet if test_parquet else '(s·∫Ω t·ª± ƒë·ªông ch·ªçn d·ª±a tr√™n dataset)'}")

    # L·∫•y feature columns v√† x√°c ƒë·ªãnh feature_dim th·ª±c t·∫ø
    # N·∫øu train_parquet l√† None (d√πng stratified load t·ª´ 2 file - ch·ªâ EMBER), d√πng m·ªôt trong 2 file ho·∫∑c test_parquet
    if train_parquet is None:
        # Ch·ªâ c√≥ th·ªÉ None v·ªõi EMBER dataset (stratified loading)
        train_parquet_label_0 = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other_label_0.parquet")
        if Path(train_parquet_label_0).exists():
            feature_cols = get_feature_columns(train_parquet_label_0, label_col)
        elif test_parquet is not None:
            feature_cols = get_feature_columns(test_parquet, label_col)
        else:
            raise ValueError("Kh√¥ng th·ªÉ l·∫•y feature columns: train_parquet l√† None v√† kh√¥ng c√≥ file label_0 ho·∫∑c test_parquet")
    else:
        feature_cols = get_feature_columns(train_parquet, label_col)
    
    # T·ª± ƒë·ªông detect feature_dim t·ª´ dataset n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh ho·∫∑c kh√¥ng kh·ªõp
    actual_feature_dim = len(feature_cols)
    print(f"Feature columns: {actual_feature_dim}")
    if feature_dim != actual_feature_dim:
        print(f"‚ö†Ô∏è  Feature dimension mismatch:")
        print(f"   - Specified feature_dim: {feature_dim}")
        print(f"   - Actual feature_dim from {dataset.upper()} dataset: {actual_feature_dim}")
        print(f"   ‚úÖ T·ª± ƒë·ªông s·ª≠ d·ª•ng actual feature_dim: {actual_feature_dim}")
        feature_dim = actual_feature_dim
    
    # QUAN TR·ªåNG: Validate v√† log th√¥ng tin target model
    oracle_source = None
    required_feature_dim = None
    oracle_client = None
    model_file_name = None
    
    # ∆Øu ti√™n s·ª≠ d·ª•ng model_name n·∫øu ƒë∆∞·ª£c cung c·∫•p
    if model_name is not None:
        print(f"\nüîÑ Kh·ªüi t·∫°o target model t·ª´ t√™n: {model_name.upper()}")
        print(f"   ‚ÑπÔ∏è  S·∫Ω t·ª± ƒë·ªông detect model type v√† t√¨m normalization stats...")
        print(f"   üîí BLACK BOX: Attacker ch·ªâ bi·∫øt t√™n model, kh√¥ng bi·∫øt implementation details")
        
        # S·ª≠ d·ª•ng create_oracle_from_name - t·ª± ƒë·ªông detect m·ªçi th·ª©
        # Tr·∫£ v·ªÅ BlackBoxOracleClient ƒë·ªÉ ·∫©n implementation details
        oracle_client = create_oracle_from_name(
            model_name=model_name,
            threshold=0.5,
            feature_dim=feature_dim,
        )
        
        # L·∫•y th√¥ng tin t·ª´ oracle client (ch·ªâ ƒë·ªÉ logging, kh√¥ng d√πng trong attack)
        # Trong black box attack th·ª±c t·∫ø, attacker kh√¥ng n√™n bi·∫øt nh·ªØng th√¥ng tin n√†y
        # Nh∆∞ng ƒë·ªÉ logging/debugging, v·∫´n l·∫•y t·ª´ internal oracle
        if hasattr(oracle_client, '_oracle'):
            internal_oracle = oracle_client._oracle
            weights_path_abs = internal_oracle.model_path
            model_type = internal_oracle.model_type
            # Khi d√πng model_name, normalization_stats_path ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông t√¨m v√† truy·ªÅn v√†o oracle
            # Kh√¥ng c·∫ßn ki·ªÉm tra l·∫°i ·ªü ƒë√¢y
            normalization_stats_path = "auto-detected"  # ƒê√°nh d·∫•u ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông detect
        else:
            # Fallback n·∫øu kh√¥ng c√≥ _oracle (tr∆∞·ªùng h·ª£p d√πng LocalOracleClient tr·ª±c ti·∫øp)
            weights_path_abs = oracle_client.model_path
            model_type = oracle_client.model_type
            normalization_stats_path = getattr(oracle_client, 'normalization_stats_path', None)
        
        model_file_name = Path(weights_path_abs).name
        model_file_size = Path(weights_path_abs).stat().st_size / (1024 * 1024)  # MB
        oracle_source = weights_path_abs
        
        print(f"   ‚úÖ Target model file: {model_file_name}")
        print(f"   ‚úÖ Model path (absolute): {weights_path_abs}")
        print(f"   ‚úÖ Model type: {model_type.upper()}")
        print(f"   ‚úÖ Model size: {model_file_size:.2f} MB")
        print(f"   ‚ö†Ô∏è  L∆ØU √ù: Th√¥ng tin tr√™n ch·ªâ ƒë·ªÉ logging, attacker kh√¥ng n√™n bi·∫øt trong black box attack th·ª±c t·∫ø")
        
        required_feature_dim = oracle_client.get_required_feature_dim()
    else:
        # S·ª≠ d·ª•ng c√°ch c≈© v·ªõi weights_path
        if weights_path is None:
            raise ValueError("Ph·∫£i cung c·∫•p weights_path ho·∫∑c model_name cho oracle module.")
        weights_path_abs = str(Path(weights_path).resolve())
        if not Path(weights_path_abs).exists():
            raise FileNotFoundError(f"‚ùå Target model kh√¥ng t·ªìn t·∫°i: {weights_path_abs}")
        
        model_file_name = Path(weights_path_abs).name
        model_file_size = Path(weights_path_abs).stat().st_size / (1024 * 1024)  # MB
        oracle_source = weights_path_abs
        
        print(f"\nüîÑ Kh·ªüi t·∫°o target model ({model_type.upper()}) v·ªõi feature_dim={feature_dim}...")
        print(f"   ‚úÖ Target model file: {model_file_name}")
        print(f"   ‚úÖ Model path (absolute): {weights_path_abs}")
        print(f"   ‚úÖ Model size: {model_file_size:.2f} MB")
        
        if weights_path != weights_path_abs:
            print(f"   ‚ö†Ô∏è  Path ƒë∆∞·ª£c resolve: {weights_path} -> {weights_path_abs}")
        
        if model_type == "lgb":
            if normalization_stats_path is None:
                raise ValueError(
                    "normalization_stats_path ph·∫£i ƒë∆∞·ª£c cung c·∫•p khi model_type='lgb'. "
                    "Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n t·ªõi file normalization_stats.npz"
                )
            if isinstance(normalization_stats_path, str):
                stats_path_abs = str(Path(normalization_stats_path).resolve())
                if not Path(stats_path_abs).exists():
                    raise FileNotFoundError(f"‚ùå Normalization stats kh√¥ng t·ªìn t·∫°i: {stats_path_abs}")
                normalization_stats_path = stats_path_abs
            
            print(f"   ‚úÖ Normalization stats file: {Path(normalization_stats_path).name}")
            print(f"   ‚úÖ Stats path (absolute): {normalization_stats_path}")
        else:
            normalization_stats_path = None
        
        # T·∫°o oracle client v·ªõi weights_path (c√°ch c≈©)
        oracle_client = LocalOracleClient(
            model_type=model_type,
            model_path=weights_path_abs,
            normalization_stats_path=normalization_stats_path,
            threshold=0.5,
            feature_dim=feature_dim,
        )
        required_feature_dim = oracle_client.get_required_feature_dim()
    required_feature_dim = oracle_client.get_required_feature_dim()
    
    if required_feature_dim is None:
        print(f"   ‚úÖ Target model c√≥ preprocessing layer - s·∫Ω t·ª± ƒë·ªông map t·ª´ {feature_dim} ƒë·∫∑c tr∆∞ng")
    else:
        print(f"   ‚úÖ Target model y√™u c·∫ßu {required_feature_dim} ƒë·∫∑c tr∆∞ng")
        if feature_dim > required_feature_dim:
            print(f"   ‚ö†Ô∏è  Dataset c√≥ {feature_dim} ƒë·∫∑c tr∆∞ng, s·∫Ω t·ª± ƒë·ªông c·∫Øt b·ªè {feature_dim - required_feature_dim} ƒë·∫∑c tr∆∞ng th·ª´a")
            print(f"      (Gi·ªØ {required_feature_dim} features ƒë·∫ßu ti√™n)")
        elif feature_dim < required_feature_dim:
            print(f"   ‚ö†Ô∏è  Dataset c√≥ {feature_dim} ƒë·∫∑c tr∆∞ng, nh∆∞ng target model y√™u c·∫ßu {required_feature_dim} ƒë·∫∑c tr∆∞ng")
            print(f"   ‚úÖ S·∫Ω t·ª± ƒë·ªông PADDING th√™m {required_feature_dim - feature_dim} ƒë·∫∑c tr∆∞ng (zeros) tr∆∞·ªõc khi query oracle")
            print(f"      L∆∞u √Ω: Padding c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô ch√≠nh x√°c c·ªßa attack")

    # QUAN TR·ªåNG: ƒê·∫£m b·∫£o seed/val sets gi·ªëng nhau gi·ªØa c√°c configs
    # Gi·∫£i ph√°p: Load ƒë·ªß l·ªõn (seed_val + pool l·ªõn nh·∫•t), shuffle v·ªõi seed, sau ƒë√≥ chia
    # T√≠nh pool l·ªõn nh·∫•t c·∫ßn thi·∫øt trong c√°c configs (ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng thi·∫øu d·ªØ li·ªáu)
    # V·ªõi c·∫•u h√¨nh hi·ªán t·∫°i: max_queries_10000 c√≥ query_batch=2000, num_rounds=5 => pool c·∫ßn 10000
    # QUAN TR·ªåNG: Th√™m buffer 20% ƒë·ªÉ ƒë·∫£m b·∫£o KH√îNG BAO GI·ªú thi·∫øu queries
    # TƒÉng buffer t·ª´ 10% l√™n 20% ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß pool cho class balancing
    max_pool_needed_base = query_batch * num_rounds
    max_pool_needed = int(max_pool_needed_base * 1.2)  # Buffer 20%
    seed_val_size = seed_size + val_size
    total_needed = seed_val_size + max_pool_needed
    
    print(f"\nüîÑ ƒêang load train data ({total_needed:,} samples: {seed_val_size:,} seed+val + {max_pool_needed:,} pool)...")
    
    # Load train data - x·ª≠ l√Ω kh√°c nhau cho EMBER v√† SOMLAP
    if dataset == "ember":
        # EMBER: C·∫¢I TI·∫æN: S·ª≠ d·ª•ng file ƒë√£ chia s·∫µn theo label n·∫øu c√≥
        train_parquet_label_0 = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other_label_0.parquet")
        train_parquet_label_1 = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_other_label_1.parquet")
        
        if Path(train_parquet_label_0).exists() and Path(train_parquet_label_1).exists():
            # S·ª≠ d·ª•ng file ƒë√£ chia s·∫µn theo label (stratified loading)
            # QUAN TR·ªåNG: Load c·∫£ ground truth labels t·ª´ train data (kh√¥ng query oracle!)
            X_train_all, y_train_all_gt = load_data_from_parquet_stratified(
                train_parquet_label_0, train_parquet_label_1, feature_cols, label_col,
                take_rows=total_needed, shuffle=True, seed=seed
            )
            print(f"‚úÖ Train data loaded (stratified): {X_train_all.shape}")
        else:
            # Fallback: D√πng file c≈©
            if train_parquet is None:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y EMBER train data files. ƒê√£ t√¨m t·∫°i:\n  - {train_parquet_label_0}\n  - {train_parquet_label_1}")
            X_train_all, y_train_all_gt = load_data_from_parquet(
                train_parquet, feature_cols, label_col, skip_rows=0, take_rows=total_needed, shuffle=True, seed=seed
            )
            print(f"‚úÖ Train data loaded: {X_train_all.shape}")
    elif dataset == "somlap":
        # SOMLAP: Ch·ªâ c√≥ 1 file duy nh·∫•t, kh√¥ng c√≥ stratified files
        if train_parquet is None:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y SOMLAP train data file")
        X_train_all, y_train_all_gt = load_data_from_parquet(
            train_parquet, feature_cols, label_col, skip_rows=0, take_rows=total_needed, shuffle=True, seed=seed
        )
        print(f"‚úÖ Train data loaded: {X_train_all.shape}")
    else:
        raise ValueError(f"Dataset kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {dataset}")

    train_dist = dict(zip(*np.unique(y_train_all_gt, return_counts=True)))
    print(f"   üìä Train data distribution (ground truth): {train_dist}")

    # C·∫¢I TI·∫æN: Stratified split cho Seed v√† Val ƒë·ªÉ c√¢n b·∫±ng class
    # S·ª≠ d·ª•ng ground truth labels t·ª´ train data (KH√îNG query oracle!)
    print(f"\nüîÑ Chia Seed v√† Val v·ªõi stratified sampling (c√¢n b·∫±ng class, d√πng ground truth labels)...")
    rng = np.random.default_rng(seed)
    
    # T√°ch indices theo class (d√πng ground truth labels)
    class_0_indices = np.where(y_train_all_gt == 0)[0]
    class_1_indices = np.where(y_train_all_gt == 1)[0]
    
    # Shuffle m·ªói class
    rng.shuffle(class_0_indices)
    rng.shuffle(class_1_indices)
    
    # Chia seed: 50% t·ª´ m·ªói class
    seed_per_class = seed_size // 2
    seed_class_0_idx = class_0_indices[:seed_per_class]
    seed_class_1_idx = class_1_indices[:min(seed_per_class, len(class_1_indices))]
    
    # N·∫øu kh√¥ng ƒë·ªß class 1, l·∫•y th√™m t·ª´ class 0
    if len(seed_class_1_idx) < seed_per_class:
        needed = seed_per_class - len(seed_class_1_idx)
        seed_class_0_idx = np.concatenate([seed_class_0_idx, class_0_indices[seed_per_class:seed_per_class+needed]])
    
    seed_indices = np.concatenate([seed_class_0_idx, seed_class_1_idx])
    rng.shuffle(seed_indices)  # Shuffle l·∫°i ƒë·ªÉ tr·ªôn classes
    
    # C·∫≠p nh·∫≠t class indices (lo·∫°i b·ªè ƒë√£ d√πng cho seed)
    class_0_indices = class_0_indices[len(seed_class_0_idx):]
    class_1_indices = class_1_indices[len(seed_class_1_idx):]
    
    # Chia val: 50% t·ª´ m·ªói class (t·ª´ ph·∫ßn c√≤n l·∫°i)
    val_per_class = val_size // 2
    val_class_0_idx = class_0_indices[:val_per_class]
    val_class_1_idx = class_1_indices[:min(val_per_class, len(class_1_indices))]
    
    # N·∫øu kh√¥ng ƒë·ªß class 1, l·∫•y th√™m t·ª´ class 0
    if len(val_class_1_idx) < val_per_class:
        needed = val_per_class - len(val_class_1_idx)
        val_class_0_idx = np.concatenate([val_class_0_idx, class_0_indices[val_per_class:val_per_class+needed]])
    
    val_indices = np.concatenate([val_class_0_idx, val_class_1_idx])
    rng.shuffle(val_indices)  # Shuffle l·∫°i ƒë·ªÉ tr·ªôn classes
    
    # L·∫•y seed v√† val
    X_seed = X_train_all[seed_indices]
    X_val = X_train_all[val_indices]
    
    # Ph·∫ßn c√≤n l·∫°i l√†m pool
    used_indices = np.concatenate([seed_indices, val_indices])
    pool_indices_all = np.setdiff1d(np.arange(len(X_train_all)), used_indices)

    # QUAN TR·ªåNG: Pool size ph·∫£i ƒë·ªß cho total queries + d∆∞ 20% ƒë·ªÉ ƒë·∫£m b·∫£o KH√îNG BAO GI·ªú thi·∫øu
    # Do class balancing c√≥ th·ªÉ th√™m queries, v√† c·∫ßn buffer l·ªõn ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß queries
    pool_needed_base = query_batch * num_rounds
    pool_needed = int(pool_needed_base * 1.2)  # D∆∞ 20% ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß queries (tƒÉng t·ª´ 10% l√™n 20%)
    
    # Ki·ªÉm tra xem c√≥ ƒë·ªß data kh√¥ng
    available_pool = len(pool_indices_all)
    if available_pool < pool_needed:
        # N·∫øu kh√¥ng ƒë·ªß data cho pool v·ªõi buffer, v·∫´n c·ªë g·∫Øng l·∫•y √≠t nh·∫•t pool_needed_base
        if available_pool < pool_needed_base:
            print(f"   ‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng ƒë·ªß data cho pool!")
            print(f"   ‚ùå Available: {available_pool:,}, Required: {pool_needed_base:,}")
            print(f"   ‚ùå Pool s·∫Ω c·∫°n ki·ªát v√† queries s·∫Ω thi·∫øu!")
            raise ValueError(
                f"Kh√¥ng ƒë·ªß data cho pool! Available: {available_pool:,}, "
                f"Required: {pool_needed_base:,} (query_batch={query_batch:,} √ó num_rounds={num_rounds})"
            )
        else:
            print(f"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng ƒë·ªß data cho pool v·ªõi buffer ({available_pool:,} < {pool_needed:,})")
            print(f"   üí° S·∫Ω d√πng t·ªëi ƒëa {available_pool:,} samples cho pool (thi·∫øu buffer)")
            pool_needed = available_pool
    
    # L·∫•y pool t·ª´ indices
    pool_indices = pool_indices_all[:pool_needed]
    X_pool = X_train_all[pool_indices]
    # QUAN TR·ªåNG: L∆∞u labels c·ªßa pool t·ª´ thief dataset ƒë·ªÉ pre-filtering
    y_pool_gt = y_train_all_gt[pool_indices]  # Ground truth labels c·ªßa pool t·ª´ thief dataset
    buffer_size = pool_needed - pool_needed_base
    
    # Log distribution (ground truth)
    seed_dist_gt = dict(zip(*np.unique(y_train_all_gt[seed_indices], return_counts=True)))
    val_dist_gt = dict(zip(*np.unique(y_train_all_gt[val_indices], return_counts=True)))
    pool_dist_gt = dict(zip(*np.unique(y_pool_gt, return_counts=True)))
    print(f"   ‚úÖ Seed distribution (stratified, ground truth): {seed_dist_gt}")
    print(f"   ‚úÖ Val distribution (stratified, ground truth): {val_dist_gt}")
    print(f"   ‚úÖ Pool distribution (ground truth from thief dataset): {pool_dist_gt}")
    print(f"   ‚úÖ Pool size: {X_pool.shape[0]:,} samples")
    print(f"      - Target: {pool_needed_base:,} (query_batch={query_batch:,} √ó num_rounds={num_rounds})")
    print(f"      - Buffer: +{buffer_size:,} ({buffer_size/pool_needed_base*100:.1f}%) ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng thi·∫øu queries")
    del X_train_all
    gc.collect()

    # Load eval set t·ª´ test file
    # QUAN TR·ªåNG: Load c·∫£ ground truth labels ƒë·ªÉ t√≠nh accuracy ch√≠nh x√°c
    print(f"\nüîÑ ƒêang load eval set ({eval_size:,} samples)...")
    # Test data c√≥ th·ªÉ d√πng file c≈© ho·∫∑c file m·ªõi
    X_eval, y_eval_gt = load_data_from_parquet(
        test_parquet, feature_cols, label_col, skip_rows=0, take_rows=eval_size, shuffle=True, seed=seed
    )
    print(f"‚úÖ Eval set: {X_eval.shape}")
    print(f"‚úÖ Ground truth labels: {y_eval_gt.shape}")

    print(f"\nüìä Data split:")
    print(f"  Seed: {X_seed.shape[0]:,}")
    print(f"  Val: {X_val.shape[0]:,}")
    print(f"  Pool: {X_pool.shape[0]:,}")
    print(f"  Eval: {X_eval.shape[0]:,}")

    # QUAN TR·ªåNG: X·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc khi query oracle
    # QUAN TR·ªåNG: Scale data d·ª±a tr√™n MODEL_TYPE C·ª¶A ORACLE (target model), KH√îNG ph·∫£i attacker_type!
    # - V·ªõi Keras/H5 Oracle: C·∫ßn scale data v·ªõi RobustScaler (model ƒë∆∞·ª£c train v·ªõi scaled data)
    # - V·ªõi LightGBM Oracle: FlexibleLGBTarget s·∫Ω t·ª± ƒë·ªông normalize n·∫øu c√≥ normalization_stats_path
    #   KH√îNG ƒë∆∞·ª£c scale v·ªõi RobustScaler - ch·ªâ c·∫ßn raw data!
    # - attacker_type ch·ªâ ·∫£nh h∆∞·ªüng ƒë·∫øn c√°ch train surrogate model, kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn c√°ch query oracle
    scaler = None
    X_eval_s = None
    X_seed_s = None
    X_val_s = None
    X_pool_s = None
    
    # L·∫•y model_type th·ª±c t·∫ø c·ªßa oracle (kh√¥ng ph·∫£i attacker_type)
    oracle_model_type = model_type  # N·∫øu d√πng model_name, model_type ƒë√£ ƒë∆∞·ª£c detect t·ª´ oracle_client
    
    if oracle_model_type == "h5":
        # Keras/H5 Oracle: C·∫ßn scale data v·ªõi RobustScaler
        print(f"\nüîÑ ƒêang scale d·ªØ li·ªáu tr∆∞·ªõc khi query oracle (Keras/H5 Oracle c·∫ßn scaled data)...")
        scaler = RobustScaler()
        scaler.fit(np.vstack([X_seed, X_val, X_pool]))

        X_eval_s = _clip_scale(scaler, X_eval)
        X_seed_s = _clip_scale(scaler, X_seed)
        X_val_s = _clip_scale(scaler, X_val)
        X_pool_s = _clip_scale(scaler, X_pool)
        
        print(f"‚úÖ ƒê√£ scale d·ªØ li·ªáu")
        print(f"   - X_eval_s shape: {X_eval_s.shape}")
        print(f"   - X_seed_s shape: {X_seed_s.shape}")
        print(f"   - X_val_s shape: {X_val_s.shape}")
        print(f"   - X_pool_s shape: {X_pool_s.shape}")
        
        # L·∫•y nh√£n t·ª´ oracle (V·ªöI D·ªÆ LI·ªÜU ƒê√É SCALE)
        print(f"\nüîÑ ƒêang l·∫•y nh√£n t·ª´ oracle (v·ªõi d·ªØ li·ªáu ƒë√£ scale cho Keras Oracle)...")
        y_eval = oracle_client.predict(X_eval_s)
        y_seed = oracle_client.predict(X_seed_s)
        y_val = oracle_client.predict(X_val_s)
    else:
        # LightGBM Oracle: KH√îNG scale v·ªõi RobustScaler - ch·ªâ c·∫ßn raw data
        # FlexibleLGBTarget s·∫Ω t·ª± ƒë·ªông normalize v·ªõi normalization_stats_path
        print(f"\nüîÑ ƒêang l·∫•y nh√£n t·ª´ oracle (LightGBM Oracle - KH√îNG scale, s·∫Ω t·ª± ƒë·ªông normalize)...")
        y_eval = oracle_client.predict(X_eval)
        y_seed = oracle_client.predict(X_seed)
        y_val = oracle_client.predict(X_val)
        
        # V·ªõi LightGBM Oracle, KH√îNG scale data - d√πng raw data
        X_eval_s = X_eval
        X_seed_s = X_seed
        X_val_s = X_val
        X_pool_s = X_pool
        
        # N·∫øu attacker_type l√† keras/dual (c·∫ßn scaled data cho training), 
        # c·∫ßn scale ri√™ng cho surrogate model training sau n√†y
        if attacker_type in ["keras", "dual"]:
            print(f"\n‚ö†Ô∏è  L∆ØU √ù: Oracle l√† LightGBM (raw data), nh∆∞ng surrogate l√† {attacker_type} (c·∫ßn scaled data)")
            print(f"   üîÑ S·∫Ω scale data ri√™ng cho surrogate model training sau...")
            scaler = RobustScaler()
            scaler.fit(np.vstack([X_seed, X_val, X_pool]))
            # T·∫°o scaled version cho surrogate training
            X_eval_s = _clip_scale(scaler, X_eval)
            X_seed_s = _clip_scale(scaler, X_seed)
            X_val_s = _clip_scale(scaler, X_val)
            X_pool_s = _clip_scale(scaler, X_pool)
    print(f"‚úÖ Oracle labels retrieved")
    eval_dist = dict(zip(*np.unique(y_eval, return_counts=True)))
    seed_dist = dict(zip(*np.unique(y_seed, return_counts=True)))
    val_dist = dict(zip(*np.unique(y_val, return_counts=True)))
    print(f"  Eval distribution: {eval_dist}")
    print(f"  Seed distribution: {seed_dist}")
    print(f"  Val distribution: {val_dist}")
    
    # QUAN TR·ªåNG: ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa oracle v·ªõi ground truth
    # ƒêi·ªÅu n√†y gi√∫p gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa val_accuracy (vs oracle) v√† final accuracy (vs ground truth)
    oracle_acc_vs_gt = accuracy_score(y_eval_gt, y_eval)
    print(f"\nüìä ƒê√°nh gi√° Oracle (Target Model):")
    print(f"   Oracle accuracy vs Ground Truth: {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
    print(f"   ‚ö†Ô∏è  L∆ØU √ù: Val accuracy trong training ƒë∆∞·ª£c t√≠nh v·ªõi oracle labels (kh√¥ng ph·∫£i ground truth)")
    print(f"   ‚ö†Ô∏è  Final accuracy ƒë∆∞·ª£c t√≠nh v·ªõi ground truth labels")
    print(f"   üí° N·∫øu oracle kh√¥ng ch√≠nh x√°c 100%, s·∫Ω c√≥ s·ª± kh√°c bi·ªát gi·ªØa val_accuracy v√† final accuracy")
    
    # KI·ªÇM TRA: N·∫øu oracle predict t·∫•t c·∫£ l√† m·ªôt class, c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ
    all_distributions = [eval_dist, seed_dist, val_dist]
    all_single_class = all(len(d) == 1 for d in all_distributions)
    if all_single_class:
        print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Oracle ƒëang predict t·∫•t c·∫£ l√† m·ªôt class duy nh·∫•t!")
        print(f"   ƒêi·ªÅu n√†y c√≥ th·ªÉ do:")
        print(f"   1. Oracle threshold qu√° cao/th·∫•p")
        print(f"   2. D·ªØ li·ªáu th·ª±c s·ª± ch·ªâ c√≥ m·ªôt class")
        print(f"   3. Oracle model c√≥ v·∫•n ƒë·ªÅ")
        if not oracle_client.supports_probabilities():
            print(f"   ‚ÑπÔ∏è  Oracle kh√¥ng h·ªó tr·ª£ probabilities -> b·ªè qua ƒëi·ªÅu ch·ªânh threshold t·ª± ƒë·ªông.")
        else:
            print(f"   üí° S·∫Ω th·ª≠ ki·ªÉm tra probabilities v√† c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh threshold...")
            try:
                test_sample_size = min(100, X_eval_s.shape[0])
                test_indices = rng.choice(X_eval_s.shape[0], size=test_sample_size, replace=False)
                test_data = X_eval_s[test_indices]
                test_probs = oracle_client.predict_proba(test_data)
                print(f"   üìä Test probabilities tr√™n {test_sample_size} samples:")
                print(f"      Range: [{test_probs.min():.4f}, {test_probs.max():.4f}]")
                print(f"      Mean: {test_probs.mean():.4f}, Median: {np.median(test_probs):.4f}")
                print(f"      Threshold hi·ªán t·∫°i: {oracle_client.get_threshold():.4f}")
                
                current_thresh = oracle_client.get_threshold()
                if test_probs.min() < current_thresh < test_probs.max():
                    print(f"   üí° Probabilities c√≥ c·∫£ d∆∞·ªõi v√† tr√™n threshold - c√≥ th·ªÉ c√≥ c·∫£ 2 classes")
                    print(f"      Th·ª≠ v·ªõi threshold th·∫•p h∆°n c√≥ th·ªÉ gi√∫p ph√¢n bi·ªát t·ªët h∆°n")
                elif test_probs.max() < current_thresh:
                    suggested_threshold = np.percentile(test_probs, 50)
                    print(f"   ‚ö†Ô∏è  T·∫§T C·∫¢ probabilities ƒë·ªÅu d∆∞·ªõi threshold {current_thresh}")
                    print(f"   üí° ƒê·ªÅ xu·∫•t gi·∫£m threshold xu·ªëng {suggested_threshold:.4f} (median) ƒë·ªÉ ph√¢n bi·ªát classes")
                    print(f"   üîÑ ƒêang ƒëi·ªÅu ch·ªânh threshold...")
                    oracle_client.set_threshold(suggested_threshold)
                    test_predictions_new = oracle_client.predict(X_eval_s[test_indices])
                    test_dist_new = dict(zip(*np.unique(test_predictions_new, return_counts=True)))
                    print(f"   ‚úÖ V·ªõi threshold m·ªõi {suggested_threshold:.4f}: {test_dist_new}")
                    
                    print(f"   üîÑ Re-querying seed, val, eval v·ªõi threshold m·ªõi...")
                    y_eval = oracle_client.predict(X_eval_s)
                    y_seed = oracle_client.predict(X_seed_s)
                    y_val = oracle_client.predict(X_val_s)
                    eval_dist = dict(zip(*np.unique(y_eval, return_counts=True)))
                    seed_dist = dict(zip(*np.unique(y_seed, return_counts=True)))
                    val_dist = dict(zip(*np.unique(y_val, return_counts=True)))
                    print(f"   ‚úÖ Distribution sau khi ƒëi·ªÅu ch·ªânh threshold:")
                    print(f"      Eval: {eval_dist}")
                    print(f"      Seed: {seed_dist}")
                    print(f"      Val: {val_dist}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra probabilities: {e}")

    metrics_history = []
    labeled_X = X_seed_s
    labeled_y = y_seed

    def evaluate(model, round_id: int, total_labels: int):
        probs = np.squeeze(model(X_eval_s), axis=-1)
        
        # T·ªëi ∆∞u threshold d·ª±a tr√™n F1-score v·ªõi ground truth labels
        # ƒêi·ªÅu n√†y quan tr·ªçng v·ªõi class imbalance nghi√™m tr·ªçng
        thresholds = np.linspace(0.1, 0.9, 81)
        best_f1 = -1
        best_threshold = 0.5
        best_preds = (probs >= 0.5).astype(int)
        
        for thresh in thresholds:
            preds_thresh = (probs >= thresh).astype(int)
            _, _, f1_thresh, _ = precision_recall_fscore_support(
                y_eval_gt, preds_thresh, average="binary", zero_division=0
            )
            if f1_thresh > best_f1:
                best_f1 = f1_thresh
                best_threshold = thresh
                best_preds = preds_thresh
        
        # S·ª≠ d·ª•ng threshold t·ªëi ∆∞u
        preds = best_preds
        
        # QUAN TR·ªåNG: Agreement = so s√°nh predictions c·ªßa surrogate v·ªõi predictions c·ªßa target model
        # Accuracy = so s√°nh predictions c·ªßa surrogate v·ªõi ground truth labels
        agreement = (preds == y_eval).mean()  # y_eval l√† predictions t·ª´ target model (oracle)
        acc = accuracy_score(y_eval_gt, preds)  # y_eval_gt l√† ground truth labels
        acc_vs_oracle = accuracy_score(y_eval, preds)  # Accuracy so v·ªõi oracle (gi·ªëng agreement nh∆∞ng d√πng accuracy_score)
        balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_eval_gt, preds, average="binary", zero_division=0
        )
        try:
            auc = roc_auc_score(y_eval_gt, probs)
        except ValueError:
            auc = float("nan")

        # T√≠nh s·ªë queries th·ª±c t·∫ø (kh√¥ng t√≠nh seed v√† val)
        # ·ªû round 0, total_labels ch·ªâ l√† seed_size, n√™n actual_queries = 0
        # T·ª´ round 1 tr·ªü ƒëi, total_labels = seed_size + queries_accumulated
        actual_queries = max(0, total_labels - seed_size)
        
        # Log metrics ƒë·ªÉ gi·∫£i th√≠ch s·ª± kh√°c bi·ªát
        print(f"\nüìä Round {round_id} Evaluation:")
        print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
        print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
        print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
        print(f"   üí° Gi·∫£i th√≠ch: Val accuracy trong training ({agreement:.4f}) cao v√¨ so v·ªõi oracle labels")
        print(f"   üí° Final accuracy ({acc:.4f}) th·∫•p h∆°n v√¨ so v·ªõi ground truth (oracle kh√¥ng ho√†n h·∫£o)")
        
        metrics = {
            "round": round_id,
            "labels_used": int(total_labels),
            "queries_used": int(actual_queries),  # S·ªë queries th·ª±c t·∫ø (ch·ªâ t√≠nh active learning)
            "optimal_threshold": float(best_threshold),
            "surrogate_acc": float(acc),  # Accuracy vs Ground Truth
            "surrogate_acc_vs_oracle": float(acc_vs_oracle),  # Accuracy vs Oracle (t∆∞∆°ng t·ª± agreement)
            "surrogate_balanced_acc": float(balanced_acc),  # Quan tr·ªçng v·ªõi class imbalance
            "surrogate_auc": float(auc),
            "surrogate_precision": float(precision),
            "surrogate_recall": float(recall),
            "surrogate_f1": float(f1),
            "agreement_with_target": float(agreement),
            "oracle_acc_vs_gt": float(oracle_acc_vs_gt),  # ƒê·ªô ch√≠nh x√°c c·ªßa oracle v·ªõi ground truth
        }
        metrics_history.append(metrics)
        return metrics

    # QUAN TR·ªåNG: Theo nghi√™n c·ª©u, d√πng early_stopping=30 v√† num_epochs cao (100)
    # ƒë·ªÉ model c√≥ ƒë·ªß th·ªùi gian h·ªçc v√† tr√°nh underfitting
    # early_stopping=30: patience ƒë·ªß l·ªõn ƒë·ªÉ v∆∞·ª£t qua local minima
    # num_epochs: ƒë·ªß epochs ƒë·ªÉ model h·ªçc t·ªët v·ªõi nhi·ªÅu d·ªØ li·ªáu (m·∫∑c ƒë·ªãnh 100 theo nghi√™n c·ª©u)
    if attacker_type == "lgb":
        # LightGBM attacker kh√¥ng c·∫ßn scale data
        # S·ª≠ d·ª•ng hyperparameters t·ªëi ∆∞u ƒë·ªÉ kh·ªõp v·ªõi target model
        attacker = LGBAttacker(seed=seed)
        attacker.train_model(labeled_X, labeled_y, X_val, y_val, boosting_rounds=2000, early_stopping=100)
        # V·ªõi LightGBM, kh√¥ng c·∫ßn scale data ƒë·ªÉ evaluate
        def evaluate_lgb(model, round_id, total_labels):
            probs = model(X_eval)
            # LightGBM predict tr·∫£ v·ªÅ 1D array ho·∫∑c 2D array
            if probs.ndim > 1:
                probs = probs.flatten()
            
            # T·ªëi ∆∞u threshold d·ª±a tr√™n F1-score v·ªõi ground truth labels
            # ƒêi·ªÅu n√†y quan tr·ªçng v·ªõi class imbalance nghi√™m tr·ªçng
            thresholds = np.linspace(0.1, 0.9, 81)
            best_f1 = -1
            best_threshold = 0.5
            best_preds = (probs >= 0.5).astype(int)
            
            for thresh in thresholds:
                preds_thresh = (probs >= thresh).astype(int)
                _, _, f1_thresh, _ = precision_recall_fscore_support(
                    y_eval_gt, preds_thresh, average="binary", zero_division=0
                )
                if f1_thresh > best_f1:
                    best_f1 = f1_thresh
                    best_threshold = thresh
                    best_preds = preds_thresh
            
            # S·ª≠ d·ª•ng threshold t·ªëi ∆∞u
            preds = best_preds
            
            # QUAN TR·ªåNG: Agreement = so s√°nh predictions c·ªßa surrogate v·ªõi predictions c·ªßa target model
            # Accuracy = so s√°nh predictions c·ªßa surrogate v·ªõi ground truth labels
            agreement = (preds == y_eval).mean()  # y_eval l√† predictions t·ª´ target model (oracle)
            acc = accuracy_score(y_eval_gt, preds)  # y_eval_gt l√† ground truth labels
            acc_vs_oracle = accuracy_score(y_eval, preds)  # Accuracy so v·ªõi oracle (gi·ªëng agreement nh∆∞ng d√πng accuracy_score)
            balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_eval_gt, preds, average="binary", zero_division=0
            )
            try:
                auc = roc_auc_score(y_eval_gt, probs)
            except ValueError:
                auc = float("nan")

            # T√≠nh s·ªë queries th·ª±c t·∫ø (kh√¥ng t√≠nh seed v√† val)
            actual_queries = total_labels - seed_size - val_size
            
            # Log metrics ƒë·ªÉ gi·∫£i th√≠ch s·ª± kh√°c bi·ªát
            print(f"\nüìä Round {round_id} Evaluation:")
            print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
            print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
            print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
            print(f"   üí° Gi·∫£i th√≠ch: Val accuracy trong training ({agreement:.4f}) cao v√¨ so v·ªõi oracle labels")
            print(f"   üí° Final accuracy ({acc:.4f}) th·∫•p h∆°n v√¨ so v·ªõi ground truth (oracle kh√¥ng ho√†n h·∫£o)")
            
            metrics = {
                "round": round_id,
                "labels_used": int(total_labels),
                "queries_used": int(actual_queries),  # S·ªë queries th·ª±c t·∫ø (ch·ªâ t√≠nh active learning)
                "optimal_threshold": float(best_threshold),
                "surrogate_acc": float(acc),  # Accuracy vs Ground Truth
                "surrogate_acc_vs_oracle": float(acc_vs_oracle),  # Accuracy vs Oracle (t∆∞∆°ng t·ª± agreement)
                "surrogate_balanced_acc": float(balanced_acc),  # Quan tr·ªçng v·ªõi class imbalance
                "surrogate_auc": float(auc),
                "surrogate_precision": float(precision),
                "surrogate_recall": float(recall),
                "surrogate_f1": float(f1),
                "agreement_with_target": float(agreement),
                "oracle_acc_vs_gt": float(oracle_acc_vs_gt),  # ƒê·ªô ch√≠nh x√°c c·ªßa oracle v·ªõi ground truth
            }
            metrics_history.append(metrics)
            return metrics
        
        evaluate = evaluate_lgb
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])
    elif attacker_type == "dual":
        # DualDNN attacker c·∫ßn scale data v√† c·∫£ ground truth labels (oracle predictions)
        # S·ª≠ d·ª•ng feature_dim th·ª±c t·∫ø t·ª´ dataset, kh√¥ng ph·∫£i t·ª´ target model
        attacker = KerasDualAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
        # DualDNN train v·ªõi (X, y_true) - y_true l√† oracle labels
        attacker.train_model(labeled_X, labeled_y, labeled_y, X_val_s, y_val, y_val, num_epochs=num_epochs)
        
        def evaluate_dual(model, round_id, total_labels):
            # DualDNN c·∫ßn c·∫£ X v√† y_true (oracle labels) khi predict
            # __call__ nh·∫≠n 2 tham s·ªë ri√™ng bi·ªát (X, y_true), kh√¥ng ph·∫£i tuple
            probs = np.squeeze(model(X_eval_s, y_eval), axis=-1)
            
            # T·ªëi ∆∞u threshold ho·∫∑c s·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh
            if fixed_threshold is not None:
                # S·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh
                best_threshold = fixed_threshold
                preds = (probs >= best_threshold).astype(int)
                print(f"   üîß S·ª≠ d·ª•ng threshold c·ªë ƒë·ªãnh: {best_threshold:.3f}")
            else:
                # T·ªëi ∆∞u threshold d·ª±a tr√™n metric ƒë∆∞·ª£c ch·ªçn
                thresholds = np.linspace(0.1, 0.9, 81)
                best_metric_value = -1
                best_threshold = 0.5
                best_preds = (probs >= 0.5).astype(int)
                
                for thresh in thresholds:
                    preds_thresh = (probs >= thresh).astype(int)
                    
                    # T√≠nh metric d·ª±a tr√™n metric ƒë∆∞·ª£c ch·ªçn
                    if threshold_optimization_metric == "f1":
                        _, _, metric_value, _ = precision_recall_fscore_support(
                            y_eval_gt, preds_thresh, average="binary", zero_division=0
                        )
                    elif threshold_optimization_metric == "accuracy":
                        metric_value = accuracy_score(y_eval_gt, preds_thresh)
                    elif threshold_optimization_metric == "balanced_accuracy":
                        metric_value = balanced_accuracy_score(y_eval_gt, preds_thresh)
                    else:
                        raise ValueError(
                            f"Unknown threshold_optimization_metric: {threshold_optimization_metric}. "
                            f"Ch·ªçn m·ªôt trong: 'f1', 'accuracy', 'balanced_accuracy'"
                        )
                    
                    if metric_value > best_metric_value:
                        best_metric_value = metric_value
                        best_threshold = thresh
                        best_preds = preds_thresh
                
                # S·ª≠ d·ª•ng threshold t·ªëi ∆∞u
                preds = best_preds
                print(f"   üîß Threshold t·ªëi ∆∞u ({threshold_optimization_metric}): {best_threshold:.3f} (metric = {best_metric_value:.4f})")
            
            # Agreement v√† accuracy metrics
            agreement = (preds == y_eval).mean()
            acc = accuracy_score(y_eval_gt, preds)
            acc_vs_oracle = accuracy_score(y_eval, preds)
            balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_eval_gt, preds, average="binary", zero_division=0
            )
            try:
                auc = roc_auc_score(y_eval_gt, probs)
            except ValueError:
                auc = float("nan")
            
            # T√≠nh s·ªë queries th·ª±c t·∫ø
            actual_queries = total_labels - seed_size - val_size
            
            print(f"\nüìä Round {round_id} Evaluation (DualDNN):")
            print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
            print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
            print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
            
            metrics = {
                "round": round_id,
                "labels_used": int(total_labels),
                "queries_used": int(actual_queries),
                "optimal_threshold": float(best_threshold),
                "surrogate_acc": float(acc),
                "surrogate_acc_vs_oracle": float(acc_vs_oracle),
                "surrogate_balanced_acc": float(balanced_acc),
                "surrogate_auc": float(auc),
                "surrogate_precision": float(precision),
                "surrogate_recall": float(recall),
                "surrogate_f1": float(f1),
                "agreement_with_target": float(agreement),
                "oracle_acc_vs_gt": float(oracle_acc_vs_gt),
            }
            metrics_history.append(metrics)
            return metrics
        
        evaluate = evaluate_dual
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])
    else:
        # Keras attacker c·∫ßn scale data
        attacker = KerasAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
        attacker.train_model(labeled_X, labeled_y, X_val_s, y_val, num_epochs=num_epochs)
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])

    # Track t·ªïng queries ƒë·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c
    total_queries_target = query_batch * num_rounds
    total_queries_accumulated = 0
    # Cho ph√©p l·ªách t·ªëi ƒëa 10% (d∆∞ ch·ª© kh√¥ng ƒë∆∞·ª£c thi·∫øu)
    min_queries_acceptable = int(total_queries_target * 0.9)  # √çt nh·∫•t 90% c·ªßa target
    max_queries_acceptable = int(total_queries_target * 1.1)  # T·ªëi ƒëa 110% c·ªßa target
    
    print(f"\nüìã M·ª•c ti√™u queries: {total_queries_target:,} ({query_batch:,} queries/round √ó {num_rounds} rounds)")
    print(f"   üìä Cho ph√©p l·ªách: {min_queries_acceptable:,} - {max_queries_acceptable:,} queries (90% - 110%)")
    print(f"   ‚ö†Ô∏è  Quan tr·ªçng: Kh√¥ng ƒë∆∞·ª£c thi·∫øu queries! (t·ªëi thi·ªÉu {min_queries_acceptable:,})")
    
    # Ki·ªÉm tra pool ban ƒë·∫ßu c√≥ ƒë·ªß kh√¥ng
    if X_pool.shape[0] < total_queries_target:
        print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Pool ban ƒë·∫ßu ({X_pool.shape[0]:,}) < T·ªïng queries d·ª± ki·∫øn ({total_queries_target:,})")
        print(f"   üí° Pool s·∫Ω c·∫°n ki·ªát tr∆∞·ªõc khi ƒë·∫°t ƒë·ªß queries. S·∫Ω c·ªë g·∫Øng l·∫•y t·ªëi ƒëa c√≥ th·ªÉ.")
    
    for query_round in range(1, num_rounds + 1):
        # Ki·ªÉm tra xem c√≤n c·∫ßn bao nhi√™u queries n·ªØa
        queries_remaining_needed = total_queries_target - total_queries_accumulated
        
        # N·∫øu ƒë√£ ƒë·∫°t ƒë·ªß queries, d·ª´ng l·∫°i
        if total_queries_accumulated >= total_queries_target:
            print(f"\n‚úÖ ƒê√£ ƒë·∫°t ƒë·ªß queries d·ª± ki·∫øn ({total_queries_target:,}). D·ª´ng active learning.")
            break
        
        # N·∫øu pool c√≤n l·∫°i √≠t h∆°n query_batch, v·∫´n c·ªë g·∫Øng l·∫•y t·ªëi ƒëa c√≥ th·ªÉ
        pool_remaining = X_pool.shape[0]
        queries_to_get_this_round = min(query_batch, pool_remaining, queries_remaining_needed)
        
        if queries_to_get_this_round <= 0:
            print(f"\n‚ö†Ô∏è  Round {query_round}: Kh√¥ng c√≤n queries ƒë·ªÉ l·∫•y. Pool: {pool_remaining}, C·∫ßn: {queries_remaining_needed}")
            break
        
        if pool_remaining < query_batch:
            print(f"\n‚ö†Ô∏è  Round {query_round}: Pool c√≤n l·∫°i ({pool_remaining}) < query_batch ({query_batch}).")
            print(f"   üîÑ S·∫Ω l·∫•y t·ªëi ƒëa {queries_to_get_this_round} queries t·ª´ pool c√≤n l·∫°i.")
        
        # C·∫¢I TI·∫æN: Stratified Entropy Sampling v·ªõi Pre-filtering b·∫±ng Thief Dataset Labels
        # Gi·∫£ ƒë·ªãnh: M·∫´u trong thief dataset ƒë√£ bi·∫øt nh√£n, m·∫´u t∆∞∆°ng t·ª± trong pool s·∫Ω c√≥ nh√£n t∆∞∆°ng t·ª±
        # S·ª≠ d·ª•ng labels c·ªßa thief dataset ƒë·ªÉ pre-filter pool tr∆∞·ªõc khi query oracle
        # Sau ƒë√≥ query oracle ƒë·ªÉ x√°c nh·∫≠n labels th·ª±c t·∫ø
        # V·∫´n gi·ªØ logic c√¢n b·∫±ng class
        print(f"\nüîÑ Round {query_round}: ƒêang ch·ªçn queries b·∫±ng Stratified Entropy Sampling v·ªõi Pre-filtering (thief dataset labels)...")
        
        # QUAN TR·ªåNG: T√°ch ri√™ng pool ƒë·ªÉ query oracle v√† pool ƒë·ªÉ train attacker
        # - Pool ƒë·ªÉ query oracle: d·ª±a tr√™n oracle_model_type (raw data cho LightGBM, scaled cho Keras)
        # - Pool ƒë·ªÉ train attacker: d·ª±a tr√™n attacker_type (scaled cho keras/dual, raw cho lgb)
        # Oracle query PH·∫¢I d√πng data ph√π h·ª£p v·ªõi oracle model, kh√¥ng ph·∫£i attacker model!
        
        # Pool ƒë·ªÉ query oracle - d·ª±a tr√™n oracle_model_type
        if oracle_model_type == "h5":
            # Keras Oracle: c·∫ßn scaled data
            pool_for_oracle = X_pool_s
        else:
            # LightGBM Oracle: c·∫ßn raw data (KH√îNG scale!)
            pool_for_oracle = X_pool
        
        # Pool ƒë·ªÉ train attacker - d·ª±a tr√™n attacker_type
        if attacker_type in ["keras", "dual"]:
            # Keras/Dual attacker: c·∫ßn scaled data
            pool_for_entropy = X_pool_s if X_pool_s is not None else X_pool
        else:
            # LightGBM attacker: c·∫ßn raw data
            pool_for_entropy = X_pool
        
        pool_size = pool_for_oracle.shape[0]  # D√πng pool_for_oracle ƒë·ªÉ pre-filter
        
        # B∆Ø·ªöC 1: Pre-filtering d·ª±a tr√™n labels c·ªßa thief dataset
        # S·ª≠ d·ª•ng y_pool_gt (labels t·ª´ thief dataset) ƒë·ªÉ ch·ªçn pool c√¢n b·∫±ng TR∆Ø·ªöC khi query oracle
        print(f"   üîÑ Pre-filtering pool d·ª±a tr√™n labels c·ªßa thief dataset...")
        pool_dist_gt_current = dict(zip(*np.unique(y_pool_gt, return_counts=True)))
        print(f"   üìä Pool distribution (thief dataset labels): {pool_dist_gt_current}")
        
        # Ch·ªçn subset t·ª´ pool d·ª±a tr√™n labels c·ªßa thief dataset ƒë·ªÉ ƒë·∫£m b·∫£o c√¢n b·∫±ng
        # M·ª•c ti√™u: Ch·ªçn ƒë·ªß samples t·ª´ m·ªói class ƒë·ªÉ c√≥ th·ªÉ ch·ªçn queries c√¢n b·∫±ng sau n√†y
        query_pool_size = min(pool_size, max(20000, queries_to_get_this_round * 10))
        
        # Stratified sampling t·ª´ pool d·ª±a tr√™n thief dataset labels
        # L·∫•y 50% t·ª´ m·ªói class (n·∫øu c√≥ ƒë·ªß)
        queries_per_class_for_pool = query_pool_size // 2
        
        pool_query_idx_list = []
        for class_label in [0, 1]:
            class_indices_in_pool = np.where(y_pool_gt == class_label)[0]
            if len(class_indices_in_pool) == 0:
                continue
            
            # L·∫•y t·ªëi ƒëa queries_per_class_for_pool t·ª´ class n√†y
            n_select_from_class = min(queries_per_class_for_pool, len(class_indices_in_pool))
            selected_indices = rng.choice(class_indices_in_pool, size=n_select_from_class, replace=False)
            pool_query_idx_list.append(selected_indices)
        
        if len(pool_query_idx_list) > 0:
            # K·∫øt h·ª£p indices t·ª´ c·∫£ 2 classes
            pool_query_idx = np.concatenate(pool_query_idx_list)
            rng.shuffle(pool_query_idx)  # Shuffle ƒë·ªÉ tr·ªôn classes
        else:
            # Fallback: N·∫øu kh√¥ng c√≥ class n√†o, d√πng to√†n b·ªô pool
            pool_query_idx = np.arange(pool_size)
        
        # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° query_pool_size
        if len(pool_query_idx) > query_pool_size:
            pool_query_idx = pool_query_idx[:query_pool_size]
        
        # L·∫•y data t·ª´ pool_for_oracle (raw/scaled t√πy oracle model type) ƒë·ªÉ query oracle
        # QUAN TR·ªåNG: Oracle query PH·∫¢I d√πng pool_for_oracle, kh√¥ng ph·∫£i pool_for_entropy!
        X_pool_query = pool_for_oracle[pool_query_idx]
        y_pool_query_gt = y_pool_gt[pool_query_idx]  # Labels t·ª´ thief dataset (ground truth c·ªßa pool)
        
        # Log distribution sau pre-filtering
        pool_query_dist_gt = dict(zip(*np.unique(y_pool_query_gt, return_counts=True)))
        print(f"   ‚úÖ Pre-filtered pool: {len(pool_query_idx)} samples (from {pool_size} total pool)")
        print(f"   üìä Pre-filtered distribution (thief dataset labels): {pool_query_dist_gt}")
        print(f"   üîç Using {'scaled' if oracle_model_type == 'h5' else 'raw'} data for oracle query (oracle is {oracle_model_type.upper()})")
        
        # B∆Ø·ªöC 2: Query oracle ƒë·ªÉ l·∫•y labels th·ª±c t·∫ø t·ª´ target model
        # ƒêi·ªÅu n√†y x√°c nh·∫≠n labels th·ª±c t·∫ø, c√≥ th·ªÉ kh√°c v·ªõi thief dataset labels
        # QUAN TR·ªåNG: Oracle query d√πng X_pool_query t·ª´ pool_for_oracle (ƒë√∫ng data type cho oracle)
        print(f"   üîÑ Querying oracle ƒë·ªÉ l·∫•y labels th·ª±c t·∫ø t·ª´ target model...")
        y_pool_query = oracle_client.predict(X_pool_query)
        pool_query_dist = dict(zip(*np.unique(y_pool_query, return_counts=True)))
        print(f"   üìä Pool distribution (oracle labels): {pool_query_dist}")
        
        # So s√°nh labels t·ª´ thief dataset vs oracle
        agreement_thief_oracle = np.mean(y_pool_query_gt == y_pool_query)
        print(f"   üìä Agreement (thief labels vs oracle labels): {agreement_thief_oracle:.4f} ({agreement_thief_oracle*100:.2f}%)")
        if agreement_thief_oracle < 0.7:
            print(f"   ‚ö†Ô∏è  WARNING: Thief labels v√† oracle labels kh√°c nhau nhi·ªÅu (>30%)")
            print(f"   üí° Pre-filtering d·ª±a tr√™n thief labels c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c, nh∆∞ng v·∫´n d√πng oracle labels cho ch·ªçn queries")
        else:
            print(f"   ‚úÖ Thief labels v√† oracle labels kh√° kh·ªõp - pre-filtering hi·ªáu qu·∫£")
        
        # B∆Ø·ªöC 3: T√≠nh entropy cho t·∫•t c·∫£ samples trong pool ƒë√£ query
        # QUAN TR·ªåNG: S·ª≠ d·ª•ng oracle labels (y_pool_query) ƒë·ªÉ ch·ªçn queries, kh√¥ng ph·∫£i thief labels
        # v√¨ ch√∫ng ta c·∫ßn labels th·ª±c t·∫ø t·ª´ target model ƒë·ªÉ ƒë·∫£m b·∫£o accuracy
        # V·ªõi dualDNN, c·∫ßn oracle labels cho entropy sampling
        
        # QUAN TR·ªåNG: ƒê·ªÉ t√≠nh entropy cho attacker, c·∫ßn d√πng pool_for_entropy (scaled cho keras/dual)
        # Nh∆∞ng X_pool_query l√† t·ª´ pool_for_oracle (raw cho LightGBM oracle)
        # C·∫ßn map v·ªÅ pool_for_entropy ƒë·ªÉ t√≠nh entropy ƒë√∫ng
        if attacker_type in ["keras", "dual"] and oracle_model_type == "lgb":
            # Oracle l√† LightGBM (raw), nh∆∞ng attacker l√† keras/dual (c·∫ßn scaled)
            # C·∫ßn l·∫•y scaled version c·ªßa X_pool_query ƒë·ªÉ t√≠nh entropy
            X_pool_query_for_entropy = pool_for_entropy[pool_query_idx]
        else:
            # Oracle v√† attacker c√πng data type, d√πng X_pool_query tr·ª±c ti·∫øp
            X_pool_query_for_entropy = X_pool_query
        
        pool_labels_for_entropy = y_pool_query if attacker_type == "dual" else np.zeros(X_pool_query.shape[0])
        dual_flag = (attacker_type == "dual")
        
        # T√≠nh entropy cho t·∫•t c·∫£ samples
        entropy_candidates = X_pool_query.shape[0]
        q_idx_all = entropy_sampling(
            attacker, 
            X_pool_query_for_entropy,  # D√πng scaled data n·∫øu attacker l√† keras/dual
            pool_labels_for_entropy,
            n_instances=entropy_candidates,
            dual=dual_flag
        )
        
        # B∆Ø·ªöC 4: Ch·ªçn queries c√¢n b·∫±ng t·ª´ m·ªói class d·ª±a tr√™n oracle labels
        # M·ª•c ti√™u: 50% class 0, 50% class 1 (ho·∫∑c t·ª∑ l·ªá g·∫ßn nh·∫•t c√≥ th·ªÉ)
        queries_per_class = queries_to_get_this_round // 2
        query_idx_list = []
        
        for class_label in [0, 1]:
            # L·ªçc indices c·ªßa class n√†y trong q_idx_all
            # q_idx_all l√† indices trong X_pool_query, ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo entropy gi·∫£m d·∫ßn
            class_mask = y_pool_query[q_idx_all] == class_label
            class_indices_in_q = np.where(class_mask)[0]  # Indices trong q_idx_all
            
            if len(class_indices_in_q) == 0:
                print(f"   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y class {class_label} trong pool")
                continue
            
            # Ch·ªçn queries_per_class samples c√≥ entropy cao nh·∫•t t·ª´ class n√†y
            # (q_idx_all ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo entropy gi·∫£m d·∫ßn)
            n_select = min(queries_per_class, len(class_indices_in_q))
            selected_indices_in_q = class_indices_in_q[:n_select]
            
            # Map t·ª´ indices trong q_idx_all sang indices trong X_pool_query
            # q_idx_all l√† indices trong X_pool_query (ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo entropy)
            selected_indices_in_pool_query = q_idx_all[selected_indices_in_q]
            
            # Map v·ªÅ indices trong pool g·ªëc (pool_for_entropy)
            # pool_query_idx l√† indices trong pool g·ªëc ƒë√£ ƒë∆∞·ª£c pre-filter
            # selected_indices_in_pool_query l√† indices trong X_pool_query (subset)
            selected_pool_indices = pool_query_idx[selected_indices_in_pool_query]
            
            query_idx_list.append(selected_pool_indices)
            print(f"   ‚úÖ Class {class_label}: Ch·ªçn {n_select}/{len(class_indices_in_q)} samples (entropy cao nh·∫•t)")
        
        # K·∫øt h·ª£p queries t·ª´ c·∫£ 2 classes
        if len(query_idx_list) > 0:
            query_idx = np.concatenate(query_idx_list)
        else:
            # Fallback: N·∫øu kh√¥ng c√≥ class n√†o, d√πng entropy sampling th√¥ng th∆∞·ªùng
            print(f"   ‚ö†Ô∏è  Fallback: D√πng entropy sampling th√¥ng th∆∞·ªùng (kh√¥ng c√≥ class n√†o)")
            entropy_candidates = min(10000, pool_for_oracle.shape[0])
            # V·ªõi dualDNN, c·∫ßn t·∫°o pool_labels_for_entropy t·ª´ pool ƒë√£ query
            if dual_flag:
                # Query oracle tr√™n pool_for_oracle (raw data cho LightGBM) ƒë·ªÉ l·∫•y labels cho entropy sampling
                pool_size_for_fallback = min(pool_for_oracle.shape[0], max(20000, queries_to_get_this_round * 10))
                if pool_size_for_fallback < pool_for_oracle.shape[0]:
                    pool_fallback_idx = rng.choice(pool_for_oracle.shape[0], size=pool_size_for_fallback, replace=False)
                    pool_fallback_X = pool_for_oracle[pool_fallback_idx]
                else:
                    pool_fallback_idx = np.arange(pool_for_oracle.shape[0])
                    pool_fallback_X = pool_for_oracle
                
                pool_labels_for_fallback = oracle_client.predict(pool_fallback_X)
                pool_labels_for_entropy_full = np.zeros(pool_for_oracle.shape[0])
                if pool_size_for_fallback < pool_for_oracle.shape[0]:
                    pool_labels_for_entropy_full[pool_fallback_idx] = pool_labels_for_fallback
                else:
                    pool_labels_for_entropy_full = pool_labels_for_fallback
            else:
                pool_labels_for_entropy_full = np.zeros(pool_for_oracle.shape[0])
            
            # T√≠nh entropy tr√™n pool_for_entropy (scaled n·∫øu attacker l√† keras/dual)
            q_idx = entropy_sampling(
                attacker, 
                pool_for_entropy, 
                pool_labels_for_entropy_full,
                n_instances=entropy_candidates,
                dual=dual_flag
            )
            X_med = pool_for_entropy[q_idx]
            num_clusters = min(queries_to_get_this_round, X_med.shape[0])
            if num_clusters > 0:
                kmed = KMedoids(n_clusters=num_clusters, init='k-medoids++', random_state=seed)
                kmed.fit(X_med)
                query_idx_in_med = kmed.medoid_indices_
                query_idx = q_idx[query_idx_in_med]
            else:
                query_idx = q_idx[:min(queries_to_get_this_round, len(q_idx))]
        
        # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° queries_to_get_this_round
        if len(query_idx) > queries_to_get_this_round:
            query_idx = query_idx[:queries_to_get_this_round]
        
        print(f"   ‚úÖ ƒê√£ ch·ªçn {len(query_idx)} queries (target: {queries_to_get_this_round})")

        # L·∫•y data cho queries ƒë√£ ch·ªçn
        # QUAN TR·ªåNG: query_idx l√† indices trong pool g·ªëc (pool_for_oracle)
        # C·∫ßn l·∫•y data t·ª´ pool ph√π h·ª£p v·ªõi attacker_type (scaled cho keras/dual, raw cho lgb)
        if attacker_type in ["keras", "dual"]:
            # Attacker c·∫ßn scaled data
            X_query_s = pool_for_entropy[query_idx] if pool_for_entropy is not None else X_pool[query_idx]
        else:
            # Attacker c·∫ßn raw data
            X_query_s = X_pool[query_idx]
        
        # T·ªëi ∆∞u: S·ª≠ d·ª•ng labels ƒë√£ query t·ª´ pool_query
        # T·∫•t c·∫£ queries ƒë·ªÅu ƒë∆∞·ª£c ch·ªçn t·ª´ pool_query (ƒë√£ query oracle)
        # query_idx l√† indices trong pool g·ªëc (pool_for_entropy)
        # pool_query_idx l√† indices trong pool g·ªëc t∆∞∆°ng ·ª©ng v·ªõi X_pool_query
        # y_pool_query l√† labels t·ª´ oracle cho X_pool_query
        # C·∫ßn t√¨m v·ªã tr√≠ c·ªßa query_idx trong pool_query_idx ƒë·ªÉ l·∫•y labels t·ª´ y_pool_query
        
        # T·ªëi ∆∞u: S·ª≠ d·ª•ng argsort + searchsorted ƒë·ªÉ mapping nhanh h∆°n
        # pool_query_idx c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c s·∫Øp x·∫øp, nh∆∞ng c√°c gi√° tr·ªã l√† unique
        sorted_idx = np.argsort(pool_query_idx)
        sorted_pool_query_idx = pool_query_idx[sorted_idx]
        # T√¨m v·ªã tr√≠ c·ªßa query_idx trong sorted_pool_query_idx
        positions_in_sorted = np.searchsorted(sorted_pool_query_idx, query_idx, side='left')
        # Ki·ªÉm tra xem query_idx c√≥ t·ªìn t·∫°i trong pool_query_idx kh√¥ng
        valid_mask = (positions_in_sorted < len(sorted_pool_query_idx)) & \
                     (sorted_pool_query_idx[positions_in_sorted] == query_idx)
        if not np.all(valid_mask):
            # Fallback n·∫øu c√≥ query_idx kh√¥ng t·ªìn t·∫°i trong pool_query_idx
            # ƒêi·ªÅu n√†y kh√¥ng n√™n x·∫£y ra, nh∆∞ng ƒë·ªÉ an to√†n
            raise ValueError(f"M·ªôt s·ªë query_idx kh√¥ng t·ªìn t·∫°i trong pool_query_idx. "
                           f"ƒêi·ªÅu n√†y c√≥ th·ªÉ do l·ªói logic mapping.")
        # Map v·ªÅ indices g·ªëc trong pool_query_idx
        query_positions = sorted_idx[positions_in_sorted]
        y_query = y_pool_query[query_positions]

        # Log class distribution ƒë·ªÉ ki·ªÉm tra
        query_dist = dict(zip(*np.unique(y_query, return_counts=True)))
        print(f"   üìä Query distribution (sau stratified sampling): {query_dist}")
        
        # KI·ªÇM TRA V√Ä C√ÇN B·∫∞NG CLASS DISTRIBUTION
        # Theo nghi√™n c·ª©u: Class imbalance c√≥ th·ªÉ l√†m model bias v·ªÅ class ƒëa s·ªë
        # Gi·∫£i ph√°p: ƒê·∫£m b·∫£o m·ªói class c√≥ √≠t nh·∫•t 30% queries (ho·∫∑c t·ªëi thi·ªÉu 100 samples)
        total_queries = len(y_query)
        if total_queries > 0:
            max_class_ratio = max(query_dist.values()) / total_queries
            min_class_samples = min(query_dist.values()) if len(query_dist) > 1 else 0
            min_required_samples = max(100, int(query_batch * 0.3))  # T·ªëi thi·ªÉu 30% ho·∫∑c 100 samples
            
            if max_class_ratio > 0.7 or min_class_samples < min_required_samples:
                print(f"   ‚ö†Ô∏è  Class imbalance: M·ªôt class chi·∫øm {max_class_ratio*100:.1f}%, class thi·ªÉu s·ªë c√≥ {min_class_samples} samples")
                print(f"   üí° C·∫ßn t·ªëi thi·ªÉu {min_required_samples} samples cho m·ªói class")
                
                # C√¢n b·∫±ng b·∫±ng c√°ch l·∫•y th√™m samples t·ª´ class thi·ªÉu s·ªë
                if len(query_dist) == 2:
                    minority_class = min(query_dist.items(), key=lambda x: x[1])[0]
                    majority_class = max(query_dist.items(), key=lambda x: x[1])[0]
                    minority_count = query_dist[minority_class]
                    
                    if minority_count < min_required_samples:
                        needed_samples = min_required_samples - minority_count
                        print(f"   üîÑ C·∫ßn th√™m {needed_samples} samples t·ª´ class {minority_class}...")
                        
                        # Query oracle tr√™n to√†n b·ªô pool c√≤n l·∫°i ƒë·ªÉ t√¨m class thi·ªÉu s·ªë
                        remaining_pool_size = X_pool_s.shape[0]
                        if remaining_pool_size > needed_samples:
                            # TƒÉng sample_size ƒë·ªÉ t√¨m ƒë·ªß class thi·ªÉu s·ªë (c√≥ th·ªÉ pool ch·ªß y·∫øu l√† class ƒëa s·ªë)
                            # ∆Ø·ªõc t√≠nh: n·∫øu class thi·ªÉu s·ªë chi·∫øm ~10%, c·∫ßn query ~10x ƒë·ªÉ t√¨m ƒë·ªß
                            sample_size = min(needed_samples * 10, remaining_pool_size)
                            candidate_idx = rng.choice(remaining_pool_size, size=sample_size, replace=False)
                            X_candidates = X_pool_s[candidate_idx]
                            y_candidates = oracle_client.predict(X_candidates)
                            
                            # L·ªçc ch·ªâ l·∫•y class thi·ªÉu s·ªë
                            minority_mask = y_candidates == minority_class
                            minority_found = np.sum(minority_mask)
                            
                            if minority_found >= needed_samples:
                                # L·∫•y ƒë·ªß samples t·ª´ class thi·ªÉu s·ªë
                                minority_indices = candidate_idx[minority_mask][:needed_samples]
                                X_additional = X_pool_s[minority_indices]
                                y_additional = oracle_client.predict(X_additional)
                                
                                X_query_s = np.vstack([X_query_s, X_additional])
                                y_query = np.concatenate([y_query, y_additional])
                                query_idx = np.concatenate([query_idx, minority_indices])
                                
                                balanced_dist = dict(zip(*np.unique(y_query, return_counts=True)))
                                print(f"   ‚úÖ ƒê√£ c√¢n b·∫±ng: {balanced_dist}")
                            else:
                                print(f"   ‚ö†Ô∏è  Ch·ªâ t√¨m th·∫•y {minority_found}/{needed_samples} samples t·ª´ class {minority_class}")
                                if minority_found > 0:
                                    minority_indices = candidate_idx[minority_mask]
                                    X_additional = X_pool_s[minority_indices]
                                    y_additional = oracle_client.predict(X_additional)
                                    X_query_s = np.vstack([X_query_s, X_additional])
                                    y_query = np.concatenate([y_query, y_additional])
                                    query_idx = np.concatenate([query_idx, minority_indices])
                                    
                                    final_dist = dict(zip(*np.unique(y_query, return_counts=True)))
                                    final_ratio = min(final_dist.values()) / sum(final_dist.values())
                                    print(f"   ‚úÖ ƒê√£ th√™m {minority_found} samples, distribution: {final_dist} (minority ratio: {final_ratio*100:.1f}%)")
                                else:
                                    print(f"   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y samples t·ª´ class {minority_class} trong pool c√≤n l·∫°i")
                                    print(f"   üí° C√≥ th·ªÉ pool c√≤n l·∫°i ch·ªß y·∫øu l√† class {majority_class}")
                elif len(query_dist) == 1:
                    print(f"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: Ch·ªâ c√≥ 1 class trong queries! Model s·∫Ω kh√¥ng h·ªçc ƒë∆∞·ª£c ph√¢n bi·ªát 2 classes")
                    # Th·ª≠ l·∫•y th√™m m·ªôt s·ªë random samples ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ c·∫£ 2 classes
                    remaining_pool_size = X_pool_s.shape[0]
                    if remaining_pool_size > 0:
                        additional_samples = min(200, remaining_pool_size, query_batch // 2)  # L·∫•y th√™m 50% ho·∫∑c t·ªëi ƒëa 200
                        additional_idx = rng.choice(remaining_pool_size, size=additional_samples, replace=False)
                        X_additional = X_pool_s[additional_idx]
                        y_additional = oracle_client.predict(X_additional)
                        additional_dist = dict(zip(*np.unique(y_additional, return_counts=True)))
                        print(f"   üîÑ L·∫•y th√™m {additional_samples} random samples: {additional_dist}")
                        
                        # Th√™m v√†o queries n·∫øu c√≥ class m·ªõi
                        if len(additional_dist) > len(query_dist) or any(c not in query_dist for c in additional_dist):
                            X_query_s = np.vstack([X_query_s, X_additional])
                            y_query = np.concatenate([y_query, y_additional])
                            query_idx = np.concatenate([query_idx, additional_idx])
                            print(f"   ‚úÖ ƒê√£ th√™m samples, distribution m·ªõi: {dict(zip(*np.unique(y_query, return_counts=True)))}")

        # QUAN TR·ªåNG: ƒê·∫£m b·∫£o s·ªë queries ch√≠nh x√°c = queries_to_get_this_round
        # KH√îNG BAO GI·ªú ƒë∆∞·ª£c thi·∫øu queries tr·ª´ khi pool th·ª±c s·ª± c·∫°n ki·ªát!
        actual_queries = len(y_query)
        
        # T√≠nh queries c√≤n c·∫ßn ƒë·ªÉ ƒë·∫°t target
        queries_remaining_needed = total_queries_target - total_queries_accumulated
        
        # M·ª•c ti√™u queries cho round n√†y: kh√¥ng v∆∞·ª£t qu√° queries_remaining_needed v√† kh√¥ng v∆∞·ª£t qu√° 110% c·ªßa query_batch
        max_queries_this_round = min(int(query_batch * 1.1), queries_remaining_needed) if queries_remaining_needed > 0 else int(query_batch * 1.1)
        min_queries_this_round = queries_to_get_this_round  # √çt nh·∫•t ph·∫£i ƒë·∫°t m·ª•c ti√™u cho round n√†y
        
        # QUAN TR·ªåNG: N·∫øu thi·∫øu queries, B·∫ÆT BU·ªòC ph·∫£i b·ªï sung t·ª´ pool
        # Ch·ªâ ch·∫•p nh·∫≠n thi·∫øu n·∫øu pool th·ª±c s·ª± c·∫°n ki·ªát
        if actual_queries < min_queries_this_round:
            # QUAN TR·ªåNG: N·∫øu c√≥ √≠t h∆°n m·ª•c ti√™u, B·∫ÆT BU·ªòC ph·∫£i b·ªï sung
            needed_samples = min_queries_this_round - actual_queries
            print(f"   ‚ö†Ô∏è  CH·ªà C√ì {actual_queries}/{min_queries_this_round} queries. C·∫¶N B·ªî SUNG {needed_samples} queries!")
            
            remaining_pool_size = X_pool_s.shape[0]
            if remaining_pool_size >= needed_samples:
                # L·∫•y th√™m random samples t·ª´ pool c√≤n l·∫°i
                additional_idx = rng.choice(remaining_pool_size, size=needed_samples, replace=False)
                X_additional = X_pool_s[additional_idx]
                y_additional = oracle_client.predict(X_additional)
                
                X_query_s = np.vstack([X_query_s, X_additional])
                y_query = np.concatenate([y_query, y_additional])
                query_idx = np.concatenate([query_idx, additional_idx])
                
                print(f"   ‚úÖ ƒê√£ b·ªï sung {needed_samples} queries t·ª´ pool. Total: {len(y_query)}")
                actual_queries = len(y_query)
            else:
                # Pool kh√¥ng ƒë·ªß, l·∫•y t·∫•t c·∫£ c√≤n l·∫°i
                pool_exhausted_flag = True
                if remaining_pool_size > 0:
                    X_additional = X_pool_s
                    y_additional = oracle_client.predict(X_additional)
                    
                    X_query_s = np.vstack([X_query_s, X_additional])
                    y_query = np.concatenate([y_query, y_additional])
                    all_indices = np.arange(X_pool_s.shape[0])
                    query_idx = np.concatenate([query_idx, all_indices])
                    
                    actual_queries = len(y_query)
                    print(f"   ‚ö†Ô∏è  Pool c√≤n l·∫°i ch·ªâ c√≥ {remaining_pool_size} samples. ƒê√£ l·∫•y t·∫•t c·∫£.")
                    print(f"   üìä Total queries trong round n√†y: {actual_queries} (m·ª•c ti√™u: {min_queries_this_round})")
                    if actual_queries < min_queries_this_round:
                        missing = min_queries_this_round - actual_queries
                        print(f"   ‚ùå V·∫™N THI·∫æU {missing} queries do pool c·∫°n ki·ªát!")
                else:
                    pool_exhausted_flag = True
                    print(f"   ‚ùå L·ªñI NGHI√äM TR·ªåNG: Pool ƒë√£ c·∫°n ki·ªát! Ch·ªâ c√≥ {actual_queries} queries thay v√¨ {min_queries_this_round}")
                    print(f"   ‚ùå Thi·∫øu {min_queries_this_round - actual_queries} queries! ƒêi·ªÅu n√†y s·∫Ω ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn hi·ªáu su·∫•t!")
        
        # Gi·ªõi h·∫°n t·ªëi ƒëa: kh√¥ng v∆∞·ª£t qu√° max_queries_this_round (110% c·ªßa query_batch ho·∫∑c queries c√≤n c·∫ßn)
        if actual_queries > max_queries_this_round:
            print(f"   ‚ö†Ô∏è  Class balancing ƒë√£ th√™m {actual_queries - max_queries_this_round} queries (v∆∞·ª£t qu√° 110%).")
            print(f"   üîÑ Gi·ªõi h·∫°n l·∫°i v·ªÅ {max_queries_this_round} queries.")
            X_query_s = X_query_s[:max_queries_this_round]
            y_query = y_query[:max_queries_this_round]
            query_idx = query_idx[:max_queries_this_round]
            actual_queries = max_queries_this_round
            final_dist = dict(zip(*np.unique(y_query, return_counts=True)))
            print(f"   üìä Query distribution sau khi gi·ªõi h·∫°n: {final_dist}")
        
        final_query_count = actual_queries
        
        # QUAN TR·ªåNG: Verify s·ªë queries tr∆∞·ªõc khi th√™m v√†o labeled set
        queries_this_round = len(y_query)
        total_queries_accumulated += queries_this_round
        if total_queries_accumulated > total_queries_target:
            over_budget_flag = True
        
        # Ki·ªÉm tra xem c√≥ ƒë·∫°t m·ª•c ti√™u kh√¥ng
        if queries_this_round >= min_queries_this_round:
            status = "‚úÖ"
        else:
            status = "‚ö†Ô∏è"
        
        print(f"   {status} Round {query_round}: ƒê√£ ch·ªçn {queries_this_round} queries (m·ª•c ti√™u: {min_queries_this_round}, t·ªëi ƒëa: {max_queries_this_round})")
        print(f"   üìä T·ªïng queries t√≠ch l≈©y: {total_queries_accumulated:,}/{total_queries_target:,} ({total_queries_accumulated/total_queries_target*100:.1f}%)")
        
        # QUAN TR·ªåNG: Verify queries_this_round ƒë·∫°t m·ª•c ti√™u tr∆∞·ªõc khi x√≥a t·ª´ pool
        # N·∫øu thi·∫øu queries v√† pool v·∫´n c√≤n, ph·∫£i c·∫£nh b√°o nghi√™m tr·ªçng
        if queries_this_round < min_queries_this_round:
            missing = min_queries_this_round - queries_this_round
            pool_remaining_before_delete = X_pool.shape[0]
            print(f"\n   ‚ùå L·ªñI NGHI√äM TR·ªåNG: Round {query_round} ch·ªâ c√≥ {queries_this_round} queries thay v√¨ {min_queries_this_round}!")
            print(f"   ‚ùå Thi·∫øu {missing} queries! ƒêi·ªÅu n√†y s·∫Ω ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn hi·ªáu su·∫•t!")
            print(f"   üí° Pool c√≤n l·∫°i tr∆∞·ªõc khi x√≥a: {pool_remaining_before_delete:,} samples")
            print(f"   üí° Ki·ªÉm tra logic b·ªï sung queries ho·∫∑c pool size ban ƒë·∫ßu!")
            # KH√îNG raise error v√¨ c√≥ th·ªÉ pool th·ª±c s·ª± c·∫°n ki·ªát, nh∆∞ng c·∫£nh b√°o r√µ r√†ng
        
        labeled_X = np.vstack([labeled_X, X_query_s])
        labeled_y = np.concatenate([labeled_y, y_query])

        # X√≥a t·ª´ pool (ƒë·∫£m b·∫£o query_idx unique)
        query_idx_unique = np.unique(query_idx)
        X_pool = np.delete(X_pool, query_idx_unique, axis=0)
        # QUAN TR·ªåNG: C≈©ng x√≥a labels t∆∞∆°ng ·ª©ng t·ª´ y_pool_gt (thief dataset labels)
        y_pool_gt = np.delete(y_pool_gt, query_idx_unique, axis=0)
        
        if attacker_type in ["keras", "dual"]:
            # X_pool_s c√≥ s·∫µn cho Keras v√† dualDNN
            X_pool_s = np.delete(X_pool_s, query_idx_unique, axis=0)
            # L∆∞u √Ω: pool_labels_for_entropy ƒë∆∞·ª£c t·∫°o l·∫°i m·ªói round t·ª´ y_pool_query,
            # kh√¥ng c·∫ßn x√≥a v√¨ n√≥ ch·ªâ l√† bi·∫øn t·∫°m th·ªùi trong m·ªói round
        else:
            # V·ªõi LightGBM, X_pool_s = X_pool
            X_pool_s = X_pool
        
        print(f"   üìä Pool c√≤n l·∫°i: {X_pool.shape[0]:,} samples")

        # QUAN TR·ªåNG: Re-train t·ª´ ƒë·∫ßu tr√™n to√†n b·ªô d·ªØ li·ªáu t√≠ch l≈©y
        # Theo nghi√™n c·ª©u: Hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu gi√∫p model h·ªçc l·∫°i ph√¢n ph·ªëi t·ªïng th·ªÉ,
        # gi·∫£m thi·ªÉu vi·ªác b·ªã l·ªách theo ph√¢n ph·ªëi c·ªßa l√¥ d·ªØ li·ªáu m·ªõi nh·∫•t
        print(f"   üîÑ Re-training model v·ªõi {labeled_X.shape[0]:,} labeled samples...")
        
        if attacker_type == "lgb":
            attacker = LGBAttacker(seed=seed)
            # S·ª≠ d·ª•ng hyperparameters t·ªëi ∆∞u ƒë·ªÉ kh·ªõp v·ªõi target model
            attacker.train_model(labeled_X, labeled_y, X_val, y_val, boosting_rounds=2000, early_stopping=100)
        elif attacker_type == "dual":
            # S·ª≠ d·ª•ng feature_dim th·ª±c t·∫ø t·ª´ dataset, kh√¥ng ph·∫£i t·ª´ target model
            attacker = KerasDualAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
            # DualDNN train v·ªõi (X, y, y_true) - y_true l√† oracle labels
            attacker.train_model(labeled_X, labeled_y, labeled_y, X_val_s, y_val, y_val, num_epochs=num_epochs)
        else:
            attacker = KerasAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
            attacker.train_model(labeled_X, labeled_y, X_val_s, y_val, num_epochs=num_epochs)

        evaluate(attacker, round_id=query_round, total_labels=labeled_X.shape[0])
    
    # Ki·ªÉm tra t·ªïng queries cu·ªëi c√πng
    final_total_queries = total_queries_accumulated
    diff = final_total_queries - total_queries_target
    diff_percent = (diff / total_queries_target * 100) if total_queries_target > 0 else 0
    query_gap_reason = "on_target"
    if final_total_queries < total_queries_target:
        query_gap_reason = "pool_exhausted" if pool_exhausted_flag else "stopped_before_target"
    elif final_total_queries > total_queries_target:
        query_gap_reason = "over_budget" if over_budget_flag else "extra_queries"
    
    print(f"\n{'='*80}")
    print(f"üìä T·ªîNG K·∫æT QUERIES:")
    print(f"{'='*80}")
    print(f"   Queries d·ª± ki·∫øn: {total_queries_target:,} ({query_batch:,} queries/round √ó {num_rounds} rounds)")
    print(f"   Queries th·ª±c t·∫ø: {final_total_queries:,}")
    print(f"   Ch√™nh l·ªách: {diff:+,} queries ({diff_percent:+.2f}%)")
    print(f"   Ng∆∞·ª°ng ch·∫•p nh·∫≠n: {min_queries_acceptable:,} - {max_queries_acceptable:,} (90% - 110%)")
    
    if final_total_queries == total_queries_target:
        print(f"   ‚úÖ S·ªë queries ch√≠nh x√°c 100%!")
    elif final_total_queries >= min_queries_acceptable and final_total_queries <= max_queries_acceptable:
        if diff > 0:
            print(f"   ‚úÖ S·ªë queries trong ng∆∞·ª°ng ch·∫•p nh·∫≠n (d∆∞ {abs(diff):,} queries)")
        else:
            print(f"   ‚ö†Ô∏è  S·ªë queries trong ng∆∞·ª°ng ch·∫•p nh·∫≠n nh∆∞ng thi·∫øu {abs(diff):,} queries ({abs(diff_percent):.2f}%)")
    elif final_total_queries < min_queries_acceptable:
        print(f"   ‚ùå L·ªñI NGHI√äM TR·ªåNG: S·ªê QUERIES THI·∫æU QU√Å NHI·ªÄU! ({abs(diff_percent):.2f}% thi·∫øu)")
        print(f"   ‚ùå Thi·∫øu {abs(diff):,} queries! ƒêi·ªÅu n√†y s·∫Ω ·∫£nh h∆∞·ªüng NGHI√äM TR·ªåNG ƒë·∫øn hi·ªáu su·∫•t t·∫•n c√¥ng!")
        print(f"   üí° L√Ω do c√≥ th·ªÉ: Pool ƒë√£ c·∫°n ki·ªát tr∆∞·ªõc khi ƒë·∫°t ƒë·ªß queries")
        print(f"   ‚ö†Ô∏è  C·∫ßn ki·ªÉm tra l·∫°i:")
        print(f"      - Pool size ban ƒë·∫ßu c√≥ ƒë·ªß kh√¥ng? (c·∫ßn √≠t nh·∫•t {total_queries_target:,} v·ªõi buffer 20%)")
        print(f"      - Logic b·ªï sung queries c√≥ ho·∫°t ƒë·ªông ƒë√∫ng kh√¥ng?")
        print(f"      - C√≥ th·ªÉ c·∫ßn tƒÉng pool size ho·∫∑c ƒëi·ªÅu ch·ªânh query_batch/num_rounds")
        # KH√îNG raise error v√¨ v·∫´n mu·ªën c√≥ k·∫øt qu·∫£, nh∆∞ng c·∫£nh b√°o r√µ r√†ng
    else:
        print(f"   ‚ö†Ô∏è  S·ªë queries v∆∞·ª£t qu√° 10% (d∆∞ {diff:,} queries, {diff_percent:.2f}%)")
    print(f"{'='*80}\n")

    output_dir.mkdir(parents=True, exist_ok=True)
    target_surrogate_dir = _resolve_optional_path(surrogate_dir) if surrogate_dir else output_dir.resolve()
    target_surrogate_dir.mkdir(parents=True, exist_ok=True)
    surrogate_basename = surrogate_name if surrogate_name else "surrogate_model"
    surrogate_path = target_surrogate_dir / surrogate_basename
    attacker.save_model(str(surrogate_path))
    
    # L·∫•y extension ph√π h·ª£p v·ªõi model type
    if attacker_type == "lgb":
        surrogate_model_path = f"{surrogate_path}.txt"
    else:
        # Keras v√† dualDNN ƒë·ªÅu d√πng .h5
        surrogate_model_path = f"{surrogate_path}.h5"

    joblib_path = output_dir / "robust_scaler.joblib"
    try:
        if scaler is not None:
            import joblib
            joblib.dump(scaler, joblib_path)
        else:
            joblib_path = None
    except Exception:
        joblib_path = None

    df_metrics = pd.DataFrame(metrics_history)
    metrics_csv = output_dir / "extraction_metrics.csv"
    df_metrics.to_csv(metrics_csv, index=False)

    summary = {
        "oracle_source": oracle_source,
        "model_file_name": model_file_name,
        "model_type": model_type,
        "normalization_stats_path": normalization_stats_path,
        "attacker_type": attacker_type,
        "surrogate_model_path": surrogate_model_path,
        "scaler_path": str(joblib_path) if joblib_path else None,
        "metrics_csv": str(metrics_csv),
        "metrics": metrics_history,
        "total_queries_target": int(total_queries_target),
        "total_queries_actual": int(final_total_queries),
        "query_gap_reason": query_gap_reason,
    }

    summary_path = output_dir / "extraction_summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    return summary


if __name__ == "__main__":
    SUMMARY = run_extraction(
        weights_path=str(PROJECT_ROOT / "artifacts" / "targets" / "final_model.h5"),
        output_dir=PROJECT_ROOT / "output",
        seed=42,
    )
    print(json.dumps(SUMMARY["metrics"][-1], indent=2))

