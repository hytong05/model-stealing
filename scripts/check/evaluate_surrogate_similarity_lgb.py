"""
Script ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target model v√† surrogate model (LightGBM)
S·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ test dataset (unlabeled data)
Khi kh√¥ng c√≥ ground truth labels, ch·ªâ t√≠nh Agreement (kh√¥ng t√≠nh Accuracy)
"""
import argparse
import json
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.targets.oracle_client import create_oracle_from_name, LocalOracleClient


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """L·∫•y danh s√°ch feature columns t·ª´ parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_test_data(parquet_path: str, feature_cols: list, max_samples: int = 10000, load_labels: bool = False):
    """
    Load d·ªØ li·ªáu test t·ª´ parquet file.
    
    Args:
        parquet_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file parquet
        feature_cols: Danh s√°ch feature columns
        max_samples: S·ªë l∆∞·ª£ng samples t·ªëi ƒëa
        load_labels: N·∫øu True, s·∫Ω load c·∫£ labels v√† ch·ªâ l·∫•y samples c√≥ label 0 ho·∫∑c 1 (ground truth)
                    N·∫øu False, load t·∫•t c·∫£ samples (b·ªè qua labels)
    
    Returns:
        X: Feature array (n_samples, n_features)
        y_true: Ground truth labels (n_samples,) - ch·ªâ tr·∫£ v·ªÅ n·∫øu load_labels=True
    """
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    all_y = [] if load_labels else None
    rows_loaded = 0
    
    print(f"üîÑ ƒêang load d·ªØ li·ªáu t·ª´ {parquet_path}...")
    
    for batch in pq_file.iter_batches(batch_size=5000, columns=feature_cols + ["Label"]):
        if rows_loaded >= max_samples:
            break
            
        batch_df = batch.to_pandas()
        
        if load_labels:
            # Load c·∫£ labels v√† ch·ªâ l·∫•y samples c√≥ label 0 ho·∫∑c 1 (ground truth)
            y = batch_df["Label"].values.astype(np.int32)
            valid_mask = (y >= 0) & (y <= 1)  # Ch·ªâ l·∫•y label 0 ho·∫∑c 1
            
            if valid_mask.sum() == 0:
                continue
            
            batch_df = batch_df[valid_mask]
            y = y[valid_mask]
            
            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            all_X.append(X)
            all_y.append(y)
            rows_loaded += len(X)
        else:
            # L·∫•y t·∫•t c·∫£ samples (kh√¥ng quan t√¢m nh√£n v√¨ s·∫Ω query target model)
            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            all_X.append(X)
            rows_loaded += len(X)
        
        if rows_loaded % 5000 == 0:
            print(f"  ƒê√£ load {rows_loaded:,}/{max_samples:,} samples...")
        
        if rows_loaded >= max_samples:
            break
    
    if all_X:
        X_concat = np.concatenate(all_X, axis=0)
        if len(X_concat) > max_samples:
            X_concat = X_concat[:max_samples]
        
        if load_labels:
            y_concat = np.concatenate(all_y, axis=0) if all_y else None
            if y_concat is not None and len(y_concat) > max_samples:
                y_concat = y_concat[:max_samples]
            print(f"‚úÖ ƒê√£ load {len(X_concat):,} samples v·ªõi ground truth labels")
            return X_concat, y_concat
        else:
            print(f"‚úÖ ƒê√£ load {len(X_concat):,} samples")
            return X_concat
    else:
        if load_labels:
            return np.empty((0, len(feature_cols)), dtype=np.float32), np.empty((0,), dtype=np.int32)
        else:
            return np.empty((0, len(feature_cols)), dtype=np.float32)


def load_normalization_stats(stats_path: str):
    """Load normalization statistics t·ª´ file .npz"""
    stats = np.load(stats_path, allow_pickle=True)
    
    feature_means = stats['feature_means']
    feature_stds = stats['feature_stds']
    
    if 'feature_cols' in stats:
        feature_cols = stats['feature_cols'].tolist() if hasattr(stats['feature_cols'], 'tolist') else stats['feature_cols']
    else:
        feature_cols = None
    
    return feature_means, feature_stds, feature_cols


def normalize_features(X, feature_means, feature_stds, feature_cols=None, required_feature_dim=None):
    """
    Normalize features gi·ªëng nh∆∞ LightGBM target model.
    
    Args:
        X: Raw features (n_samples, n_features)
        feature_means: Mean values cho normalization
        feature_stds: Std values cho normalization
        feature_cols: Danh s√°ch feature columns (n·∫øu c√≥)
        required_feature_dim: S·ªë features m√† model y√™u c·∫ßu
    
    Returns:
        X_normalized: Normalized features
    """
    X = np.asarray(X, dtype=np.float32)
    
    # Align features n·∫øu c·∫ßn
    if required_feature_dim is not None:
        current_dim = X.shape[1]
        if current_dim < required_feature_dim:
            # Pad v·ªõi zeros
            padding = np.zeros((X.shape[0], required_feature_dim - current_dim), dtype=np.float32)
            X = np.concatenate([X, padding], axis=1)
        elif current_dim > required_feature_dim:
            # Truncate
            X = X[:, :required_feature_dim]
    
    # Normalize: (X - mean) / std
    # Tr√°nh chia cho 0
    feature_stds_safe = np.where(feature_stds == 0, 1.0, feature_stds)
    X_normalized = (X - feature_means) / feature_stds_safe
    
    return X_normalized


def load_LightGBM_surrogate_model(model_path: str, normalization_stats_path: str = None, feature_dim: int = None, threshold: float = 0.5):
    """
    Load LightGBM surrogate model v√† normalization stats.
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file LightGBM model (.txt, .lgb, .pkl)
        normalization_stats_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file normalization stats (.npz)
        feature_dim: S·ªë features (n·∫øu kh√¥ng c√≥ s·∫Ω auto-detect t·ª´ model)
    
    Returns:
        predict function: (X) -> (predictions, probabilities)
    """
    print(f"üîÑ ƒêang load LightGBM surrogate model t·ª´ {model_path}...")
    
    # Load model
    try:
        model = lgb.Booster(model_file=model_path)
        model_actual_features = model.num_feature()
        print(f"‚úÖ ƒê√£ load LightGBM model")
        print(f"   Model features (t·ª´ model file): {model_actual_features}")
        
        # Auto-detect feature_dim t·ª´ model n·∫øu ch∆∞a c√≥
        if feature_dim is None:
            feature_dim = model_actual_features
            print(f"   Auto-detected feature_dim: {feature_dim}")
        else:
            # So s√°nh feature_dim ƒë∆∞·ª£c truy·ªÅn v√†o v·ªõi s·ªë features th·ª±c t·∫ø c·ªßa model
            print(f"   Feature_dim ƒë∆∞·ª£c truy·ªÅn v√†o: {feature_dim}")
            if feature_dim != model_actual_features:
                print(f"   ‚ö†Ô∏è  WARNING: Feature mismatch!")
                print(f"      - Model th·ª±c t·∫ø c√≥: {model_actual_features} features")
                print(f"      - Feature_dim ƒë∆∞·ª£c truy·ªÅn v√†o: {feature_dim} features")
                print(f"      - S·∫Ω s·ª≠ d·ª•ng s·ªë features th·ª±c t·∫ø c·ªßa model: {model_actual_features}")
                print(f"      - Data s·∫Ω ƒë∆∞·ª£c align/truncate ƒë·ªÉ kh·ªõp v·ªõi model")
                # S·ª≠ d·ª•ng s·ªë features th·ª±c t·∫ø c·ªßa model
                feature_dim = model_actual_features
            else:
                print(f"   ‚úÖ Feature dimensions kh·ªõp: {feature_dim}")
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ load LightGBM model: {e}")
        raise
    
    # Load normalization stats n·∫øu c√≥
    feature_means = None
    feature_stds = None
    if normalization_stats_path and Path(normalization_stats_path).exists():
        print(f"üîÑ ƒêang load normalization stats t·ª´ {normalization_stats_path}...")
        try:
            feature_means, feature_stds, _ = load_normalization_stats(normalization_stats_path)
            print(f"‚úÖ ƒê√£ load normalization stats")
            print(f"   Feature means shape: {feature_means.shape}")
            print(f"   Feature stds shape: {feature_stds.shape}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load normalization stats: {e}")
            print(f"   S·∫Ω predict kh√¥ng normalize")
    else:
        print(f"‚ö†Ô∏è  Kh√¥ng c√≥ normalization stats, s·∫Ω predict kh√¥ng normalize")
    
    def predict(X):
        """
        Predict v·ªõi LightGBM model.
        
        Args:
            X: Raw feature array (shape: [n_samples, n_features])
        
        Returns:
            (predictions, probabilities): predictions l√† binary classes, probabilities l√† raw outputs
        """
        current_dim = X.shape[1]
        
        # Debug: In th√¥ng tin v·ªÅ feature dimensions
        if current_dim != feature_dim:
            print(f"   üîç Debug predict: Input c√≥ {current_dim} features, model c·∫ßn {feature_dim} features")
        
        # Normalize n·∫øu c√≥ stats
        # T·∫°o local copies ƒë·ªÉ tr√°nh UnboundLocalError
        local_feature_means = feature_means
        local_feature_stds = feature_stds
        
        if local_feature_means is not None and local_feature_stds is not None:
            # Ki·ªÉm tra xem normalization stats c√≥ kh·ªõp v·ªõi model kh√¥ng
            stats_dim = local_feature_means.shape[0]
            if stats_dim != feature_dim:
                print(f"   ‚ö†Ô∏è  WARNING: Normalization stats c√≥ {stats_dim} features nh∆∞ng model c·∫ßn {feature_dim} features")
                print(f"      - S·∫Ω ch·ªâ normalize {min(stats_dim, feature_dim)} features ƒë·∫ßu ti√™n")
                # Truncate stats n·∫øu c·∫ßn (t·∫°o copies ƒë·ªÉ kh√¥ng modify original)
                if stats_dim > feature_dim:
                    local_feature_means = local_feature_means[:feature_dim].copy()
                    local_feature_stds = local_feature_stds[:feature_dim].copy()
                else:
                    # Pad stats v·ªõi zeros n·∫øu c·∫ßn (kh√¥ng n√™n x·∫£y ra)
                    padding_means = np.zeros(feature_dim - stats_dim, dtype=local_feature_means.dtype)
                    padding_stds = np.ones(feature_dim - stats_dim, dtype=local_feature_stds.dtype)
                    local_feature_means = np.concatenate([local_feature_means, padding_means])
                    local_feature_stds = np.concatenate([local_feature_stds, padding_stds])
            
            X_normalized = normalize_features(X, local_feature_means, local_feature_stds, required_feature_dim=feature_dim)
        else:
            # Align features n·∫øu c·∫ßn (kh√¥ng normalize)
            if feature_dim is not None:
                if current_dim < feature_dim:
                    padding = np.zeros((X.shape[0], feature_dim - current_dim), dtype=np.float32)
                    X_normalized = np.concatenate([X, padding], axis=1)
                    print(f"   üîç Debug: ƒê√£ pad {feature_dim - current_dim} features v·ªõi zeros")
                elif current_dim > feature_dim:
                    X_normalized = X[:, :feature_dim]
                    print(f"   üîç Debug: ƒê√£ truncate t·ª´ {current_dim} xu·ªëng {feature_dim} features")
                else:
                    X_normalized = X
            else:
                X_normalized = X
        
        # Verify final dimensions
        if X_normalized.shape[1] != feature_dim:
            raise ValueError(f"‚ùå L·ªói: X_normalized c√≥ {X_normalized.shape[1]} features nh∆∞ng model c·∫ßn {feature_dim}")
        
        # Predict
        num_iteration = None
        if hasattr(model, 'best_iteration') and model.best_iteration is not None:
            if model.best_iteration > 0:
                num_iteration = model.best_iteration
        
        try:
            probs = model.predict(X_normalized, num_iteration=num_iteration)
        except Exception as e:
            print(f"   ‚ùå L·ªói khi predict: {e}")
            print(f"   üîç Debug: X_normalized shape: {X_normalized.shape}, model.num_feature(): {model.num_feature()}")
            raise
        
        # ƒê·∫£m b·∫£o output l√† 1D array
        if probs.ndim > 1:
            probs = np.squeeze(probs)
        
        # Debug: In th·ªëng k√™ v·ªÅ probabilities
        if len(probs) > 0:
            print(f"   üîç Debug predict: Probs range: [{probs.min():.4f}, {probs.max():.4f}], mean: {probs.mean():.4f}, std: {probs.std():.4f}")
            # Ki·ªÉm tra xem c√≥ ph·∫£i t·∫•t c·∫£ probabilities ƒë·ªÅu < threshold kh√¥ng
            below_threshold = (probs < threshold).sum()
            above_threshold = (probs >= threshold).sum()
            print(f"   üîç Debug predict: Probabilities < threshold ({threshold}): {below_threshold}, >= threshold: {above_threshold}")
        
        # LightGBM predict tr·∫£ v·ªÅ probability c·ªßa class 1 (malware)
        # Chuy·ªÉn th√†nh binary labels v·ªõi threshold
        predictions = (probs >= threshold).astype(int)
        
        # Debug: In th·ªëng k√™ v·ªÅ predictions
        unique_preds, counts_preds = np.unique(predictions, return_counts=True)
        print(f"   üîç Debug predict: Predictions distribution: {dict(zip(unique_preds, counts_preds))}")
        
        # C·∫£nh b√°o n·∫øu t·∫•t c·∫£ predictions ƒë·ªÅu gi·ªëng nhau
        if len(unique_preds) == 1:
            print(f"   ‚ö†Ô∏è  WARNING: T·∫•t c·∫£ predictions ƒë·ªÅu l√† {unique_preds[0]}!")
            print(f"      - ƒêi·ªÅu n√†y cho th·∫•y model c√≥ th·ªÉ qu√° y·∫øu ho·∫∑c ƒë∆∞·ª£c train kh√¥ng ƒë√∫ng")
            print(f"      - C√≥ th·ªÉ do feature mismatch (train tr√™n dataset kh√°c v·ªõi test)")
            print(f"      - Ho·∫∑c model ch·ªâ h·ªçc ƒë∆∞·ª£c pattern trivial (lu√¥n predict m·ªôt class)")
        
        return predictions, probs
    
    return predict


def evaluate_model_similarity(
    target_model,
    surrogate_predict,
    X_test,
    y_target,
    y_true=None,
    model_name: str = ""
):
    """
    ƒê√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target v√† surrogate model.
    
    Args:
        target_model: Target model oracle
        surrogate_predict: Surrogate model predict function
        X_test: Test features
        y_target: Predictions t·ª´ target model
        y_true: Ground truth labels (n·∫øu c√≥) - ƒë·ªÉ t√≠nh Accuracy th·∫≠t s·ª±
        model_name: T√™n model
    
    Returns:
        metrics dict v·ªõi accuracy (n·∫øu c√≥ y_true) v√† agreement
    """
    print(f"\nüîÑ ƒêang ƒë√°nh gi√° {model_name}...")
    print(f"   Test data shape: {X_test.shape}")
    print(f"   Target predictions distribution: {dict(zip(*np.unique(y_target, return_counts=True)))}")
    
    # Predict v·ªõi surrogate
    print(f"   ƒêang predict v·ªõi surrogate model...")
    y_surrogate, probs_surrogate = surrogate_predict(X_test)
    
    # Debug: In th·ªëng k√™ v·ªÅ surrogate predictions
    print(f"   Surrogate predictions shape: {y_surrogate.shape}")
    print(f"   Surrogate probabilities shape: {probs_surrogate.shape}")
    unique_surr, counts_surr = np.unique(y_surrogate, return_counts=True)
    print(f"   Surrogate predictions distribution: {dict(zip(unique_surr, counts_surr))}")
    if len(probs_surrogate) > 0:
        print(f"   Surrogate probabilities stats: min={probs_surrogate.min():.4f}, max={probs_surrogate.max():.4f}, mean={probs_surrogate.mean():.4f}, std={probs_surrogate.std():.4f}")
    
    # T√≠nh metrics
    # Agreement = t·ªâ l·ªá nh·∫•t qu√°n gi·ªØa target v√† surrogate predictions
    agreement = accuracy_score(y_target, y_surrogate)
    
    # Accuracy = t·ªâ l·ªá ch√≠nh x√°c c·ªßa surrogate so v·ªõi ground truth (n·∫øu c√≥)
    if y_true is not None:
        accuracy = accuracy_score(y_true, y_surrogate)
    else:
        accuracy = None
    
    # T√≠nh precision, recall, f1 cho agreement (target vs surrogate)
    precision_agreement, recall_agreement, f1_agreement, _ = precision_recall_fscore_support(
        y_target, y_surrogate, average="binary", zero_division=0
    )
    
    # T√≠nh AUC d·ª±a tr√™n target predictions
    try:
        auc = roc_auc_score(y_target, probs_surrogate)
    except ValueError:
        auc = float("nan")
    
    # Confusion matrix (so s√°nh target predictions vs surrogate predictions)
    cm_agreement = confusion_matrix(y_target, y_surrogate)
    tn_agreement, fp_agreement, fn_agreement, tp_agreement = cm_agreement.ravel() if cm_agreement.size == 4 else (0, 0, 0, 0)
    
    # T√≠nh metrics v·ªõi ground truth n·∫øu c√≥
    if y_true is not None:
        accuracy = accuracy_score(y_true, y_surrogate)
        precision_accuracy, recall_accuracy, f1_accuracy, _ = precision_recall_fscore_support(
            y_true, y_surrogate, average="binary", zero_division=0
        )
        try:
            auc_accuracy = roc_auc_score(y_true, probs_surrogate)
        except ValueError:
            auc_accuracy = float("nan")
        cm_accuracy = confusion_matrix(y_true, y_surrogate)
        tn_accuracy, fp_accuracy, fn_accuracy, tp_accuracy = cm_accuracy.ravel() if cm_accuracy.size == 4 else (0, 0, 0, 0)
        true_dist = dict(zip(*np.unique(y_true, return_counts=True)))
    else:
        accuracy = None
        precision_accuracy = None
        recall_accuracy = None
        f1_accuracy = None
        auc_accuracy = None
        tn_accuracy = fp_accuracy = fn_accuracy = tp_accuracy = None
        true_dist = None
    
    # Ph√¢n b·ªë predictions
    target_dist = dict(zip(*np.unique(y_target, return_counts=True)))
    surrogate_dist = dict(zip(*np.unique(y_surrogate, return_counts=True)))
    
    metrics = {
        "model_name": model_name,
        "accuracy": float(accuracy) if accuracy is not None else None,
        "agreement": float(agreement),
        "auc": float(auc) if not np.isnan(auc) else None,
        "auc_accuracy": float(auc_accuracy) if (auc_accuracy is not None and not np.isnan(auc_accuracy)) else None,
        "precision_agreement": float(precision_agreement),
        "recall_agreement": float(recall_agreement),
        "f1_agreement": float(f1_agreement),
        "precision_accuracy": float(precision_accuracy) if precision_accuracy is not None else None,
        "recall_accuracy": float(recall_accuracy) if recall_accuracy is not None else None,
        "f1_accuracy": float(f1_accuracy) if f1_accuracy is not None else None,
        "confusion_matrix_agreement": {
            "tn": int(tn_agreement),
            "fp": int(fp_agreement),
            "fn": int(fn_agreement),
            "tp": int(tp_agreement)
        },
        "confusion_matrix_accuracy": {
            "tn": int(tn_accuracy),
            "fp": int(fp_accuracy),
            "fn": int(fn_accuracy),
            "tp": int(tp_accuracy)
        } if tn_accuracy is not None else None,
        "target_distribution": {int(k): int(v) for k, v in target_dist.items()},
        "surrogate_distribution": {int(k): int(v) for k, v in surrogate_dist.items()},
        "ground_truth_distribution": {int(k): int(v) for k, v in true_dist.items()} if true_dist is not None else None,
    }
    
    # In k·∫øt qu·∫£
    print(f"  Agreement: {agreement:.4f} (t·ªâ l·ªá nh·∫•t qu√°n: surrogate predictions kh·ªõp v·ªõi target predictions)")
    if accuracy is not None:
        print(f"  Accuracy: {accuracy:.4f} (t·ªâ l·ªá ch√≠nh x√°c: surrogate predictions kh·ªõp v·ªõi ground truth)")
    else:
        print(f"  ‚ö†Ô∏è  Accuracy: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh")
    print(f"  AUC (vs target): {auc:.4f}" if not np.isnan(auc) else "  AUC (vs target): NaN")
    if auc_accuracy is not None and not np.isnan(auc_accuracy):
        print(f"  AUC (vs ground truth): {auc_accuracy:.4f}")
    
    return metrics


def main():
    parser = argparse.ArgumentParser(description="ƒê√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target v√† surrogate model (LightGBM)")
    parser.add_argument("--surrogate_model_path", type=str, required=True,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file surrogate model (LightGBM .txt, .lgb, .pkl)")
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="Threshold ƒë·ªÉ chuy·ªÉn probabilities th√†nh binary labels (m·∫∑c ƒë·ªãnh: 0.5)")
    parser.add_argument("--test_parquet", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn test data parquet file (m·∫∑c ƒë·ªãnh: ember_2018_v2 train data)")
    parser.add_argument("--target_model_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn target model (m·∫∑c ƒë·ªãnh: artifacts/targets/LEE.lgb)")
    parser.add_argument("--target_model_name", type=str, default="LEE",
                       help="T√™n target model (m·∫∑c ƒë·ªãnh: LEE)")
    parser.add_argument("--normalization_stats_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn normalization stats .npz file (t·ª± ƒë·ªông t√¨m n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh)")
    
    args = parser.parse_args()
    
    # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n surrogate model
    surrogate_model_path = args.surrogate_model_path
    if not Path(surrogate_model_path).is_absolute():
        surrogate_model_path = str((PROJECT_ROOT / surrogate_model_path).resolve())
    
    if not Path(surrogate_model_path).exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y surrogate model t·∫°i: {surrogate_model_path}")
    
    # X·ª≠ l√Ω threshold
    threshold = args.threshold
    if not (0.0 <= threshold <= 1.0):
        raise ValueError(f"‚ùå Threshold ph·∫£i n·∫±m trong kho·∫£ng [0.0, 1.0], nh·∫≠n ƒë∆∞·ª£c: {threshold}")
    
    # X·ª≠ l√Ω test data path
    if args.test_parquet:
        test_parquet = args.test_parquet
        if not Path(test_parquet).is_absolute():
            test_parquet = str((PROJECT_ROOT / test_parquet).resolve())
    else:
        test_parquet = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_minus1.parquet")
    
    # X·ª≠ l√Ω target model path
    if args.target_model_path:
        target_model_path = args.target_model_path
        if not Path(target_model_path).is_absolute():
            target_model_path = str((PROJECT_ROOT / target_model_path).resolve())
    else:
        target_model_path = str(PROJECT_ROOT / "artifacts" / "targets" / f"{args.target_model_name}.lgb")
    
    target_model_name = args.target_model_name
    
    # T√¨m normalization stats - ∆∞u ti√™n trong output directory
    if args.normalization_stats_path:
        normalization_stats_path = args.normalization_stats_path
        if not Path(normalization_stats_path).is_absolute():
            normalization_stats_path = str((PROJECT_ROOT / normalization_stats_path).resolve())
        if not Path(normalization_stats_path).exists():
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y normalization stats t·∫°i: {normalization_stats_path}")
    else:
        surrogate_output_dir = Path(surrogate_model_path).parent
        possible_stats_paths = [
            surrogate_output_dir / "normalization_stats.npz",  # ∆Øu ti√™n nh·∫•t
            PROJECT_ROOT / "artifacts" / "targets" / f"{target_model_name}_normalization_stats.npz",  # Target model stats
            PROJECT_ROOT / "artifacts" / "targets" / "normalization_stats.npz",  # Generic stats
        ]
        
        normalization_stats_path = None
        for path in possible_stats_paths:
            if path.exists():
                normalization_stats_path = str(path.resolve())
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, s·∫Ω d√πng target model stats
        if normalization_stats_path is None:
            normalization_stats_path = str(PROJECT_ROOT / "artifacts" / "targets" / f"{target_model_name}_normalization_stats.npz")
            if not Path(normalization_stats_path).exists():
                normalization_stats_path = None
    
    output_dir = PROJECT_ROOT / "logs" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS (LightGBM)")
    print("=" * 80)
    print(f"Target Model: {target_model_path}")
    print(f"Target Model Name: {target_model_name}")
    print(f"Surrogate Model: {surrogate_model_path}")
    print(f"Test Data: {test_parquet}")
    print(f"Threshold: {threshold}")
    if normalization_stats_path:
        print(f"Normalization Stats: {normalization_stats_path}")
    else:
        print(f"Normalization Stats: Kh√¥ng c√≥ (s·∫Ω predict kh√¥ng normalize)")
    
    # Load feature columns
    feature_cols = get_feature_columns(test_parquet)
    print(f"\n‚úÖ Feature columns: {len(feature_cols)}")
    
    # Load test data (unlabeled - label -1)
    print(f"\nüîÑ ƒêang load test data t·ª´ {test_parquet}...")
    print(f"    (Dataset v·ªõi label -1, kh√¥ng c√≥ ground truth labels)")
    X_test = load_test_data(test_parquet, feature_cols, max_samples=10000, load_labels=False)
    y_true = None  # Kh√¥ng c√≥ ground truth labels trong dataset n√†y
    
    if len(X_test) == 0:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ test!")
        return
    
    print(f"‚úÖ ƒê√£ load {len(X_test):,} samples (unlabeled data)")
    
    # Load target model (LightGBM)
    print(f"\nüîÑ ƒêang load target model (LightGBM)...")
    try:
        target_oracle = create_oracle_from_name(
            model_name=target_model_name,
            models_dir=str(PROJECT_ROOT / "artifacts" / "targets"),
            feature_dim=len(feature_cols)
        )
        print(f"‚úÖ ƒê√£ load target model")
    except Exception as e:
        print(f"‚ùå L·ªói khi load target model: {e}")
        print(f"    ƒêang th·ª≠ load tr·ª±c ti·∫øp v·ªõi LocalOracleClient...")
        import traceback
        traceback.print_exc()
        
        # Fallback: th·ª≠ load tr·ª±c ti·∫øp
        try:
            target_norm_stats_path = PROJECT_ROOT / "artifacts" / "targets" / f"{target_model_name}_normalization_stats.npz"
            if not target_norm_stats_path.exists():
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y normalization stats t·∫°i {target_norm_stats_path}")
            
            target_oracle = LocalOracleClient(
                model_type="lgb",
                model_path=target_model_path,
                normalization_stats_path=str(target_norm_stats_path),
                threshold=threshold,
                feature_dim=len(feature_cols)
            )
            print(f"‚úÖ ƒê√£ load target model b·∫±ng LocalOracleClient tr·ª±c ti·∫øp")
        except Exception as e2:
            print(f"‚ùå L·ªói khi load tr·ª±c ti·∫øp: {e2}")
            traceback.print_exc()
            return
    
    # Query target model ƒë·ªÉ l·∫•y nh√£n th·ª±c t·∫ø
    print(f"\nüîÑ ƒêang query target model ƒë·ªÉ l·∫•y nh√£n...")
    try:
        y_target_proba = target_oracle.predict_proba(X_test)
        
        # LightGBM predict tr·∫£ v·ªÅ probability c·ªßa class 1 (malware) - shape: (n_samples,)
        # Chuy·ªÉn th√†nh binary labels v·ªõi threshold
        if y_target_proba.ndim == 1:
            y_target = (y_target_proba >= threshold).astype(int)
        elif y_target_proba.ndim == 2 and y_target_proba.shape[1] == 2:
            y_target = (y_target_proba[:, 1] >= threshold).astype(int)
        else:
            y_target_proba_flat = np.squeeze(y_target_proba)
            y_target = (y_target_proba_flat >= threshold).astype(int)
        
        print(f"‚úÖ ƒê√£ l·∫•y nh√£n t·ª´ target model")
        unique, counts = np.unique(y_target, return_counts=True)
        print(f"  Ph√¢n b·ªë nh√£n: {dict(zip(unique, counts))}")
    except Exception as e:
        print(f"‚ùå L·ªói khi query target model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Load surrogate model (LightGBM)
    print(f"\nüîÑ ƒêang load surrogate model (LightGBM)...")
    if not Path(surrogate_model_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y surrogate model t·∫°i {surrogate_model_path}")
        return
    
    try:
        surrogate_predict = load_LightGBM_surrogate_model(
            model_path=surrogate_model_path,
            normalization_stats_path=normalization_stats_path,
            feature_dim=len(feature_cols),
            threshold=threshold
        )
        print(f"‚úÖ ƒê√£ load surrogate model")
    except Exception as e:
        print(f"‚ùå L·ªói khi load surrogate model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ƒê√°nh gi√° surrogate model
    all_results = []
    
    try:
        metrics = evaluate_model_similarity(
            target_oracle,
            surrogate_predict,
            X_test,
            y_target,
            y_true=y_true,
            model_name=Path(surrogate_model_path).parent.name
        )
        all_results.append(metrics)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi ƒë√°nh gi√° surrogate model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # T·∫°o report
    print(f"\n{'='*80}")
    print("üìä T·∫†O B√ÅO C√ÅO")
    print(f"{'='*80}\n")
    
    report_path = output_dir / "surrogate_similarity_lgb_report.txt"
    report_md_path = output_dir / "surrogate_similarity_lgb_report.md"
    json_path = output_dir / "surrogate_similarity_lgb_results.json"
    
    # Text report
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS (LightGBM)\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("TH√îNG TIN MODELS:\n")
        f.write("-" * 80 + "\n")
        f.write(f"Target Model: {target_model_path}\n")
        f.write(f"Surrogate Model: {surrogate_model_path}\n")
        if normalization_stats_path:
            f.write(f"Normalization Stats: {normalization_stats_path}\n")
        else:
            f.write(f"Normalization Stats: Kh√¥ng c√≥\n")
        f.write("\n")
        
        f.write("TH√îNG TIN D·ªÆ LI·ªÜU TEST:\n")
        f.write("-" * 80 + "\n")
        f.write(f"File: {test_parquet}\n")
        f.write(f"S·ªë samples: {len(X_test):,}\n")
        if y_true is not None:
            unique_true, counts_true = np.unique(y_true, return_counts=True)
            f.write(f"Ph√¢n b·ªë ground truth labels: {dict(zip(unique_true, counts_true))}\n")
        unique_target, counts_target = np.unique(y_target, return_counts=True)
        f.write(f"Ph√¢n b·ªë nh√£n t·ª´ target model: {dict(zip(unique_target, counts_target))}\n\n")
        
        f.write("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:\n")
        f.write("-" * 80 + "\n\n")
        
        for result in all_results:
            f.write(f"{result['model_name'].upper().replace('_', ' ')}:\n")
            
            if result['accuracy'] is not None:
                f.write(f"  Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
                f.write(f"    ‚Üí ƒê·ªô ch√≠nh x√°c c·ªßa surrogate model so v·ªõi ground truth labels\n")
            else:
                f.write(f"  Accuracy: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh\n")
            
            f.write(f"  Agreement: {result['agreement']:.4f} ({result['agreement']*100:.2f}%)\n")
            f.write(f"    ‚Üí ƒê·ªô nh·∫•t qu√°n gi·ªØa surrogate v√† target model predictions\n")
            
            if result['auc'] is not None:
                f.write(f"  AUC (vs target): {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"  AUC (vs ground truth): {result['auc_accuracy']:.4f}\n")
            
            f.write(f"  Precision (agreement): {result.get('precision_agreement', 0):.4f}\n")
            f.write(f"  Recall (agreement): {result.get('recall_agreement', 0):.4f}\n")
            f.write(f"  F1-score (agreement): {result.get('f1_agreement', 0):.4f}\n")
            
            if result.get('precision_accuracy') is not None:
                f.write(f"  Precision (accuracy): {result['precision_accuracy']:.4f}\n")
                f.write(f"  Recall (accuracy): {result['recall_accuracy']:.4f}\n")
                f.write(f"  F1-score (accuracy): {result['f1_accuracy']:.4f}\n")
            
            cm_agreement = result.get('confusion_matrix_agreement')
            if cm_agreement:
                f.write(f"  Confusion Matrix (Agreement - Target vs Surrogate):\n")
                f.write(f"    TN: {cm_agreement['tn']}, FP: {cm_agreement['fp']}\n")
                f.write(f"    FN: {cm_agreement['fn']}, TP: {cm_agreement['tp']}\n")
            
            if result.get('confusion_matrix_accuracy'):
                cm_accuracy = result['confusion_matrix_accuracy']
                f.write(f"  Confusion Matrix (Accuracy - Ground Truth vs Surrogate):\n")
                f.write(f"    TN: {cm_accuracy['tn']}, FP: {cm_accuracy['fp']}\n")
                f.write(f"    FN: {cm_accuracy['fn']}, TP: {cm_accuracy['tp']}\n")
            
            if result.get('ground_truth_distribution'):
                f.write(f"  Ground truth distribution: {result['ground_truth_distribution']}\n")
            f.write(f"  Target distribution: {result['target_distribution']}\n")
            f.write(f"  Surrogate distribution: {result['surrogate_distribution']}\n")
            f.write("\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("T√ìM T·∫ÆT K·∫æT QU·∫¢:\n")
        f.write("=" * 80 + "\n\n")
        
        if all_results:
            result = all_results[0]
            f.write(f"Surrogate model ƒë·∫°t:\n")
            if result['accuracy'] is not None:
                f.write(f"  - Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%) - so v·ªõi ground truth\n")
            else:
                f.write(f"  - Accuracy: Kh√¥ng c√≥ ground truth labels\n")
            f.write(f"  - Agreement: {result['agreement']:.4f} ({result['agreement']*100:.2f}%) - so v·ªõi target model\n")
            if result['auc'] is not None:
                f.write(f"  - AUC (vs target): {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"  - AUC (vs ground truth): {result['auc_accuracy']:.4f}\n")
            f.write(f"  - F1-score (agreement): {result.get('f1_agreement', 0):.4f}\n")
            if result.get('f1_accuracy') is not None:
                f.write(f"  - F1-score (accuracy): {result['f1_accuracy']:.4f}\n")
    
    # Markdown report
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o ƒê√°nh Gi√° ƒê·ªô T∆∞∆°ng ƒê·ªìng Gi·ªØa Target v√† Surrogate Models (LightGBM)\n\n")
        
        f.write("## Th√¥ng Tin Models\n\n")
        f.write(f"- **Target Model**: `{target_model_path}`\n")
        f.write(f"- **Surrogate Model**: `{surrogate_model_path}`\n")
        if normalization_stats_path:
            f.write(f"- **Normalization Stats**: `{normalization_stats_path}`\n")
        else:
            f.write(f"- **Normalization Stats**: Kh√¥ng c√≥\n")
        f.write("\n")
        
        f.write("## Th√¥ng Tin D·ªØ Li·ªáu Test\n\n")
        f.write(f"- **File**: `{test_parquet}`\n")
        f.write(f"- **S·ªë samples**: {len(X_test):,}\n")
        if y_true is not None:
            unique_true, counts_true = np.unique(y_true, return_counts=True)
            f.write(f"- **Ph√¢n b·ªë ground truth labels**: {dict(zip(unique_true, counts_true))}\n")
        unique_target, counts_target = np.unique(y_target, return_counts=True)
        f.write(f"- **Ph√¢n b·ªë nh√£n t·ª´ target model**: {dict(zip(unique_target, counts_target))}\n\n")
        
        f.write("## K·∫øt Qu·∫£ ƒê√°nh Gi√°\n\n")
        f.write("### Metric Definitions\n\n")
        f.write("- **Accuracy**: ƒê·ªô ch√≠nh x√°c c·ªßa surrogate model so v·ªõi ground truth labels\n")
        f.write("- **Agreement**: ƒê·ªô nh·∫•t qu√°n gi·ªØa surrogate v√† target model predictions\n\n")
        
        f.write("| Model | Accuracy | Agreement | AUC (vs target) | AUC (vs GT) | F1 (agreement) | F1 (accuracy) |\n")
        f.write("|-------|----------|-----------|-----------------|-------------|----------------|---------------|\n")
        
        for result in all_results:
            accuracy_str = f"{result['accuracy']:.4f}" if result['accuracy'] is not None else "N/A"
            auc_target_str = f"{result['auc']:.4f}" if result['auc'] is not None else "N/A"
            auc_gt_str = f"{result.get('auc_accuracy', 'N/A'):.4f}" if result.get('auc_accuracy') is not None else "N/A"
            f1_agreement = result.get('f1_agreement', 0)
            f1_accuracy_str = f"{result['f1_accuracy']:.4f}" if result.get('f1_accuracy') is not None else "N/A"
            f.write(f"| {result['model_name']} | {accuracy_str} | "
                   f"{result['agreement']:.4f} | {auc_target_str} | {auc_gt_str} | "
                   f"{f1_agreement:.4f} | {f1_accuracy_str} |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng Model\n\n")
        
        for result in all_results:
            f.write(f"### {result['model_name'].replace('_', ' ').title()}\n\n")
            
            if result['accuracy'] is not None:
                f.write(f"- **Accuracy**: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%) - so v·ªõi ground truth\n")
            else:
                f.write(f"- **Accuracy**: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh\n")
            
            f.write(f"- **Agreement**: {result['agreement']:.4f} ({result['agreement']*100:.2f}%) - so v·ªõi target model\n")
            
            if result['auc'] is not None:
                f.write(f"- **AUC (vs target)**: {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"- **AUC (vs ground truth)**: {result['auc_accuracy']:.4f}\n")
            
            f.write(f"- **Precision (agreement)**: {result.get('precision_agreement', 0):.4f}\n")
            f.write(f"- **Recall (agreement)**: {result.get('recall_agreement', 0):.4f}\n")
            f.write(f"- **F1-score (agreement)**: {result.get('f1_agreement', 0):.4f}\n")
            
            if result.get('precision_accuracy') is not None:
                f.write(f"- **Precision (accuracy)**: {result['precision_accuracy']:.4f}\n")
                f.write(f"- **Recall (accuracy)**: {result['recall_accuracy']:.4f}\n")
                f.write(f"- **F1-score (accuracy)**: {result['f1_accuracy']:.4f}\n")
            f.write("\n")
            
            cm_agreement = result.get('confusion_matrix_agreement')
            if cm_agreement:
                f.write("**Confusion Matrix (Agreement - Target vs Surrogate):**\n\n")
                f.write(f"| | Predicted 0 | Predicted 1 |\n")
                f.write(f"|------|------------|-------------|\n")
                f.write(f"| Target 0 | {cm_agreement['tn']} | {cm_agreement['fp']} |\n")
                f.write(f"| Target 1 | {cm_agreement['fn']} | {cm_agreement['tp']} |\n\n")
            
            if result.get('confusion_matrix_accuracy'):
                cm_accuracy = result['confusion_matrix_accuracy']
                f.write("**Confusion Matrix (Accuracy - Ground Truth vs Surrogate):**\n\n")
                f.write(f"| | Predicted 0 | Predicted 1 |\n")
                f.write(f"|------|------------|-------------|\n")
                f.write(f"| Actual 0 | {cm_accuracy['tn']} | {cm_accuracy['fp']} |\n")
                f.write(f"| Actual 1 | {cm_accuracy['fn']} | {cm_accuracy['tp']} |\n\n")
            
            if result.get('ground_truth_distribution'):
                f.write(f"- **Ground truth distribution**: {result['ground_truth_distribution']}\n")
            f.write(f"- **Target distribution**: {result['target_distribution']}\n")
            f.write(f"- **Surrogate distribution**: {result['surrogate_distribution']}\n\n")
    
    # JSON results
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "test_info": {
                "file": test_parquet,
                "num_samples": int(len(X_test)),
                "target_distribution": {int(k): int(v) for k, v in dict(zip(*np.unique(y_target, return_counts=True))).items()}
            },
            "results": all_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON results: {json_path}")
    
    # In t√≥m t·∫Øt
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    if all_results:
        df = pd.DataFrame(all_results)
        cols_to_show = ["model_name", "accuracy", "agreement", "auc"]
        if "f1_agreement" in df.columns:
            cols_to_show.append("f1_agreement")
        if "f1_accuracy" in df.columns and df["f1_accuracy"].notna().any():
            cols_to_show.append("f1_accuracy")
        print(df[cols_to_show].to_string(index=False))
    
    return all_results


if __name__ == "__main__":
    main()

