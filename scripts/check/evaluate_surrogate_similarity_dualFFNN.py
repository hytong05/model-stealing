"""
Script ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target model v√† surrogate model
S·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ test_ember_2018_v2_features_label_minus1.parquet (unlabeled data)
Khi kh√¥ng c√≥ ground truth labels, ch·ªâ t√≠nh Agreement (kh√¥ng t√≠nh Accuracy)
"""
# QUAN TR·ªåNG: Set environment variable TR∆Ø·ªöC KHI import TensorFlow ƒë·ªÉ d√πng legacy Keras
import os
os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "3")  # 3 = FATAL only (·∫©n ERROR, WARNING, INFO)

import argparse
import json
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import joblib
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix
from sklearn.preprocessing import RobustScaler

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.targets.oracle_client import create_oracle_from_name, LocalOracleClient
from src.models.dnn import create_dnn2
import tensorflow as tf


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """L·∫•y danh s√°ch feature columns t·ª´ parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_test_data(parquet_path: str, feature_cols: list, max_samples: int = 10000, load_labels: bool = False):
    """
    Load d·ªØ li·ªáu test t·ª´ parquet file.
    
    Args:
        parquet_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file parquet
        feature_cols: Danh s√°ch feature columns
        max_samples: S·ªë l∆∞·ª£ng samples t·ªëi ƒëa
        load_labels: N·∫øu True, s·∫Ω load c·∫£ labels v√† ch·ªâ l·∫•y samples c√≥ label 0 ho·∫∑c 1 (ground truth)
                    N·∫øu False, load t·∫•t c·∫£ samples (b·ªè qua labels)
    
    Returns:
        X: Feature array (n_samples, n_features)
        y_true: Ground truth labels (n_samples,) - ch·ªâ tr·∫£ v·ªÅ n·∫øu load_labels=True
    """
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    all_y = [] if load_labels else None
    rows_loaded = 0
    
    print(f"üîÑ ƒêang load d·ªØ li·ªáu t·ª´ {parquet_path}...")
    
    for batch in pq_file.iter_batches(batch_size=5000, columns=feature_cols + ["Label"]):
        if rows_loaded >= max_samples:
            break
            
        batch_df = batch.to_pandas()
        
        if load_labels:
            # Load c·∫£ labels v√† ch·ªâ l·∫•y samples c√≥ label 0 ho·∫∑c 1 (ground truth)
            y = batch_df["Label"].values.astype(np.int32)
            valid_mask = (y >= 0) & (y <= 1)  # Ch·ªâ l·∫•y label 0 ho·∫∑c 1
            
            if valid_mask.sum() == 0:
                continue
            
            batch_df = batch_df[valid_mask]
            y = y[valid_mask]
            
            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            all_X.append(X)
            all_y.append(y)
            rows_loaded += len(X)
        else:
            # L·∫•y t·∫•t c·∫£ samples (kh√¥ng quan t√¢m nh√£n v√¨ s·∫Ω query target model)
            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            all_X.append(X)
            rows_loaded += len(X)
        
        if rows_loaded % 5000 == 0:
            print(f"  ƒê√£ load {rows_loaded:,}/{max_samples:,} samples...")
        
        if rows_loaded >= max_samples:
            break
    
    if all_X:
        X_concat = np.concatenate(all_X, axis=0)
        if len(X_concat) > max_samples:
            X_concat = X_concat[:max_samples]
        
        if load_labels:
            y_concat = np.concatenate(all_y, axis=0) if all_y else None
            if y_concat is not None and len(y_concat) > max_samples:
                y_concat = y_concat[:max_samples]
            print(f"‚úÖ ƒê√£ load {len(X_concat):,} samples v·ªõi ground truth labels")
            return X_concat, y_concat
        else:
            print(f"‚úÖ ƒê√£ load {len(X_concat):,} samples")
            return X_concat
    else:
        if load_labels:
            return np.empty((0, len(feature_cols)), dtype=np.float32), np.empty((0,), dtype=np.int32)
        else:
            return np.empty((0, len(feature_cols)), dtype=np.float32)


def load_dualDNN_surrogate_model(model_path: str, scaler_path: str, feature_dim: int = 2381, threshold: float = 0.5):
    """
    Load dualDNN surrogate model v√† scaler.
    dualDNN c·∫ßn 2 inputs: (X_scaled, y_true) khi predict.
    """
    # Environment variables ƒë√£ ƒë∆∞·ª£c set ·ªü ƒë·∫ßu file
    
    # Load scaler
    if scaler_path and os.path.exists(scaler_path):
        print(f"üîÑ ƒêang load scaler t·ª´ {scaler_path}...")
        scaler = joblib.load(scaler_path)
        print(f"‚úÖ ƒê√£ load scaler")
    else:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y scaler t·∫°i {scaler_path}, t·∫°o RobustScaler m·ªõi...")
        scaler = RobustScaler()
        # Note: scaler s·∫Ω c·∫ßn ƒë∆∞·ª£c fit tr∆∞·ªõc khi s·ª≠ d·ª•ng
    
    # Load model - rebuild architecture v√† load weights
    # L∆∞u √Ω: Model dualDNN ƒë∆∞·ª£c train v·ªõi Keras 2, n√™n c·∫ßn rebuild architecture
    # thay v√¨ load tr·ª±c ti·∫øp ƒë·ªÉ tr√°nh compatibility issues
    print(f"üîÑ ƒêang load dualDNN model t·ª´ {model_path}...")
    print(f"    (ƒêang rebuild architecture v√† load weights do Keras version compatibility)")
    model = None
    try:
        # Rebuild model architecture
        model = create_dnn2(seed=42, mc=False, input_shape=(feature_dim,))
        # Load weights t·ª´ file .h5
        model.load_weights(model_path)
        print(f"‚úÖ ƒê√£ rebuild architecture v√† load weights th√†nh c√¥ng")
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ load model: {e}")
        print(f"    ƒêang th·ª≠ c√°ch kh√°c...")
        # Fallback: th·ª≠ load tr·ª±c ti·∫øp (n·∫øu m√¥i tr∆∞·ªùng h·ªó tr·ª£)
        try:
            model = tf.keras.models.load_model(model_path, compile=False)
            print(f"‚úÖ ƒê√£ load model tr·ª±c ti·∫øp (fallback)")
        except Exception as e2:
            print(f"‚ùå Kh√¥ng th·ªÉ load model v·ªõi b·∫•t k·ª≥ c√°ch n√†o:")
            print(f"   L·ªói rebuild: {e}")
            print(f"   L·ªói load tr·ª±c ti·∫øp: {e2}")
            raise e2
    
    print(f"‚úÖ ƒê√£ load dualDNN model")
    
    def predict(X, y_true):
        """
        Predict v·ªõi dualDNN model.
        
        Args:
            X: Raw feature array (shape: [n_samples, n_features])
            y_true: Ground truth labels ho·∫∑c target predictions (shape: [n_samples,])
        
        Returns:
            (predictions, probabilities): predictions l√† binary classes, probabilities l√† raw outputs
        """
        # Scale data v·ªõi RobustScaler v√† clip v·ªÅ [-5, 5]
        X_scaled = scaler.transform(X)
        X_scaled = np.clip(X_scaled, -5, 5)
        
        # dualDNN c·∫ßn y_true l√†m input th·ª© 2
        y_true_reshaped = y_true.reshape(-1, 1)  # Reshape th√†nh [n_samples, 1]
        
        # Predict v·ªõi 2 inputs: (X_scaled, y_true)
        probs = np.squeeze(model.predict((X_scaled, y_true_reshaped), verbose=0), axis=-1)
        
        # N·∫øu model output l√† 2D (softmax), l·∫•y class 1
        if probs.ndim > 1 and probs.shape[-1] == 2:
            probs = probs[:, 1]
        
        predictions = (probs >= threshold).astype(int)
        return predictions, probs
    
    return predict, scaler


def evaluate_model_similarity(
    target_model,
    surrogate_predict,
    X_test,
    y_target,
    y_true=None,
    model_name: str = "",
    is_dualDNN: bool = False
):
    """
    ƒê√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target v√† surrogate model.
    
    Args:
        target_model: Target model oracle
        surrogate_predict: Surrogate model predict function
        X_test: Test features
        y_target: Predictions t·ª´ target model
        y_true: Ground truth labels (n·∫øu c√≥) - ƒë·ªÉ t√≠nh Accuracy th·∫≠t s·ª±
        model_name: T√™n model
        is_dualDNN: C√≥ ph·∫£i dualDNN model kh√¥ng
    
    Returns:
        metrics dict v·ªõi accuracy (n·∫øu c√≥ y_true) v√† agreement
    """
    print(f"\nüîÑ ƒêang ƒë√°nh gi√° {model_name}...")
    
    # Predict v·ªõi surrogate
    # dualDNN c·∫ßn y_target l√†m input th·ª© 2
    if is_dualDNN:
        y_surrogate, probs_surrogate = surrogate_predict(X_test, y_target)
    else:
        y_surrogate, probs_surrogate = surrogate_predict(X_test)
    
    # T√≠nh metrics
    # Agreement = t·ªâ l·ªá nh·∫•t qu√°n gi·ªØa target v√† surrogate predictions
    agreement = accuracy_score(y_target, y_surrogate)
    
    # Accuracy = t·ªâ l·ªá ch√≠nh x√°c c·ªßa surrogate so v·ªõi ground truth (n·∫øu c√≥)
    if y_true is not None:
        accuracy = accuracy_score(y_true, y_surrogate)
    else:
        accuracy = None
    
    # T√≠nh precision, recall, f1 cho agreement (target vs surrogate)
    precision_agreement, recall_agreement, f1_agreement, _ = precision_recall_fscore_support(
        y_target, y_surrogate, average="binary", zero_division=0
    )
    
    # T√≠nh AUC d·ª±a tr√™n target predictions
    try:
        auc = roc_auc_score(y_target, probs_surrogate)
    except ValueError:
        auc = float("nan")
    
    # Confusion matrix (so s√°nh target predictions vs surrogate predictions)
    cm_agreement = confusion_matrix(y_target, y_surrogate)
    tn_agreement, fp_agreement, fn_agreement, tp_agreement = cm_agreement.ravel() if cm_agreement.size == 4 else (0, 0, 0, 0)
    
    # T√≠nh metrics v·ªõi ground truth n·∫øu c√≥
    if y_true is not None:
        accuracy = accuracy_score(y_true, y_surrogate)
        precision_accuracy, recall_accuracy, f1_accuracy, _ = precision_recall_fscore_support(
            y_true, y_surrogate, average="binary", zero_division=0
        )
        try:
            auc_accuracy = roc_auc_score(y_true, probs_surrogate)
        except ValueError:
            auc_accuracy = float("nan")
        cm_accuracy = confusion_matrix(y_true, y_surrogate)
        tn_accuracy, fp_accuracy, fn_accuracy, tp_accuracy = cm_accuracy.ravel() if cm_accuracy.size == 4 else (0, 0, 0, 0)
        true_dist = dict(zip(*np.unique(y_true, return_counts=True)))
    else:
        accuracy = None
        precision_accuracy = None
        recall_accuracy = None
        f1_accuracy = None
        auc_accuracy = None
        tn_accuracy = fp_accuracy = fn_accuracy = tp_accuracy = None
        true_dist = None
    
    # Ph√¢n b·ªë predictions
    target_dist = dict(zip(*np.unique(y_target, return_counts=True)))
    surrogate_dist = dict(zip(*np.unique(y_surrogate, return_counts=True)))
    
    metrics = {
        "model_name": model_name,
        "accuracy": float(accuracy) if accuracy is not None else None,
        "agreement": float(agreement),
        "auc": float(auc) if not np.isnan(auc) else None,
        "auc_accuracy": float(auc_accuracy) if (auc_accuracy is not None and not np.isnan(auc_accuracy)) else None,
        "precision_agreement": float(precision_agreement),
        "recall_agreement": float(recall_agreement),
        "f1_agreement": float(f1_agreement),
        "precision_accuracy": float(precision_accuracy) if precision_accuracy is not None else None,
        "recall_accuracy": float(recall_accuracy) if recall_accuracy is not None else None,
        "f1_accuracy": float(f1_accuracy) if f1_accuracy is not None else None,
        "confusion_matrix_agreement": {
            "tn": int(tn_agreement),
            "fp": int(fp_agreement),
            "fn": int(fn_agreement),
            "tp": int(tp_agreement)
        },
        "confusion_matrix_accuracy": {
            "tn": int(tn_accuracy),
            "fp": int(fp_accuracy),
            "fn": int(fn_accuracy),
            "tp": int(tp_accuracy)
        } if tn_accuracy is not None else None,
        "target_distribution": {int(k): int(v) for k, v in target_dist.items()},
        "surrogate_distribution": {int(k): int(v) for k, v in surrogate_dist.items()},
        "ground_truth_distribution": {int(k): int(v) for k, v in true_dist.items()} if true_dist is not None else None,
    }
    
    # In k·∫øt qu·∫£
    print(f"  Agreement: {agreement:.4f} (t·ªâ l·ªá nh·∫•t qu√°n: surrogate predictions kh·ªõp v·ªõi target predictions)")
    if accuracy is not None:
        print(f"  Accuracy: {accuracy:.4f} (t·ªâ l·ªá ch√≠nh x√°c: surrogate predictions kh·ªõp v·ªõi ground truth)")
    else:
        print(f"  ‚ö†Ô∏è  Accuracy: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh")
    print(f"  AUC (vs target): {auc:.4f}" if not np.isnan(auc) else "  AUC (vs target): NaN")
    if auc_accuracy is not None and not np.isnan(auc_accuracy):
        print(f"  AUC (vs ground truth): {auc_accuracy:.4f}")
    
    return metrics


def main():
    parser = argparse.ArgumentParser(description="ƒê√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target v√† surrogate model (dualDNN)")
    parser.add_argument("--surrogate_model_path", type=str, required=True,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file surrogate model (dualDNN .h5)")
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="Threshold ƒë·ªÉ chuy·ªÉn probabilities th√†nh binary labels (m·∫∑c ƒë·ªãnh: 0.5)")
    parser.add_argument("--test_parquet", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn test data parquet file (m·∫∑c ƒë·ªãnh: ember_2018_v2 train data)")
    parser.add_argument("--target_model_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn target model (m·∫∑c ƒë·ªãnh: artifacts/targets/LEE.lgb)")
    parser.add_argument("--target_model_name", type=str, default="LEE",
                       help="T√™n target model (m·∫∑c ƒë·ªãnh: LEE)")
    parser.add_argument("--scaler_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn scaler .joblib file (t·ª± ƒë·ªông t√¨m n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh)")
    
    args = parser.parse_args()
    
    # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n surrogate model
    surrogate_model_path = args.surrogate_model_path
    if not Path(surrogate_model_path).is_absolute():
        surrogate_model_path = str((PROJECT_ROOT / surrogate_model_path).resolve())
    
    if not Path(surrogate_model_path).exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y surrogate model t·∫°i: {surrogate_model_path}")
    
    # X·ª≠ l√Ω threshold
    threshold = args.threshold
    if not (0.0 <= threshold <= 1.0):
        raise ValueError(f"‚ùå Threshold ph·∫£i n·∫±m trong kho·∫£ng [0.0, 1.0], nh·∫≠n ƒë∆∞·ª£c: {threshold}")
    
    # X·ª≠ l√Ω test data path
    if args.test_parquet:
        test_parquet = args.test_parquet
        if not Path(test_parquet).is_absolute():
            test_parquet = str((PROJECT_ROOT / test_parquet).resolve())
    else:
        test_parquet = str(PROJECT_ROOT / "data" / "ember_2018_v2" / "train" / "train_ember_2018_v2_features_label_minus1.parquet")
    
    # X·ª≠ l√Ω target model path
    if args.target_model_path:
        target_model_path = args.target_model_path
        if not Path(target_model_path).is_absolute():
            target_model_path = str((PROJECT_ROOT / target_model_path).resolve())
    else:
        target_model_path = str(PROJECT_ROOT / "artifacts" / "targets" / f"{args.target_model_name}.lgb")
    
    target_model_name = args.target_model_name
    
    # T√¨m scaler - ∆∞u ti√™n trong output directory t∆∞∆°ng ·ª©ng v·ªõi model name
    if args.scaler_path:
        scaler_path = args.scaler_path
        if not Path(scaler_path).is_absolute():
            scaler_path = str((PROJECT_ROOT / scaler_path).resolve())
        if not Path(scaler_path).exists():
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y scaler t·∫°i: {scaler_path}")
        scaler_path = Path(scaler_path)
    else:
        surrogate_name = Path(surrogate_model_path).stem  # LEE-ember-dualDNN-2000
        possible_scaler_paths = [
            PROJECT_ROOT / "output" / surrogate_name / "robust_scaler.joblib",  # ∆Øu ti√™n nh·∫•t
            Path(surrogate_model_path).parent / "robust_scaler.joblib",  # C√πng th∆∞ m·ª•c v·ªõi model
            PROJECT_ROOT / "storage" / "dualDNN" / "robust_scaler.joblib",  # Th∆∞ m·ª•c storage
        ]
        
        scaler_path = None
        for path in possible_scaler_paths:
            if path.exists():
                scaler_path = path
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, s·∫Ω t·∫°o scaler m·ªõi
        if scaler_path is None:
            scaler_path = PROJECT_ROOT / "output" / surrogate_name / "robust_scaler.joblib"  # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh ƒë·ªÉ hi·ªÉn th·ªã
    
    output_dir = PROJECT_ROOT / "logs" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS")
    print("=" * 80)
    print(f"Target Model: {target_model_path}")
    print(f"Target Model Name: {target_model_name}")
    print(f"Surrogate Model: {surrogate_model_path}")
    print(f"Test Data: {test_parquet}")
    print(f"Threshold: {threshold}")
    print(f"Scaler: {scaler_path}")
    
    # Load feature columns
    feature_cols = get_feature_columns(test_parquet)
    print(f"\n‚úÖ Feature columns: {len(feature_cols)}")
    
    # Load test data (unlabeled - label -1)
    # L∆∞u √Ω: Dataset n√†y kh√¥ng c√≥ ground truth labels, ch·ªâ t√≠nh Agreement (kh√¥ng t√≠nh Accuracy)
    print(f"\nüîÑ ƒêang load test data t·ª´ {test_parquet}...")
    print(f"    (Dataset v·ªõi label -1, kh√¥ng c√≥ ground truth labels)")
    X_test = load_test_data(test_parquet, feature_cols, max_samples=10000, load_labels=False)
    y_true = None  # Kh√¥ng c√≥ ground truth labels trong dataset n√†y
    
    if len(X_test) == 0:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ test!")
        return
    
    print(f"‚úÖ ƒê√£ load {len(X_test):,} samples (unlabeled data)")
    
    # Load target model (LightGBM)
    print(f"\nüîÑ ƒêang load target model (LightGBM)...")
    try:
        # D√πng create_oracle_from_name v·ªõi models_dir ƒë·ªÉ t·ª± ƒë·ªông detect
        # H√†m n√†y s·∫Ω t·ª± t√¨m model v√† normalization stats trong th∆∞ m·ª•c
        target_oracle = create_oracle_from_name(
            model_name=target_model_name,
            models_dir=str(PROJECT_ROOT / "artifacts" / "targets"),
            feature_dim=len(feature_cols)
        )
        print(f"‚úÖ ƒê√£ load target model")
    except Exception as e:
        print(f"‚ùå L·ªói khi load target model: {e}")
        print(f"    ƒêang th·ª≠ load tr·ª±c ti·∫øp v·ªõi LocalOracleClient...")
        import traceback
        traceback.print_exc()
        
        # Fallback: th·ª≠ load tr·ª±c ti·∫øp
        try:
            from src.targets.oracle_client import LocalOracleClient
            # T√¨m normalization stats
            target_norm_stats_path = PROJECT_ROOT / "artifacts" / "targets" / f"{target_model_name}_normalization_stats.npz"
            if not target_norm_stats_path.exists():
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y normalization stats t·∫°i {target_norm_stats_path}")
            
            target_oracle = LocalOracleClient(
                model_type="lgb",
                model_path=target_model_path,
                normalization_stats_path=str(target_norm_stats_path),
                threshold=threshold,
                feature_dim=len(feature_cols)
            )
            print(f"‚úÖ ƒê√£ load target model b·∫±ng LocalOracleClient tr·ª±c ti·∫øp")
        except Exception as e2:
            print(f"‚ùå L·ªói khi load tr·ª±c ti·∫øp: {e2}")
            traceback.print_exc()
            return
    
    # Query target model ƒë·ªÉ l·∫•y nh√£n th·ª±c t·∫ø
    print(f"\nüîÑ ƒêang query target model ƒë·ªÉ l·∫•y nh√£n...")
    try:
        # FlexibleLGBTarget.predict_proba() tr·∫£ v·ªÅ probability c·ªßa class 1 (malware) - 1D array
        # Data s·∫Ω ƒë∆∞·ª£c normalize t·ª± ƒë·ªông b·ªüi FlexibleLGBTarget tr∆∞·ªõc khi predict
        # (gi·ªëng nh∆∞ trong notebook: X = (X - feature_means) / feature_stds)
        y_target_proba = target_oracle.predict_proba(X_test)
        
        # LightGBM predict tr·∫£ v·ªÅ probability c·ªßa class 1 (malware) - shape: (n_samples,)
        # Chuy·ªÉn th√†nh binary labels v·ªõi threshold
        if y_target_proba.ndim == 1:
            # 1D array: probabilities c·ªßa class 1
            y_target = (y_target_proba >= threshold).astype(int)
        elif y_target_proba.ndim == 2 and y_target_proba.shape[1] == 2:
            # 2D array v·ªõi 2 columns: [prob_class_0, prob_class_1]
            y_target = (y_target_proba[:, 1] >= threshold).astype(int)
        else:
            # Fallback: x·ª≠ l√Ω nh∆∞ 1D
            y_target_proba_flat = np.squeeze(y_target_proba)
            y_target = (y_target_proba_flat >= threshold).astype(int)
        
        print(f"‚úÖ ƒê√£ l·∫•y nh√£n t·ª´ target model")
        unique, counts = np.unique(y_target, return_counts=True)
        print(f"  Ph√¢n b·ªë nh√£n: {dict(zip(unique, counts))}")
    except Exception as e:
        print(f"‚ùå L·ªói khi query target model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Load surrogate model (dualDNN)
    print(f"\nüîÑ ƒêang load surrogate model (dualDNN)...")
    if not Path(surrogate_model_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y surrogate model t·∫°i {surrogate_model_path}")
        return
    
    # Ki·ªÉm tra scaler path
    scaler_exists = scaler_path is not None and scaler_path.exists()
    if not scaler_exists:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y scaler, s·∫Ω t·∫°o v√† fit scaler m·ªõi v·ªõi d·ªØ li·ªáu test")
        print(f"    (ƒê√£ t√¨m trong: {[str(p) for p in possible_scaler_paths[:2]]})")
    
    try:
        surrogate_predict, scaler = load_dualDNN_surrogate_model(
            model_path=surrogate_model_path,
            scaler_path=str(scaler_path) if scaler_exists else None,
            feature_dim=len(feature_cols),
            threshold=threshold
        )
        
        # N·∫øu scaler ch∆∞a ƒë∆∞·ª£c fit (kh√¥ng t√¨m th·∫•y file), c·∫ßn fit v·ªõi d·ªØ li·ªáu test
        if not scaler_exists:
            print(f"üîÑ ƒêang fit scaler v·ªõi d·ªØ li·ªáu test...")
            scaler.fit(X_test)
            print(f"‚úÖ ƒê√£ fit scaler v·ªõi {len(X_test):,} samples")
        
        print(f"‚úÖ ƒê√£ load surrogate model")
    except Exception as e:
        print(f"‚ùå L·ªói khi load surrogate model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ƒê√°nh gi√° surrogate model
    all_results = []
    
    try:
        metrics = evaluate_model_similarity(
            target_oracle,
            surrogate_predict,
            X_test,
            y_target,
            y_true=y_true,  # Truy·ªÅn ground truth labels ƒë·ªÉ t√≠nh Accuracy
            model_name=Path(surrogate_model_path).parent.name,
            is_dualDNN=True
        )
        all_results.append(metrics)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi ƒë√°nh gi√° surrogate model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # T·∫°o report
    print(f"\n{'='*80}")
    print("üìä T·∫†O B√ÅO C√ÅO")
    print(f"{'='*80}\n")
    
    report_path = output_dir / "surrogate_similarity_report.txt"
    report_md_path = output_dir / "surrogate_similarity_report.md"
    json_path = output_dir / "surrogate_similarity_results.json"
    
    # Text report
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("TH√îNG TIN MODELS:\n")
        f.write("-" * 80 + "\n")
        f.write(f"Target Model: {target_model_path}\n")
        f.write(f"Surrogate Model: {surrogate_model_path}\n")
        if scaler_exists and scaler_path is not None:
            f.write(f"Scaler: {scaler_path}\n")
        else:
            f.write(f"Scaler: Kh√¥ng t√¨m th·∫•y (ƒë√£ t·∫°o v√† fit m·ªõi v·ªõi d·ªØ li·ªáu test)\n")
        f.write("\n")
        
        f.write("TH√îNG TIN D·ªÆ LI·ªÜU TEST:\n")
        f.write("-" * 80 + "\n")
        f.write(f"File: {test_parquet}\n")
        f.write(f"S·ªë samples: {len(X_test):,}\n")
        if y_true is not None:
            unique_true, counts_true = np.unique(y_true, return_counts=True)
            f.write(f"Ph√¢n b·ªë ground truth labels: {dict(zip(unique_true, counts_true))}\n")
        unique_target, counts_target = np.unique(y_target, return_counts=True)
        f.write(f"Ph√¢n b·ªë nh√£n t·ª´ target model: {dict(zip(unique_target, counts_target))}\n\n")
        
        f.write("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:\n")
        f.write("-" * 80 + "\n\n")
        
        for result in all_results:
            f.write(f"{result['model_name'].upper().replace('_', ' ')}:\n")
            
            # Accuracy: so s√°nh v·ªõi ground truth (n·∫øu c√≥)
            if result['accuracy'] is not None:
                f.write(f"  Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
                f.write(f"    ‚Üí ƒê·ªô ch√≠nh x√°c c·ªßa surrogate model so v·ªõi ground truth labels\n")
            else:
                f.write(f"  Accuracy: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh\n")
            
            # Agreement: so s√°nh v·ªõi target model predictions
            f.write(f"  Agreement: {result['agreement']:.4f} ({result['agreement']*100:.2f}%)\n")
            f.write(f"    ‚Üí ƒê·ªô nh·∫•t qu√°n gi·ªØa surrogate v√† target model predictions\n")
            
            # AUC
            if result['auc'] is not None:
                f.write(f"  AUC (vs target): {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"  AUC (vs ground truth): {result['auc_accuracy']:.4f}\n")
            
            # Precision, Recall, F1 cho Agreement
            f.write(f"  Precision (agreement): {result.get('precision_agreement', result.get('precision', 0)):.4f}\n")
            f.write(f"  Recall (agreement): {result.get('recall_agreement', result.get('recall', 0)):.4f}\n")
            f.write(f"  F1-score (agreement): {result.get('f1_agreement', result.get('f1_score', 0)):.4f}\n")
            
            # Precision, Recall, F1 cho Accuracy (n·∫øu c√≥)
            if result.get('precision_accuracy') is not None:
                f.write(f"  Precision (accuracy): {result['precision_accuracy']:.4f}\n")
                f.write(f"  Recall (accuracy): {result['recall_accuracy']:.4f}\n")
                f.write(f"  F1-score (accuracy): {result['f1_accuracy']:.4f}\n")
            
            # Confusion Matrix cho Agreement
            cm_agreement = result.get('confusion_matrix_agreement', result.get('confusion_matrix'))
            if cm_agreement:
                f.write(f"  Confusion Matrix (Agreement - Target vs Surrogate):\n")
                f.write(f"    TN: {cm_agreement['tn']}, FP: {cm_agreement['fp']}\n")
                f.write(f"    FN: {cm_agreement['fn']}, TP: {cm_agreement['tp']}\n")
            
            # Confusion Matrix cho Accuracy (n·∫øu c√≥)
            if result.get('confusion_matrix_accuracy'):
                cm_accuracy = result['confusion_matrix_accuracy']
                f.write(f"  Confusion Matrix (Accuracy - Ground Truth vs Surrogate):\n")
                f.write(f"    TN: {cm_accuracy['tn']}, FP: {cm_accuracy['fp']}\n")
                f.write(f"    FN: {cm_accuracy['fn']}, TP: {cm_accuracy['tp']}\n")
            
            # Ph√¢n b·ªë
            if result.get('ground_truth_distribution'):
                f.write(f"  Ground truth distribution: {result['ground_truth_distribution']}\n")
            f.write(f"  Target distribution: {result['target_distribution']}\n")
            f.write(f"  Surrogate distribution: {result['surrogate_distribution']}\n")
            f.write("\n")
        
        # T√≥m t·∫Øt k·∫øt qu·∫£
        f.write("\n" + "=" * 80 + "\n")
        f.write("T√ìM T·∫ÆT K·∫æT QU·∫¢:\n")
        f.write("=" * 80 + "\n\n")
        
        if all_results:
            result = all_results[0]
            f.write(f"Surrogate model ƒë·∫°t:\n")
            if result['accuracy'] is not None:
                f.write(f"  - Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%) - so v·ªõi ground truth\n")
            else:
                f.write(f"  - Accuracy: Kh√¥ng c√≥ ground truth labels\n")
            f.write(f"  - Agreement: {result['agreement']:.4f} ({result['agreement']*100:.2f}%) - so v·ªõi target model\n")
            if result['auc'] is not None:
                f.write(f"  - AUC (vs target): {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"  - AUC (vs ground truth): {result['auc_accuracy']:.4f}\n")
            f.write(f"  - F1-score (agreement): {result.get('f1_agreement', result.get('f1_score', 0)):.4f}\n")
            if result.get('f1_accuracy') is not None:
                f.write(f"  - F1-score (accuracy): {result['f1_accuracy']:.4f}\n")
    
    # Markdown report
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o ƒê√°nh Gi√° ƒê·ªô T∆∞∆°ng ƒê·ªìng Gi·ªØa Target v√† Surrogate Models\n\n")
        
        f.write("## Th√¥ng Tin Models\n\n")
        f.write(f"- **Target Model**: `{target_model_path}`\n")
        f.write(f"- **Surrogate Model**: `{surrogate_model_path}`\n")
        if scaler_exists and scaler_path is not None:
            f.write(f"- **Scaler**: `{scaler_path}`\n")
        else:
            f.write(f"- **Scaler**: Kh√¥ng t√¨m th·∫•y (ƒë√£ t·∫°o v√† fit m·ªõi v·ªõi d·ªØ li·ªáu test)\n")
        f.write("\n")
        
        f.write("## Th√¥ng Tin D·ªØ Li·ªáu Test\n\n")
        f.write(f"- **File**: `{test_parquet}`\n")
        f.write(f"- **S·ªë samples**: {len(X_test):,}\n")
        if y_true is not None:
            unique_true, counts_true = np.unique(y_true, return_counts=True)
            f.write(f"- **Ph√¢n b·ªë ground truth labels**: {dict(zip(unique_true, counts_true))}\n")
        unique_target, counts_target = np.unique(y_target, return_counts=True)
        f.write(f"- **Ph√¢n b·ªë nh√£n t·ª´ target model**: {dict(zip(unique_target, counts_target))}\n\n")
        
        f.write("## K·∫øt Qu·∫£ ƒê√°nh Gi√°\n\n")
        f.write("### Metric Definitions\n\n")
        f.write("- **Accuracy**: ƒê·ªô ch√≠nh x√°c c·ªßa surrogate model so v·ªõi ground truth labels\n")
        f.write("- **Agreement**: ƒê·ªô nh·∫•t qu√°n gi·ªØa surrogate v√† target model predictions\n\n")
        
        f.write("| Model | Accuracy | Agreement | AUC (vs target) | AUC (vs GT) | F1 (agreement) | F1 (accuracy) |\n")
        f.write("|-------|----------|-----------|-----------------|-------------|----------------|---------------|\n")
        
        for result in all_results:
            accuracy_str = f"{result['accuracy']:.4f}" if result['accuracy'] is not None else "N/A"
            auc_target_str = f"{result['auc']:.4f}" if result['auc'] is not None else "N/A"
            auc_gt_str = f"{result.get('auc_accuracy', 'N/A'):.4f}" if result.get('auc_accuracy') is not None else "N/A"
            f1_agreement = result.get('f1_agreement', result.get('f1_score', 0))
            f1_accuracy_str = f"{result['f1_accuracy']:.4f}" if result.get('f1_accuracy') is not None else "N/A"
            f.write(f"| {result['model_name']} | {accuracy_str} | "
                   f"{result['agreement']:.4f} | {auc_target_str} | {auc_gt_str} | "
                   f"{f1_agreement:.4f} | {f1_accuracy_str} |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng Model\n\n")
        
        for result in all_results:
            f.write(f"### {result['model_name'].replace('_', ' ').title()}\n\n")
            
            # Accuracy
            if result['accuracy'] is not None:
                f.write(f"- **Accuracy**: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%) - so v·ªõi ground truth\n")
            else:
                f.write(f"- **Accuracy**: Kh√¥ng c√≥ ground truth labels ƒë·ªÉ t√≠nh\n")
            
            # Agreement
            f.write(f"- **Agreement**: {result['agreement']:.4f} ({result['agreement']*100:.2f}%) - so v·ªõi target model\n")
            
            # AUC
            if result['auc'] is not None:
                f.write(f"- **AUC (vs target)**: {result['auc']:.4f}\n")
            if result.get('auc_accuracy') is not None:
                f.write(f"- **AUC (vs ground truth)**: {result['auc_accuracy']:.4f}\n")
            
            # Precision, Recall, F1 cho Agreement
            f.write(f"- **Precision (agreement)**: {result.get('precision_agreement', result.get('precision', 0)):.4f}\n")
            f.write(f"- **Recall (agreement)**: {result.get('recall_agreement', result.get('recall', 0)):.4f}\n")
            f.write(f"- **F1-score (agreement)**: {result.get('f1_agreement', result.get('f1_score', 0)):.4f}\n")
            
            # Precision, Recall, F1 cho Accuracy (n·∫øu c√≥)
            if result.get('precision_accuracy') is not None:
                f.write(f"- **Precision (accuracy)**: {result['precision_accuracy']:.4f}\n")
                f.write(f"- **Recall (accuracy)**: {result['recall_accuracy']:.4f}\n")
                f.write(f"- **F1-score (accuracy)**: {result['f1_accuracy']:.4f}\n")
            f.write("\n")
            
            # Confusion Matrix cho Agreement
            cm_agreement = result.get('confusion_matrix_agreement', result.get('confusion_matrix'))
            if cm_agreement:
                f.write("**Confusion Matrix (Agreement - Target vs Surrogate):**\n\n")
                f.write(f"| | Predicted 0 | Predicted 1 |\n")
                f.write(f"|------|------------|-------------|\n")
                f.write(f"| Target 0 | {cm_agreement['tn']} | {cm_agreement['fp']} |\n")
                f.write(f"| Target 1 | {cm_agreement['fn']} | {cm_agreement['tp']} |\n\n")
            
            # Confusion Matrix cho Accuracy (n·∫øu c√≥)
            if result.get('confusion_matrix_accuracy'):
                cm_accuracy = result['confusion_matrix_accuracy']
                f.write("**Confusion Matrix (Accuracy - Ground Truth vs Surrogate):**\n\n")
                f.write(f"| | Predicted 0 | Predicted 1 |\n")
                f.write(f"|------|------------|-------------|\n")
                f.write(f"| Actual 0 | {cm_accuracy['tn']} | {cm_accuracy['fp']} |\n")
                f.write(f"| Actual 1 | {cm_accuracy['fn']} | {cm_accuracy['tp']} |\n\n")
            
            # Ph√¢n b·ªë
            if result.get('ground_truth_distribution'):
                f.write(f"- **Ground truth distribution**: {result['ground_truth_distribution']}\n")
            f.write(f"- **Target distribution**: {result['target_distribution']}\n")
            f.write(f"- **Surrogate distribution**: {result['surrogate_distribution']}\n\n")
    
    # JSON results
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "test_info": {
                "file": test_parquet,
                "num_samples": int(len(X_test)),
                "target_distribution": {int(k): int(v) for k, v in dict(zip(*np.unique(y_target, return_counts=True))).items()}
            },
            "results": all_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON results: {json_path}")
    
    # In t√≥m t·∫Øt
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    if all_results:
        df = pd.DataFrame(all_results)
        # Ch·ªçn c√°c c·ªôt c√≥ s·∫µn
        cols_to_show = ["model_name", "accuracy", "agreement", "auc"]
        if "f1_agreement" in df.columns:
            cols_to_show.append("f1_agreement")
        elif "f1_score" in df.columns:
            cols_to_show.append("f1_score")
        if "f1_accuracy" in df.columns and df["f1_accuracy"].notna().any():
            cols_to_show.append("f1_accuracy")
        print(df[cols_to_show].to_string(index=False))
    
    return all_results


if __name__ == "__main__":
    main()

