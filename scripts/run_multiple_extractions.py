"""
Script ƒë·ªÉ ch·∫°y extraction v·ªõi c√°c s·ªë l∆∞·ª£ng queries kh√°c nhau v√† t·∫°o report

H·ªó tr·ª£ c·∫£ target model .h5 (Keras) v√† .lgb (LightGBM)
"""
import json
import sys
import argparse
from pathlib import Path
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from scripts.extract_final_model import run_extraction


def main():
    parser = argparse.ArgumentParser(description="Ch·∫°y model extraction v·ªõi nhi·ªÅu c·∫•u h√¨nh")
    parser.add_argument("--model_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file model (.h5 ho·∫∑c .lgb). M·∫∑c ƒë·ªãnh: t·ª± ƒë·ªông t√¨m")
    parser.add_argument("--model_type", type=str, choices=["h5", "lgb"], default=None,
                       help="Lo·∫°i model: 'h5' (Keras) ho·∫∑c 'lgb' (LightGBM). M·∫∑c ƒë·ªãnh: t·ª± ƒë·ªông ph√°t hi·ªán t·ª´ extension")
    parser.add_argument("--normalization_stats_path", type=str, default=None,
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file normalization_stats.npz. M·∫∑c ƒë·ªãnh: t·ª± ƒë·ªông t√¨m")
    parser.add_argument("--attacker_type", type=str, choices=["keras", "lgb", "dual"], default=None,
                       help="Lo·∫°i surrogate model: 'keras' (DNN), 'lgb' (LightGBM), ho·∫∑c 'dual' (dualDNN). M·∫∑c ƒë·ªãnh: t·ª± ƒë·ªông theo model_type")
    parser.add_argument("--auto_create_stats", action="store_true", default=False,
                       help="T·ª± ƒë·ªông t·∫°o file normalization stats n·∫øu kh√¥ng t√¨m th·∫•y (ch·ªâ cho model .lgb)")
    args = parser.parse_args()
    
    # T·ª± ƒë·ªông t√¨m model file n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if args.model_path is None:
        # Th·ª≠ t√¨m c√°c file model ph·ªï bi·∫øn
        possible_models = [
            PROJECT_ROOT / "src" / "final_model.h5",
            PROJECT_ROOT / "src" / "final_model_LEE.lgb",
            PROJECT_ROOT / "src" / "final_model_LSE.lgb",
            PROJECT_ROOT / "src" / "best_model.lgb",
            PROJECT_ROOT / "src" / "final_model.lgb",
        ]
        
        weights_path = None
        for model_path in possible_models:
            if model_path.exists():
                weights_path = str(model_path.resolve())  # Convert to absolute path
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y model: {weights_path}")
                break
        
        if weights_path is None:
            raise FileNotFoundError(
                f"Kh√¥ng t√¨m th·∫•y file model n√†o. Vui l√≤ng ch·ªâ ƒë·ªãnh b·∫±ng --model_path. "
                f"ƒê√£ t√¨m t·∫°i: {[str(p) for p in possible_models]}"
            )
    else:
        # Convert user-provided path to absolute path
        weights_path_obj = Path(args.model_path)
        if not weights_path_obj.is_absolute():
            weights_path = str((PROJECT_ROOT / args.model_path).resolve())
        else:
            weights_path = str(weights_path_obj.resolve())
        
        # Validate model file exists
        if not Path(weights_path).exists():
            raise FileNotFoundError(
                f"‚ùå Model file kh√¥ng t·ªìn t·∫°i: {weights_path}\n"
                f"   ƒê√£ th·ª≠ resolve t·ª´: {args.model_path}"
            )
    
    # T·ª± ƒë·ªông ph√°t hi·ªán model_type t·ª´ extension n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if args.model_type is None:
        model_path_obj = Path(weights_path)
        if model_path_obj.suffix.lower() in ['.lgb', '.txt', '.pkl', '.d5']:
            args.model_type = "lgb"
            print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: LGB (t·ª´ extension {model_path_obj.suffix})")
        elif model_path_obj.suffix.lower() in ['.h5', '.hdf5']:
            args.model_type = "h5"
            print(f"‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán model type: H5 (t·ª´ extension {model_path_obj.suffix})")
        else:
            # M·∫∑c ƒë·ªãnh l√† h5
            args.model_type = "h5"
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ph√°t hi·ªán model type t·ª´ extension, m·∫∑c ƒë·ªãnh: H5")
    
    # QUAN TR·ªåNG: ƒê·∫£m b·∫£o weights_path l√† absolute path v√† validate
    weights_path_abs = str(Path(weights_path).resolve())
    weights_path = weights_path_abs  # Update ƒë·ªÉ d√πng cho ph·∫ßn c√≤n l·∫°i
    
    # Validate model file exists
    if not Path(weights_path).exists():
        raise FileNotFoundError(f"‚ùå Model file kh√¥ng t·ªìn t·∫°i: {weights_path}")
    
    # Get model info for verification
    model_path_obj = Path(weights_path)
    model_name = model_path_obj.name
    model_size = model_path_obj.stat().st_size / (1024 * 1024)  # MB
    
    print(f"\n‚úÖ ƒê√£ x√°c nh·∫≠n target model:")
    print(f"   Path (absolute): {weights_path}")
    print(f"   File name: {model_name}")
    print(f"   File size: {model_size:.2f} MB")
    
    # Ki·ªÉm tra normalization stats cho LightGBM
    if args.model_type == "lgb" and args.normalization_stats_path is None:
        # T·ª± ƒë·ªông t√¨m file normalization stats d·ª±a tr√™n t√™n model
        model_name_without_ext = model_path_obj.stem  # L·∫•y t√™n file kh√¥ng c√≥ extension
        
        # Th·ª≠ c√°c pattern ph·ªï bi·∫øn:
        # 1. final_model_LEE.npz (c√πng t√™n v·ªõi model)
        # 2. final_model_LEE_normalization_stats.npz
        # 3. normalization_stats.npz (m·∫∑c ƒë·ªãnh)
        possible_stats_paths = [
            model_path_obj.parent / f"{model_name_without_ext}.npz",
            model_path_obj.parent / f"{model_name_without_ext}_normalization_stats.npz",
            model_path_obj.parent / "normalization_stats.npz",
            PROJECT_ROOT / "src" / "normalization_stats.npz",
        ]
        
        normalization_stats_path = None
        for stats_path in possible_stats_paths:
            if stats_path.exists():
                normalization_stats_path = str(stats_path.resolve())  # Absolute path
                print(f"‚úÖ T·ª± ƒë·ªông t√¨m th·∫•y normalization stats: {normalization_stats_path}")
                print(f"   Stats file: {Path(normalization_stats_path).name}")
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y v√† cho ph√©p auto-create
        if normalization_stats_path is None and args.auto_create_stats:
            print(f"\n‚ö†Ô∏è  KH√îNG T√åM TH·∫§Y file normalization stats!")
            print(f"   üîÑ ƒêang t·ª± ƒë·ªông t·∫°o file normalization stats...")
            try:
                # Import function ƒë·ªÉ t·∫°o stats
                from scripts.create_normalization_stats import (
                    get_feature_columns,
                    compute_normalization_stats,
                )

                # T√¨m training parquet file
                train_parquet = PROJECT_ROOT / "data" / "train_ember_2018_v2_features_label_other.parquet"
                if not train_parquet.exists():
                    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y training data: {train_parquet}")

                # T·∫°o file stats v·ªõi t√™n t∆∞∆°ng ·ª©ng v·ªõi model
                output_stats_path = model_path_obj.parent / f"{model_name_without_ext}_normalization_stats.npz"
                label_col = "Label"

                print(f"   üìä ƒêang ƒë·ªçc features t·ª´ {train_parquet}...")
                feature_cols = get_feature_columns(str(train_parquet), label_col)

                print(f"   üìä ƒêang t√≠nh normalization stats...")
                feature_means, feature_stds = compute_normalization_stats(
                    str(train_parquet), feature_cols, label_col, sample_size=50000, batch_size=2048
                )

                print(f"   üíæ ƒêang l∆∞u v√†o {output_stats_path}...")
                import numpy as np

                np.savez(
                    str(output_stats_path),
                    feature_means=feature_means,
                    feature_stds=feature_stds,
                    feature_cols=np.array(feature_cols, dtype=object),
                )

                normalization_stats_path = str(output_stats_path.resolve())  # Absolute path
                print(f"   ‚úÖ ƒê√£ t·∫°o file normalization stats: {normalization_stats_path}")
                print(f"   Stats file: {Path(normalization_stats_path).name} (cho model {model_name})")

            except Exception as e:
                print(f"   ‚ùå L·ªói khi t·∫°o normalization stats: {e}")
                import traceback

                traceback.print_exc()
                print(f"\n   üí° Vui l√≤ng t·∫°o th·ªß c√¥ng b·∫±ng:")
                print(f"      python scripts/create_normalization_stats.py \\")
                print(f"          --output_path {model_path_obj.parent / f'{model_name_without_ext}_normalization_stats.npz'}")
                print(f"   ho·∫∑c ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë√£ c√≥ s·∫µn qua --normalization_stats_path")
                raise
    else:
        # User provided normalization_stats_path - convert to absolute
        if args.normalization_stats_path is not None:
            stats_path_obj = Path(args.normalization_stats_path)
            if not stats_path_obj.is_absolute():
                normalization_stats_path = str((PROJECT_ROOT / args.normalization_stats_path).resolve())
            else:
                normalization_stats_path = str(stats_path_obj.resolve())
            
            # Validate stats file exists
            if not Path(normalization_stats_path).exists():
                raise FileNotFoundError(
                    f"‚ùå Normalization stats file kh√¥ng t·ªìn t·∫°i: {normalization_stats_path}\n"
                    f"   ƒê√£ th·ª≠ resolve t·ª´: {args.normalization_stats_path}"
                )
        else:
            normalization_stats_path = None
    
    base_output_dir = PROJECT_ROOT / "output"
    
    # ƒê∆∞·ªùng d·∫´n data files
    train_parquet = str(PROJECT_ROOT / "data" / "train_ember_2018_v2_features_label_other.parquet")
    test_parquet = str(PROJECT_ROOT / "data" / "test_ember_2018_v2_features_label_other.parquet")
    
    # T·∫°o t√™n output directory d·ª±a tr√™n model type
    model_suffix = args.model_type.upper()
    
    # C√°c c·∫•u h√¨nh kh√°c nhau
    # L∆∞u √Ω: total_queries = query_batch √ó num_rounds (ch·ªâ t√≠nh s·ªë queries trong active learning rounds)
    # Labels s·ª≠ d·ª•ng = seed_size + val_size + total_queries
    configurations = [
        {
            "name": f"max_queries_10000_{model_suffix}",
            "query_batch": 2000,
            "num_rounds": 5,
            "total_queries": 10000,  # 2000 √ó 5 = 10000
            "description": "T·ªïng 10,000 queries (2000 queries/round √ó 5 rounds)"
        },
        {
            "name": f"max_queries_5000_{model_suffix}",
            "query_batch": 1250,
            "num_rounds": 4,
            "total_queries": 5000,  # 1250 √ó 4 = 5000
            "description": "T·ªïng 5,000 queries (1250 queries/round √ó 4 rounds)"
        },
        {
            "name": f"max_queries_2000_{model_suffix}",
            "query_batch": 2000,
            "num_rounds": 1,
            "total_queries": 2000,  # 2000 √ó 1 = 2000
            "description": "T·ªïng 2,000 queries (2000 queries/round √ó 1 round)"
        }
    ]
    
    results = []
    
    print("=" * 80)
    print("B·∫ÆT ƒê·∫¶U CH·∫†Y EXTRACTION V·ªöI C√ÅC C·∫§U H√åNH KH√ÅC NHAU")
    print("=" * 80)
    print(f"\nüìã C·∫•u h√¨nh chung cho T·∫§T C·∫¢ configs:")
    print(f"   ‚úÖ Target model: {Path(weights_path).name}")
    print(f"      Path (absolute): {weights_path}")
    print(f"      Model type: {args.model_type.upper()}")
    if normalization_stats_path:
        print(f"   ‚úÖ Normalization stats: {Path(normalization_stats_path).name}")
        print(f"      Path (absolute): {normalization_stats_path}")
    else:
        print(f"   ‚ÑπÔ∏è  Normalization stats: Kh√¥ng s·ª≠ d·ª•ng (Keras model)")
    if args.attacker_type:
        print(f"   Attacker type: {args.attacker_type.upper()}")
    else:
        print(f"   Attacker type: T·ª± ƒë·ªông ({args.model_type.upper()})")
    print("=" * 80)
    print(f"\n‚ö†Ô∏è  L∆ØU √ù: T·∫•t c·∫£ c√°c configs s·∫Ω t·∫•n c√¥ng C√ôNG M·ªòT target model: {Path(weights_path).name}")
    print("=" * 80)
    
    for config in configurations:
        print(f"\n{'='*80}")
        print(f"üî¨ C·∫§U H√åNH: {config['name']}")
        print(f"   {config['description']}")
        print(f"{'='*80}\n")
        
        output_dir = base_output_dir / config["name"]
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # QUAN TR·ªåNG: Verify l·∫°i model path cho m·ªói config ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã nh·∫ßm l·∫´n
        if not Path(weights_path).exists():
            raise FileNotFoundError(
                f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Target model kh√¥ng t·ªìn t·∫°i khi ch·∫°y config {config['name']}!\n"
                f"   Model path: {weights_path}\n"
                f"   C√≥ th·ªÉ model ƒë√£ b·ªã x√≥a ho·∫∑c di chuy·ªÉn trong qu√° tr√¨nh ch·∫°y."
            )
        
        print(f"\nüîç X√°c nh·∫≠n target model cho config {config['name']}:")
        print(f"   ‚úÖ Model file: {Path(weights_path).name}")
        print(f"   ‚úÖ Path: {weights_path}")
        if normalization_stats_path:
            if not Path(normalization_stats_path).exists():
                raise FileNotFoundError(
                    f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Normalization stats kh√¥ng t·ªìn t·∫°i!\n"
                    f"   Stats path: {normalization_stats_path}"
                )
            print(f"   ‚úÖ Normalization stats: {Path(normalization_stats_path).name}")
        
        try:
            summary = run_extraction(
                weights_path=weights_path,  # ƒê·∫£m b·∫£o l√† absolute path
                output_dir=output_dir,
                train_parquet=train_parquet,
                test_parquet=test_parquet,
                seed=42,
                seed_size=2000,
                val_size=1000,
                eval_size=4000,
                query_batch=config["query_batch"],
                num_rounds=config["num_rounds"],
                num_epochs=100,  # Theo nghi√™n c·ª©u: 100 epochs v·ªõi early_stopping=30 (ch·ªâ d√πng cho Keras)
                model_type=args.model_type,
                normalization_stats_path=normalization_stats_path,  # ƒê·∫£m b·∫£o l√† absolute path
                attacker_type=args.attacker_type,
            )
            
            # QUAN TR·ªåNG: Verify model trong summary kh·ªõp v·ªõi model ƒë√£ ch·ªâ ƒë·ªãnh
            # ƒê·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã nh·∫ßm l·∫´n target model
            if "weights_path" in summary:
                summary_model_path = summary["weights_path"]
                summary_model_name = summary.get("model_file_name", Path(summary_model_path).name)
                expected_model_name = Path(weights_path).name
                
                # Verify b·∫±ng absolute path
                if Path(summary_model_path).resolve() != Path(weights_path).resolve():
                    print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Summary model path ({summary_model_path}) != Model path ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh ({weights_path})")
                    print(f"   Tuy nhi√™n s·∫Ω ti·∫øp t·ª•c v√¨ c√≥ th·ªÉ do resolve path.")
                
                # Verify b·∫±ng t√™n file ƒë·ªÉ ch·∫Øc ch·∫Øn kh√¥ng b·ªã nh·∫ßm model
                if summary_model_name != expected_model_name:
                    print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG: Model file name kh√¥ng kh·ªõp!")
                    print(f"   Summary model: {summary_model_name}")
                    print(f"   Expected model: {expected_model_name}")
                    print(f"   C√≥ th·ªÉ ƒë√£ b·ªã nh·∫ßm l·∫´n model!")
                    raise ValueError(
                        f"Model file name kh√¥ng kh·ªõp: summary c√≥ {summary_model_name} "
                        f"nh∆∞ng expected l√† {expected_model_name}. "
                        f"C√≥ th·ªÉ ƒë√£ t·∫•n c√¥ng sai target model!"
                    )
                
                print(f"   ‚úÖ Verified: Model trong summary kh·ªõp ({summary_model_name})")
            
            # L·∫•y metrics cu·ªëi c√πng
            final_metrics = summary["metrics"][-1] if summary["metrics"] else {}
            
            # L·∫•y s·ªë queries th·ª±c t·∫ø t·ª´ metrics (kh√¥ng t√≠nh seed v√† val)
            actual_queries_used = final_metrics.get("queries_used", config["total_queries"])
            
            result = {
                "config_name": config["name"],
                "description": config["description"],
                "total_queries": config["total_queries"],  # S·ªë queries d·ª± ki·∫øn
                "actual_queries_used": actual_queries_used,  # S·ªë queries th·ª±c t·∫ø
                "query_batch": config["query_batch"],
                "num_rounds": config["num_rounds"],
                "total_labels_used": final_metrics.get("labels_used", 0),
                "optimal_threshold": final_metrics.get("optimal_threshold", 0.5),
                "final_accuracy": final_metrics.get("surrogate_acc", 0.0),
                "final_balanced_accuracy": final_metrics.get("surrogate_balanced_acc", 0.0),  # Quan tr·ªçng v·ªõi class imbalance
                "final_auc": final_metrics.get("surrogate_auc", float("nan")),
                "final_precision": final_metrics.get("surrogate_precision", 0.0),
                "final_recall": final_metrics.get("surrogate_recall", 0.0),
                "final_f1": final_metrics.get("surrogate_f1", 0.0),
                "final_agreement": final_metrics.get("agreement_with_target", 0.0),
                "output_dir": str(output_dir),
                "metrics_csv": summary.get("metrics_csv", ""),
                "surrogate_model_path": summary.get("surrogate_model_path", ""),
            }
            
            results.append(result)
            
            print(f"\n‚úÖ Ho√†n th√†nh {config['name']}")
            print(f"   Accuracy: {result['final_accuracy']:.4f}")
            print(f"   Balanced Accuracy: {result['final_balanced_accuracy']:.4f} (quan tr·ªçng v·ªõi class imbalance)")
            print(f"   F1-score: {result['final_f1']:.4f}")
            print(f"   Agreement: {result['final_agreement']:.4f}")
            print(f"   Optimal Threshold: {result['optimal_threshold']:.4f}")
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi ch·∫°y {config['name']}: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "config_name": config["name"],
                "description": config["description"],
                "error": str(e)
            })
    
    # T·∫°o report
    print(f"\n{'='*80}")
    print("üìä T·∫†O REPORT")
    print(f"{'='*80}\n")
    
    report_path = base_output_dir / "extraction_comparison_report.txt"
    report_md_path = base_output_dir / "extraction_comparison_report.md"
    
    # T·∫°o report text
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO SO S√ÅNH C√ÅC SURROGATE MODELS\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("T√ìM T·∫ÆT:\n")
        f.write("-" * 80 + "\n")
        f.write(f"ƒê√£ ch·∫°y extraction v·ªõi {len(configurations)} c·∫•u h√¨nh kh√°c nhau:\n\n")
        
        for result in results:
            if "error" in result:
                f.write(f"‚ùå {result['config_name']}: L·ªñI - {result['error']}\n")
            else:
                f.write(f"‚úÖ {result['config_name']}:\n")
                f.write(f"   - Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"   - Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                f.write(f"   - Labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n")
                f.write(f"   - Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"   - Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"   - F1-score: {result['final_f1']:.4f}\n")
                f.write(f"   - Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                f.write(f"   - Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"   - AUC: {result['final_auc']:.4f}\n")
                f.write(f"   - Precision: {result['final_precision']:.4f}\n")
                f.write(f"   - Recall: {result['final_recall']:.4f}\n")
                f.write(f"   - Output: {result['output_dir']}\n")
                f.write("\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("CHI TI·∫æT T·ª™NG C·∫§U H√åNH:\n")
        f.write("=" * 80 + "\n\n")
        
        for result in results:
            if "error" not in result:
                f.write(f"\n{result['config_name'].upper().replace('_', ' ')}:\n")
                f.write("-" * 80 + "\n")
                f.write(f"M√¥ t·∫£: {result['description']}\n")
                f.write(f"Query batch: {result['query_batch']:,}\n")
                f.write(f"S·ªë rounds: {result['num_rounds']}\n")
                f.write(f"Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                f.write(f"T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n\n")
                
                f.write("Metrics cu·ªëi c√πng:\n")
                f.write(f"  - Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"  - Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"  - F1-score: {result['final_f1']:.4f}\n")
                f.write(f"  - Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                f.write(f"  - Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"  - AUC: {result['final_auc']:.4f}\n")
                f.write(f"  - Precision: {result['final_precision']:.4f}\n")
                f.write(f"  - Recall: {result['final_recall']:.4f}\n")
                f.write(f"\nFiles:\n")
                f.write(f"  - Metrics CSV: {result['metrics_csv']}\n")
                f.write(f"  - Surrogate model: {result['surrogate_model_path']}\n")
                f.write(f"  - Output directory: {result['output_dir']}\n")
    
    # T·∫°o report Markdown
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o So S√°nh C√°c Surrogate Models\n\n")
        f.write("## T√≥m T·∫Øt\n\n")
        f.write(f"ƒê√£ ch·∫°y extraction v·ªõi {len(configurations)} c·∫•u h√¨nh kh√°c nhau v·ªÅ s·ªë l∆∞·ª£ng queries.\n\n")
        
        f.write("## B·∫£ng So S√°nh\n\n")
        f.write("| C·∫•u h√¨nh | Queries | Labels | Accuracy | Balanced Acc | F1 | Agreement | Threshold | AUC |\n")
        f.write("|----------|---------|--------|----------|--------------|----|-----------|-----------|-----|\n")
        
        for result in results:
            if "error" not in result:
                auc_str = f"{result['final_auc']:.4f}" if not pd.isna(result['final_auc']) else "N/A"
                actual_queries = result.get('actual_queries_used', result['total_queries'])
                balanced_acc = result.get('final_balanced_accuracy', 0.0)
                threshold = result.get('optimal_threshold', 0.5)
                f.write(f"| {result['config_name']} | {actual_queries:,} | {result['total_labels_used']:,} | "
                       f"{result['final_accuracy']:.4f} | {balanced_acc:.4f} | {result['final_f1']:.4f} | "
                       f"{result['final_agreement']:.4f} | {threshold:.3f} | {auc_str} |\n")
            else:
                f.write(f"| {result['config_name']} | ERROR | - | - | - | - | - | - | - |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng C·∫•u H√¨nh\n\n")
        
        for result in results:
            if "error" not in result:
                f.write(f"### {result['config_name'].replace('_', ' ').title()}\n\n")
                f.write(f"**M√¥ t·∫£:** {result['description']}\n\n")
                f.write(f"- Query batch: {result['query_batch']:,}\n")
                f.write(f"- S·ªë rounds: {result['num_rounds']}\n")
                f.write(f"- Queries d·ª± ki·∫øn: {result['total_queries']:,}\n")
                f.write(f"- Queries th·ª±c t·∫ø: {result.get('actual_queries_used', result['total_queries']):,}\n")
                f.write(f"- T·ªïng labels s·ª≠ d·ª•ng (bao g·ªìm seed+val): {result['total_labels_used']:,}\n\n")
                
                f.write("**Metrics cu·ªëi c√πng:**\n\n")
                f.write(f"- Accuracy: {result['final_accuracy']:.4f} ({result['final_accuracy']*100:.2f}%)\n")
                f.write(f"- Balanced Accuracy: {result.get('final_balanced_accuracy', 0.0):.4f} ({result.get('final_balanced_accuracy', 0.0)*100:.2f}%) [quan tr·ªçng v·ªõi class imbalance]\n")
                f.write(f"- F1-score: {result['final_f1']:.4f}\n")
                f.write(f"- Optimal Threshold: {result.get('optimal_threshold', 0.5):.4f}\n")
                f.write(f"- Agreement: {result['final_agreement']:.4f} ({result['final_agreement']*100:.2f}%)\n")
                if not pd.isna(result['final_auc']):
                    f.write(f"- AUC: {result['final_auc']:.4f}\n")
                f.write(f"- Precision: {result['final_precision']:.4f}\n")
                f.write(f"- Recall: {result['final_recall']:.4f}\n\n")
                
                f.write("**Files:**\n\n")
                f.write(f"- Metrics CSV: `{result['metrics_csv']}`\n")
                f.write(f"- Surrogate model: `{result['surrogate_model_path']}`\n")
                f.write(f"- Output directory: `{result['output_dir']}`\n\n")
    
    # L∆∞u JSON summary
    json_path = base_output_dir / "extraction_comparison_summary.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON summary: {json_path}")
    
    # In t√≥m t·∫Øt ra console
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    df_results = pd.DataFrame([r for r in results if "error" not in r])
    if not df_results.empty:
        df_display = df_results.copy()
        if 'actual_queries_used' not in df_display.columns:
            df_display['actual_queries_used'] = df_display['total_queries']
        print(df_display[["config_name", "actual_queries_used", "total_labels_used", 
                          "final_accuracy", "final_balanced_accuracy", "final_f1", 
                          "final_agreement", "optimal_threshold"]].to_string(index=False))
    
    return results


if __name__ == "__main__":
    main()

