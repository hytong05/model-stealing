import json
import os
import sys
from pathlib import Path
import gc

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, balanced_accuracy_score
from sklearn.preprocessing import RobustScaler

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.attackers import KerasAttacker, LGBAttacker, KerasDualAttacker
from src.targets.flexible_target import FlexibleKerasTarget, FlexibleLGBTarget
from src.sampling import entropy_sampling
from sklearn_extra.cluster import KMedoids


def _clip_scale(scaler: RobustScaler, X: np.ndarray) -> np.ndarray:
    """Scale data vá»›i RobustScaler vÃ  clip vá» [-5, 5] giá»‘ng pipeline gá»‘c."""
    transformed = scaler.transform(X)
    return np.clip(transformed, -5, 5)


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """Láº¥y danh sÃ¡ch feature columns tá»« parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_data_from_parquet(
    parquet_path: str,
    feature_cols: list,
    label_col: str,
    skip_rows: int = 0,
    take_rows: int = None,
    shuffle: bool = False,
    batch_size: int = 10000,
    seed: int = None,
) -> tuple:
    """
    Load dá»¯ liá»‡u tá»« parquet file, loáº¡i bá» label -1 vÃ  tráº£ vá» X, y.
    Giá»‘ng logic trong final_model.ipynb nhÆ°ng khÃ´ng normalize (sáº½ normalize sau).
    
    Args:
        seed: Random seed cho shuffle. Náº¿u None thÃ¬ dÃ¹ng np.random khÃ´ng cÃ³ seed (khÃ´ng reproducible).
    """
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    all_y = []
    rows_seen = 0
    emitted = 0
    removed_total = 0
    batch_count = 0

    try:
        total_batches = (pq_file.metadata.num_rows + batch_size - 1) // batch_size

        for batch in pq_file.iter_batches(batch_size=batch_size, columns=feature_cols + [label_col]):
            batch_count += 1
            batch_len = len(batch)
            batch_start = rows_seen
            rows_seen += batch_len

            if rows_seen <= skip_rows:
                if batch_count % 50 == 0:
                    print(f"  â³ ÄÃ£ xá»­ lÃ½ {batch_count}/{total_batches} batches (Ä‘ang skip)...")
                continue

            batch_df = batch.to_pandas()

            if skip_rows > batch_start:
                start_idx = skip_rows - batch_start
                batch_df = batch_df.iloc[start_idx:]

            if label_col in batch_df.columns:
                label_series = batch_df[label_col]
            else:
                alt_cols = [col for col in batch_df.columns if col.lower() == label_col.lower()]
                if alt_cols:
                    label_series = batch_df[alt_cols[0]]
                else:
                    raise KeyError(
                        f"Label column '{label_col}' khÃ´ng tá»“n táº¡i. CÃ¡c cá»™t: {list(batch_df.columns)[:5]}..."
                    )

            # Loáº¡i bá» label -1 (unlabeled)
            valid_mask = label_series != -1
            if not np.any(valid_mask):
                removed_total += len(label_series)
                del batch_df, label_series
                gc.collect()
                continue

            removed_total += int(np.sum(~valid_mask))
            batch_df = batch_df[valid_mask]
            label_series = label_series[valid_mask]

            if take_rows is not None:
                remaining = take_rows - emitted
                if remaining <= 0:
                    break
                if len(batch_df) > remaining:
                    batch_df = batch_df.iloc[:remaining]
                    label_series = label_series.iloc[:remaining]

            if batch_df.empty:
                del batch_df, label_series
                gc.collect()
                continue

            X = batch_df[feature_cols].values.astype(np.float32)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            y = label_series.values.astype(np.int32)

            all_X.append(X)
            all_y.append(y)
            emitted += len(X)

            if batch_count % 20 == 0:
                del batch_df, label_series
                gc.collect()
                if take_rows is None:
                    print(f"  â³ ÄÃ£ xá»­ lÃ½ {batch_count}/{total_batches} batches, loaded {emitted:,} samples...")
                else:
                    print(
                        f"  â³ ÄÃ£ xá»­ lÃ½ {batch_count}/{total_batches} batches, loaded {emitted:,}/{take_rows:,} samples..."
                    )
            else:
                del batch_df, label_series

            if take_rows is not None and emitted >= take_rows:
                break

        if all_X:
            X_concat = np.concatenate(all_X, axis=0)
            y_concat = np.concatenate(all_y, axis=0)
            del all_X, all_y
            gc.collect()
        else:
            X_concat = np.empty((0, len(feature_cols)), dtype=np.float32)
            y_concat = np.empty((0,), dtype=np.int32)

        if shuffle and len(X_concat) > 0:
            print(f"  ğŸ”„ Äang shuffle {len(X_concat):,} samples...")
            if seed is not None:
                rng = np.random.default_rng(seed)
                indices = rng.permutation(len(X_concat))
            else:
                indices = np.random.permutation(len(X_concat))
            X_concat = X_concat[indices]
            y_concat = y_concat[indices]

        if removed_total > 0:
            print(f"  âš ï¸  ÄÃ£ loáº¡i bá» {removed_total:,} samples cÃ³ label -1 (unlabeled)")

        return X_concat, y_concat
    finally:
        del pq_file
        gc.collect()


def run_extraction(
    weights_path: str,
    output_dir: Path,
    train_parquet: str = None,
    test_parquet: str = None,
    seed: int = 42,
    feature_dim: int = 2381,
    seed_size: int = 2000,
    val_size: int = 2000,
    eval_size: int = 4000,
    query_batch: int = 2000,
    num_rounds: int = 5,
    num_epochs: int = 5,
    model_type: str = "h5",  # "h5" hoáº·c "lgb"
    normalization_stats_path: str = None,  # Cáº§n thiáº¿t náº¿u model_type="lgb"
    attacker_type: str = None,  # "keras", "lgb", hoáº·c "dual" (dualDNN), None Ä‘á»ƒ tá»± Ä‘á»™ng chá»n theo model_type
) -> dict:
    rng = np.random.default_rng(seed)

    # Chá»‰ set TF environment variables náº¿u dÃ¹ng Keras model
    if model_type == "h5" or attacker_type in ["keras", "dual"]:
        os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
        os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")

    label_col = "Label"
    
    # Auto-detect attacker_type náº¿u khÃ´ng Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
    if attacker_type is None:
        attacker_type = "keras" if model_type == "h5" else "lgb"

    # Load dá»¯ liá»‡u tá»« EMBER parquet files
    if train_parquet is None:
        train_parquet = str(PROJECT_ROOT / "src" / "train_ember_2018_v2_features_label_other.parquet")
    if test_parquet is None:
        test_parquet = str(PROJECT_ROOT / "src" / "test_ember_2018_v2_features_label_other.parquet")

    print("=" * 60)
    print("ğŸ“Š Äang load dá»¯ liá»‡u EMBER...")
    print("=" * 60)
    print(f"Train file: {train_parquet}")
    print(f"Test file: {test_parquet}")

    # Láº¥y feature columns vÃ  xÃ¡c Ä‘á»‹nh feature_dim thá»±c táº¿
    feature_cols = get_feature_columns(train_parquet, label_col)
    actual_feature_dim = len(feature_cols)
    print(f"Feature columns: {actual_feature_dim}")
    
    # Cáº­p nháº­t feature_dim náº¿u khÃ¡c vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh
    if actual_feature_dim != feature_dim:
        print(f"âš ï¸  Feature dimension mismatch: dataset has {actual_feature_dim} features, "
              f"but feature_dim parameter is {feature_dim}")
        print(f"   Updating feature_dim to {actual_feature_dim} (tá»« dataset)")
        feature_dim = actual_feature_dim
    
    # QUAN TRá»ŒNG: Validate vÃ  log thÃ´ng tin target model
    weights_path_abs = str(Path(weights_path).resolve())
    if not Path(weights_path_abs).exists():
        raise FileNotFoundError(f"âŒ Target model khÃ´ng tá»“n táº¡i: {weights_path_abs}")
    
    model_file_name = Path(weights_path_abs).name
    model_file_size = Path(weights_path_abs).stat().st_size / (1024 * 1024)  # MB
    
    print(f"\nğŸ”„ Khá»Ÿi táº¡o target model ({model_type.upper()}) vá»›i feature_dim={feature_dim}...")
    print(f"   âœ… Target model file: {model_file_name}")
    print(f"   âœ… Model path (absolute): {weights_path_abs}")
    print(f"   âœ… Model size: {model_file_size:.2f} MB")
    
    if weights_path != weights_path_abs:
        print(f"   âš ï¸  Path Ä‘Æ°á»£c resolve: {weights_path} -> {weights_path_abs}")
    
    if model_type == "lgb":
        # LightGBM model cáº§n normalization stats
        if normalization_stats_path is None:
            raise ValueError(
                "normalization_stats_path pháº£i Ä‘Æ°á»£c cung cáº¥p khi model_type='lgb'. "
                "Vui lÃ²ng cung cáº¥p Ä‘Æ°á»ng dáº«n tá»›i file normalization_stats.npz"
            )
        
        # Validate normalization stats path
        if isinstance(normalization_stats_path, str):
            stats_path_abs = str(Path(normalization_stats_path).resolve())
            if not Path(stats_path_abs).exists():
                raise FileNotFoundError(f"âŒ Normalization stats khÃ´ng tá»“n táº¡i: {stats_path_abs}")
            normalization_stats_path = stats_path_abs
        
        print(f"   âœ… Normalization stats file: {Path(normalization_stats_path).name}")
        print(f"   âœ… Stats path (absolute): {normalization_stats_path}")
        
        oracle = FlexibleLGBTarget(
            model_path=weights_path_abs,  # Sá»­ dá»¥ng absolute path
            normalization_stats_path=normalization_stats_path,  # Sá»­ dá»¥ng absolute path
            threshold=0.5,
            name=f"lgb-target-{model_file_name}",
            feature_dim=feature_dim
        )
    else:
        # Keras/H5 model
        oracle = FlexibleKerasTarget(weights_path_abs, feature_dim=feature_dim, threshold=0.5)
    
    required_feature_dim = oracle.get_required_feature_dim()
    
    if required_feature_dim is None:
        print(f"   âœ… Target model cÃ³ preprocessing layer - sáº½ tá»± Ä‘á»™ng map tá»« {feature_dim} Ä‘áº·c trÆ°ng")
    else:
        print(f"   âœ… Target model yÃªu cáº§u {required_feature_dim} Ä‘áº·c trÆ°ng")
        if feature_dim > required_feature_dim:
            print(f"   âš ï¸  Dataset cÃ³ {feature_dim} Ä‘áº·c trÆ°ng, sáº½ tá»± Ä‘á»™ng cáº¯t bá» {feature_dim - required_feature_dim} Ä‘áº·c trÆ°ng thá»«a")
        elif feature_dim < required_feature_dim:
            print(f"   âŒ Dataset cÃ³ {feature_dim} Ä‘áº·c trÆ°ng, nhÆ°ng target model yÃªu cáº§u {required_feature_dim} Ä‘áº·c trÆ°ng")
            raise ValueError(f"Dataset khÃ´ng Ä‘á»§ Ä‘áº·c trÆ°ng: {feature_dim} < {required_feature_dim}")

    # QUAN TRá»ŒNG: Äáº£m báº£o seed/val sets giá»‘ng nhau giá»¯a cÃ¡c configs
    # Giáº£i phÃ¡p: Load Ä‘á»§ lá»›n (seed_val + pool lá»›n nháº¥t), shuffle vá»›i seed, sau Ä‘Ã³ chia
    # TÃ­nh pool lá»›n nháº¥t cáº§n thiáº¿t trong cÃ¡c configs (Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng thiáº¿u dá»¯ liá»‡u)
    # Vá»›i cáº¥u hÃ¬nh hiá»‡n táº¡i: max_queries_10000 cÃ³ query_batch=2000, num_rounds=5 => pool cáº§n 10000
    # QUAN TRá»ŒNG: ThÃªm buffer 20% Ä‘á»ƒ Ä‘áº£m báº£o KHÃ”NG BAO GIá»œ thiáº¿u queries
    # TÄƒng buffer tá»« 10% lÃªn 20% Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ pool cho class balancing
    max_pool_needed_base = query_batch * num_rounds
    max_pool_needed = int(max_pool_needed_base * 1.2)  # Buffer 20%
    seed_val_size = seed_size + val_size
    total_needed = seed_val_size + max_pool_needed
    
    print(f"\nğŸ”„ Äang load train data ({total_needed:,} samples: {seed_val_size:,} seed+val + {max_pool_needed:,} pool)...")
    X_train_all, _ = load_data_from_parquet(
        train_parquet, feature_cols, label_col, skip_rows=0, take_rows=total_needed, shuffle=True, seed=seed
    )
    print(f"âœ… Train data loaded: {X_train_all.shape}")

    # Chia train data thÃ nh seed, val, pool
    # Seed vÃ  val giá»‘ng nhau cho táº¥t cáº£ configs
    idx_offset = 0
    X_seed = X_train_all[idx_offset : idx_offset + seed_size]
    idx_offset += seed_size

    X_val = X_train_all[idx_offset : idx_offset + val_size]
    idx_offset += val_size

    # QUAN TRá»ŒNG: Pool size pháº£i Ä‘á»§ cho total queries + dÆ° 20% Ä‘á»ƒ Ä‘áº£m báº£o KHÃ”NG BAO GIá»œ thiáº¿u
    # Do class balancing cÃ³ thá»ƒ thÃªm queries, vÃ  cáº§n buffer lá»›n Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ queries
    pool_needed_base = query_batch * num_rounds
    pool_needed = int(pool_needed_base * 1.2)  # DÆ° 20% Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ queries (tÄƒng tá»« 10% lÃªn 20%)
    
    # Kiá»ƒm tra xem cÃ³ Ä‘á»§ data khÃ´ng
    available_pool = X_train_all.shape[0] - idx_offset
    if available_pool < pool_needed:
        # Náº¿u khÃ´ng Ä‘á»§ data cho pool vá»›i buffer, váº«n cá»‘ gáº¯ng láº¥y Ã­t nháº¥t pool_needed_base
        if available_pool < pool_needed_base:
            print(f"   âŒ Lá»–I NGHIÃŠM TRá»ŒNG: KhÃ´ng Ä‘á»§ data cho pool!")
            print(f"   âŒ Available: {available_pool:,}, Required: {pool_needed_base:,}")
            print(f"   âŒ Pool sáº½ cáº¡n kiá»‡t vÃ  queries sáº½ thiáº¿u!")
            raise ValueError(
                f"KhÃ´ng Ä‘á»§ data cho pool! Available: {available_pool:,}, "
                f"Required: {pool_needed_base:,} (query_batch={query_batch:,} Ã— num_rounds={num_rounds})"
            )
        else:
            print(f"   âš ï¸  Cáº¢NH BÃO: KhÃ´ng Ä‘á»§ data cho pool vá»›i buffer ({available_pool:,} < {pool_needed:,})")
            print(f"   ğŸ’¡ Sáº½ dÃ¹ng tá»‘i Ä‘a {available_pool:,} samples cho pool (thiáº¿u buffer)")
            pool_needed = available_pool
    
    X_pool = X_train_all[idx_offset : idx_offset + pool_needed]
    buffer_size = pool_needed - pool_needed_base
    print(f"   âœ… Pool size: {X_pool.shape[0]:,} samples")
    print(f"      - Target: {pool_needed_base:,} (query_batch={query_batch:,} Ã— num_rounds={num_rounds})")
    print(f"      - Buffer: +{buffer_size:,} ({buffer_size/pool_needed_base*100:.1f}%) Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng thiáº¿u queries")
    del X_train_all
    gc.collect()

    # Load eval set tá»« test file
    # QUAN TRá»ŒNG: Load cáº£ ground truth labels Ä‘á»ƒ tÃ­nh accuracy chÃ­nh xÃ¡c
    print(f"\nğŸ”„ Äang load eval set ({eval_size:,} samples)...")
    X_eval, y_eval_gt = load_data_from_parquet(
        test_parquet, feature_cols, label_col, skip_rows=0, take_rows=eval_size, shuffle=True, seed=seed
    )
    print(f"âœ… Eval set: {X_eval.shape}")
    print(f"âœ… Ground truth labels: {y_eval_gt.shape}")

    print(f"\nğŸ“Š Data split:")
    print(f"  Seed: {X_seed.shape[0]:,}")
    print(f"  Val: {X_val.shape[0]:,}")
    print(f"  Pool: {X_pool.shape[0]:,}")
    print(f"  Eval: {X_eval.shape[0]:,}")

    # QUAN TRá»ŒNG: Xá»­ lÃ½ dá»¯ liá»‡u trÆ°á»›c khi query oracle
    # - Vá»›i Keras/H5 model: Cáº§n scale data vá»›i RobustScaler (model Ä‘Æ°á»£c train vá»›i scaled data)
    # - Vá»›i LightGBM model: FlexibleLGBTarget sáº½ tá»± Ä‘á»™ng normalize náº¿u cÃ³ normalization_stats_path
    #   KhÃ´ng cáº§n scale thÃªm vá»›i RobustScaler
    # - Vá»›i dualDNN: CÅ©ng cáº§n scale data giá»‘ng Keras
    scaler = None
    X_eval_s = None
    X_seed_s = None
    X_val_s = None
    X_pool_s = None
    
    if model_type == "h5" or attacker_type in ["keras", "dual"]:
        # Keras model cáº§n scale data
        print(f"\nğŸ”„ Äang scale dá»¯ liá»‡u trÆ°á»›c khi query oracle (Keras model)...")
        scaler = RobustScaler()
        scaler.fit(np.vstack([X_seed, X_val, X_pool]))

        X_eval_s = _clip_scale(scaler, X_eval)
        X_seed_s = _clip_scale(scaler, X_seed)
        X_val_s = _clip_scale(scaler, X_val)
        X_pool_s = _clip_scale(scaler, X_pool)
        
        print(f"âœ… ÄÃ£ scale dá»¯ liá»‡u")
        print(f"   - X_eval_s shape: {X_eval_s.shape}")
        print(f"   - X_seed_s shape: {X_seed_s.shape}")
        print(f"   - X_val_s shape: {X_val_s.shape}")
        print(f"   - X_pool_s shape: {X_pool_s.shape}")
        
        # Láº¥y nhÃ£n tá»« oracle (Vá»šI Dá»® LIá»†U ÄÃƒ SCALE)
        print(f"\nğŸ”„ Äang láº¥y nhÃ£n tá»« oracle (vá»›i dá»¯ liá»‡u Ä‘Ã£ scale)...")
        y_eval = oracle(X_eval_s)
        y_seed = oracle(X_seed_s)
        y_val = oracle(X_val_s)
    else:
        # LightGBM model: FlexibleLGBTarget sáº½ tá»± Ä‘á»™ng normalize
        print(f"\nğŸ”„ Äang láº¥y nhÃ£n tá»« oracle (LightGBM sáº½ tá»± Ä‘á»™ng normalize)...")
        y_eval = oracle(X_eval)
        y_seed = oracle(X_seed)
        y_val = oracle(X_val)
        
        # Vá»›i LightGBM, khÃ´ng cáº§n scale data
        X_eval_s = X_eval
        X_seed_s = X_seed
        X_val_s = X_val
        X_pool_s = X_pool
    print(f"âœ… Oracle labels retrieved")
    eval_dist = dict(zip(*np.unique(y_eval, return_counts=True)))
    seed_dist = dict(zip(*np.unique(y_seed, return_counts=True)))
    val_dist = dict(zip(*np.unique(y_val, return_counts=True)))
    print(f"  Eval distribution: {eval_dist}")
    print(f"  Seed distribution: {seed_dist}")
    print(f"  Val distribution: {val_dist}")
    
    # QUAN TRá»ŒNG: ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a oracle vá»›i ground truth
    # Äiá»u nÃ y giÃºp giáº£i thÃ­ch sá»± khÃ¡c biá»‡t giá»¯a val_accuracy (vs oracle) vÃ  final accuracy (vs ground truth)
    oracle_acc_vs_gt = accuracy_score(y_eval_gt, y_eval)
    print(f"\nğŸ“Š ÄÃ¡nh giÃ¡ Oracle (Target Model):")
    print(f"   Oracle accuracy vs Ground Truth: {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
    print(f"   âš ï¸  LÆ¯U Ã: Val accuracy trong training Ä‘Æ°á»£c tÃ­nh vá»›i oracle labels (khÃ´ng pháº£i ground truth)")
    print(f"   âš ï¸  Final accuracy Ä‘Æ°á»£c tÃ­nh vá»›i ground truth labels")
    print(f"   ğŸ’¡ Náº¿u oracle khÃ´ng chÃ­nh xÃ¡c 100%, sáº½ cÃ³ sá»± khÃ¡c biá»‡t giá»¯a val_accuracy vÃ  final accuracy")
    
    # KIá»‚M TRA: Náº¿u oracle predict táº¥t cáº£ lÃ  má»™t class, cÃ³ thá»ƒ cÃ³ váº¥n Ä‘á»
    all_distributions = [eval_dist, seed_dist, val_dist]
    all_single_class = all(len(d) == 1 for d in all_distributions)
    if all_single_class:
        print(f"\nâš ï¸  Cáº¢NH BÃO: Oracle Ä‘ang predict táº¥t cáº£ lÃ  má»™t class duy nháº¥t!")
        print(f"   Äiá»u nÃ y cÃ³ thá»ƒ do:")
        print(f"   1. Oracle threshold quÃ¡ cao/tháº¥p")
        print(f"   2. Dá»¯ liá»‡u thá»±c sá»± chá»‰ cÃ³ má»™t class")
        print(f"   3. Oracle model cÃ³ váº¥n Ä‘á»")
        print(f"   ğŸ’¡ Sáº½ thá»­ kiá»ƒm tra probabilities vÃ  cÃ³ thá»ƒ Ä‘iá»u chá»‰nh threshold...")
        
        # Kiá»ƒm tra probabilities Ä‘á»ƒ xem cÃ³ pháº£i do threshold khÃ´ng
        try:
            test_sample_size = min(100, X_eval_s.shape[0])
            test_indices = rng.choice(X_eval_s.shape[0], size=test_sample_size, replace=False)
            # Sá»­ dá»¥ng X_eval_s Ä‘Ã£ Ä‘Æ°á»£c scale/normalize
            test_data = X_eval_s[test_indices]
            test_probs = oracle.predict_proba(test_data)
            print(f"   ğŸ“Š Test probabilities trÃªn {test_sample_size} samples:")
            print(f"      Range: [{test_probs.min():.4f}, {test_probs.max():.4f}]")
            print(f"      Mean: {test_probs.mean():.4f}, Median: {np.median(test_probs):.4f}")
            print(f"      Threshold hiá»‡n táº¡i: {oracle.model_threshold}")
            
            # Náº¿u probabilities táº­p trung gáº§n threshold, cÃ³ thá»ƒ cáº§n Ä‘iá»u chá»‰nh
            if test_probs.min() < oracle.model_threshold < test_probs.max():
                print(f"   ğŸ’¡ Probabilities cÃ³ cáº£ dÆ°á»›i vÃ  trÃªn threshold - cÃ³ thá»ƒ cÃ³ cáº£ 2 classes")
                print(f"      Thá»­ vá»›i threshold tháº¥p hÆ¡n cÃ³ thá»ƒ giÃºp phÃ¢n biá»‡t tá»‘t hÆ¡n")
            elif test_probs.max() < oracle.model_threshold:
                # Táº¥t cáº£ probabilities Ä‘á»u dÆ°á»›i threshold - cáº§n giáº£m threshold
                suggested_threshold = np.percentile(test_probs, 50)  # Median
                print(f"   âš ï¸  Táº¤T Cáº¢ probabilities Ä‘á»u dÆ°á»›i threshold {oracle.model_threshold}")
                print(f"   ğŸ’¡ Äá» xuáº¥t giáº£m threshold xuá»‘ng {suggested_threshold:.4f} (median) Ä‘á»ƒ phÃ¢n biá»‡t classes")
                print(f"   ğŸ”„ Äang Ä‘iá»u chá»‰nh threshold...")
                oracle.model_threshold = suggested_threshold
                # Test láº¡i vá»›i threshold má»›i
                test_predictions_new = oracle(X_eval_s[test_indices])
                test_dist_new = dict(zip(*np.unique(test_predictions_new, return_counts=True)))
                print(f"   âœ… Vá»›i threshold má»›i {suggested_threshold:.4f}: {test_dist_new}")
                
                # QUAN TRá»ŒNG: Re-query seed, val, eval vá»›i threshold má»›i
                print(f"   ğŸ”„ Re-querying seed, val, eval vá»›i threshold má»›i...")
                # Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ (scaled hoáº·c raw tÃ¹y theo model type)
                y_eval = oracle(X_eval_s)
                y_seed = oracle(X_seed_s)
                y_val = oracle(X_val_s)
                eval_dist = dict(zip(*np.unique(y_eval, return_counts=True)))
                seed_dist = dict(zip(*np.unique(y_seed, return_counts=True)))
                val_dist = dict(zip(*np.unique(y_val, return_counts=True)))
                print(f"   âœ… Distribution sau khi Ä‘iá»u chá»‰nh threshold:")
                print(f"      Eval: {eval_dist}")
                print(f"      Seed: {seed_dist}")
                print(f"      Val: {val_dist}")
        except Exception as e:
            print(f"   âš ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra probabilities: {e}")

    metrics_history = []
    labeled_X = X_seed_s
    labeled_y = y_seed

    def evaluate(model, round_id: int, total_labels: int):
        probs = np.squeeze(model(X_eval_s), axis=-1)
        
        # Tá»‘i Æ°u threshold dá»±a trÃªn F1-score vá»›i ground truth labels
        # Äiá»u nÃ y quan trá»ng vá»›i class imbalance nghiÃªm trá»ng
        thresholds = np.linspace(0.1, 0.9, 81)
        best_f1 = -1
        best_threshold = 0.5
        best_preds = (probs >= 0.5).astype(int)
        
        for thresh in thresholds:
            preds_thresh = (probs >= thresh).astype(int)
            _, _, f1_thresh, _ = precision_recall_fscore_support(
                y_eval_gt, preds_thresh, average="binary", zero_division=0
            )
            if f1_thresh > best_f1:
                best_f1 = f1_thresh
                best_threshold = thresh
                best_preds = preds_thresh
        
        # Sá»­ dá»¥ng threshold tá»‘i Æ°u
        preds = best_preds
        
        # QUAN TRá»ŒNG: Agreement = so sÃ¡nh predictions cá»§a surrogate vá»›i predictions cá»§a target model
        # Accuracy = so sÃ¡nh predictions cá»§a surrogate vá»›i ground truth labels
        agreement = (preds == y_eval).mean()  # y_eval lÃ  predictions tá»« target model (oracle)
        acc = accuracy_score(y_eval_gt, preds)  # y_eval_gt lÃ  ground truth labels
        acc_vs_oracle = accuracy_score(y_eval, preds)  # Accuracy so vá»›i oracle (giá»‘ng agreement nhÆ°ng dÃ¹ng accuracy_score)
        balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_eval_gt, preds, average="binary", zero_division=0
        )
        try:
            auc = roc_auc_score(y_eval_gt, probs)
        except ValueError:
            auc = float("nan")

        # TÃ­nh sá»‘ queries thá»±c táº¿ (khÃ´ng tÃ­nh seed vÃ  val)
        actual_queries = total_labels - seed_size - val_size
        
        # Log metrics Ä‘á»ƒ giáº£i thÃ­ch sá»± khÃ¡c biá»‡t
        print(f"\nğŸ“Š Round {round_id} Evaluation:")
        print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
        print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
        print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
        print(f"   ğŸ’¡ Giáº£i thÃ­ch: Val accuracy trong training ({agreement:.4f}) cao vÃ¬ so vá»›i oracle labels")
        print(f"   ğŸ’¡ Final accuracy ({acc:.4f}) tháº¥p hÆ¡n vÃ¬ so vá»›i ground truth (oracle khÃ´ng hoÃ n háº£o)")
        
        metrics = {
            "round": round_id,
            "labels_used": int(total_labels),
            "queries_used": int(actual_queries),  # Sá»‘ queries thá»±c táº¿ (chá»‰ tÃ­nh active learning)
            "optimal_threshold": float(best_threshold),
            "surrogate_acc": float(acc),  # Accuracy vs Ground Truth
            "surrogate_acc_vs_oracle": float(acc_vs_oracle),  # Accuracy vs Oracle (tÆ°Æ¡ng tá»± agreement)
            "surrogate_balanced_acc": float(balanced_acc),  # Quan trá»ng vá»›i class imbalance
            "surrogate_auc": float(auc),
            "surrogate_precision": float(precision),
            "surrogate_recall": float(recall),
            "surrogate_f1": float(f1),
            "agreement_with_target": float(agreement),
            "oracle_acc_vs_gt": float(oracle_acc_vs_gt),  # Äá»™ chÃ­nh xÃ¡c cá»§a oracle vá»›i ground truth
        }
        metrics_history.append(metrics)
        return metrics

    # QUAN TRá»ŒNG: Theo nghiÃªn cá»©u, dÃ¹ng early_stopping=30 vÃ  num_epochs cao (100)
    # Ä‘á»ƒ model cÃ³ Ä‘á»§ thá»i gian há»c vÃ  trÃ¡nh underfitting
    # early_stopping=30: patience Ä‘á»§ lá»›n Ä‘á»ƒ vÆ°á»£t qua local minima
    # num_epochs: Ä‘á»§ epochs Ä‘á»ƒ model há»c tá»‘t vá»›i nhiá»u dá»¯ liá»‡u (máº·c Ä‘á»‹nh 100 theo nghiÃªn cá»©u)
    if attacker_type == "lgb":
        # LightGBM attacker khÃ´ng cáº§n scale data
        attacker = LGBAttacker(seed=seed)
        attacker.train_model(labeled_X, labeled_y, X_val, y_val, boosting_rounds=100, early_stopping=15)
        # Vá»›i LightGBM, khÃ´ng cáº§n scale data Ä‘á»ƒ evaluate
        def evaluate_lgb(model, round_id, total_labels):
            probs = model(X_eval)
            # LightGBM predict tráº£ vá» 1D array hoáº·c 2D array
            if probs.ndim > 1:
                probs = probs.flatten()
            
            # Tá»‘i Æ°u threshold dá»±a trÃªn F1-score vá»›i ground truth labels
            # Äiá»u nÃ y quan trá»ng vá»›i class imbalance nghiÃªm trá»ng
            thresholds = np.linspace(0.1, 0.9, 81)
            best_f1 = -1
            best_threshold = 0.5
            best_preds = (probs >= 0.5).astype(int)
            
            for thresh in thresholds:
                preds_thresh = (probs >= thresh).astype(int)
                _, _, f1_thresh, _ = precision_recall_fscore_support(
                    y_eval_gt, preds_thresh, average="binary", zero_division=0
                )
                if f1_thresh > best_f1:
                    best_f1 = f1_thresh
                    best_threshold = thresh
                    best_preds = preds_thresh
            
            # Sá»­ dá»¥ng threshold tá»‘i Æ°u
            preds = best_preds
            
            # QUAN TRá»ŒNG: Agreement = so sÃ¡nh predictions cá»§a surrogate vá»›i predictions cá»§a target model
            # Accuracy = so sÃ¡nh predictions cá»§a surrogate vá»›i ground truth labels
            agreement = (preds == y_eval).mean()  # y_eval lÃ  predictions tá»« target model (oracle)
            acc = accuracy_score(y_eval_gt, preds)  # y_eval_gt lÃ  ground truth labels
            acc_vs_oracle = accuracy_score(y_eval, preds)  # Accuracy so vá»›i oracle (giá»‘ng agreement nhÆ°ng dÃ¹ng accuracy_score)
            balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_eval_gt, preds, average="binary", zero_division=0
            )
            try:
                auc = roc_auc_score(y_eval_gt, probs)
            except ValueError:
                auc = float("nan")

            # TÃ­nh sá»‘ queries thá»±c táº¿ (khÃ´ng tÃ­nh seed vÃ  val)
            actual_queries = total_labels - seed_size - val_size
            
            # Log metrics Ä‘á»ƒ giáº£i thÃ­ch sá»± khÃ¡c biá»‡t
            print(f"\nğŸ“Š Round {round_id} Evaluation:")
            print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
            print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
            print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
            print(f"   ğŸ’¡ Giáº£i thÃ­ch: Val accuracy trong training ({agreement:.4f}) cao vÃ¬ so vá»›i oracle labels")
            print(f"   ğŸ’¡ Final accuracy ({acc:.4f}) tháº¥p hÆ¡n vÃ¬ so vá»›i ground truth (oracle khÃ´ng hoÃ n háº£o)")
            
            metrics = {
                "round": round_id,
                "labels_used": int(total_labels),
                "queries_used": int(actual_queries),  # Sá»‘ queries thá»±c táº¿ (chá»‰ tÃ­nh active learning)
                "optimal_threshold": float(best_threshold),
                "surrogate_acc": float(acc),  # Accuracy vs Ground Truth
                "surrogate_acc_vs_oracle": float(acc_vs_oracle),  # Accuracy vs Oracle (tÆ°Æ¡ng tá»± agreement)
                "surrogate_balanced_acc": float(balanced_acc),  # Quan trá»ng vá»›i class imbalance
                "surrogate_auc": float(auc),
                "surrogate_precision": float(precision),
                "surrogate_recall": float(recall),
                "surrogate_f1": float(f1),
                "agreement_with_target": float(agreement),
                "oracle_acc_vs_gt": float(oracle_acc_vs_gt),  # Äá»™ chÃ­nh xÃ¡c cá»§a oracle vá»›i ground truth
            }
            metrics_history.append(metrics)
            return metrics
        
        evaluate = evaluate_lgb
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])
    elif attacker_type == "dual":
        # DualDNN attacker cáº§n scale data vÃ  cáº£ ground truth labels (oracle predictions)
        attacker = KerasDualAttacker(early_stopping=30, seed=seed)
        # DualDNN train vá»›i (X, y_true) - y_true lÃ  oracle labels
        attacker.train_model(labeled_X, labeled_y, labeled_y, X_val_s, y_val, y_val, num_epochs=num_epochs)
        
        def evaluate_dual(model, round_id, total_labels):
            # DualDNN cáº§n cáº£ X vÃ  y_true (oracle labels) khi predict
            # __call__ nháº­n 2 tham sá»‘ riÃªng biá»‡t (X, y_true), khÃ´ng pháº£i tuple
            probs = np.squeeze(model(X_eval_s, y_eval), axis=-1)
            
            # Tá»‘i Æ°u threshold dá»±a trÃªn F1-score vá»›i ground truth labels
            thresholds = np.linspace(0.1, 0.9, 81)
            best_f1 = -1
            best_threshold = 0.5
            best_preds = (probs >= 0.5).astype(int)
            
            for thresh in thresholds:
                preds_thresh = (probs >= thresh).astype(int)
                _, _, f1_thresh, _ = precision_recall_fscore_support(
                    y_eval_gt, preds_thresh, average="binary", zero_division=0
                )
                if f1_thresh > best_f1:
                    best_f1 = f1_thresh
                    best_threshold = thresh
                    best_preds = preds_thresh
            
            # Sá»­ dá»¥ng threshold tá»‘i Æ°u
            preds = best_preds
            
            # Agreement vÃ  accuracy metrics
            agreement = (preds == y_eval).mean()
            acc = accuracy_score(y_eval_gt, preds)
            acc_vs_oracle = accuracy_score(y_eval, preds)
            balanced_acc = balanced_accuracy_score(y_eval_gt, preds)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_eval_gt, preds, average="binary", zero_division=0
            )
            try:
                auc = roc_auc_score(y_eval_gt, probs)
            except ValueError:
                auc = float("nan")
            
            # TÃ­nh sá»‘ queries thá»±c táº¿
            actual_queries = total_labels - seed_size - val_size
            
            print(f"\nğŸ“Š Round {round_id} Evaluation (DualDNN):")
            print(f"   Agreement (vs Oracle): {agreement:.4f} ({agreement*100:.2f}%)")
            print(f"   Accuracy (vs Ground Truth): {acc:.4f} ({acc*100:.2f}%)")
            print(f"   Oracle Accuracy (vs Ground Truth): {oracle_acc_vs_gt:.4f} ({oracle_acc_vs_gt*100:.2f}%)")
            
            metrics = {
                "round": round_id,
                "labels_used": int(total_labels),
                "queries_used": int(actual_queries),
                "optimal_threshold": float(best_threshold),
                "surrogate_acc": float(acc),
                "surrogate_acc_vs_oracle": float(acc_vs_oracle),
                "surrogate_balanced_acc": float(balanced_acc),
                "surrogate_auc": float(auc),
                "surrogate_precision": float(precision),
                "surrogate_recall": float(recall),
                "surrogate_f1": float(f1),
                "agreement_with_target": float(agreement),
                "oracle_acc_vs_gt": float(oracle_acc_vs_gt),
            }
            metrics_history.append(metrics)
            return metrics
        
        evaluate = evaluate_dual
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])
    else:
        # Keras attacker cáº§n scale data
        attacker = KerasAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
        attacker.train_model(labeled_X, labeled_y, X_val_s, y_val, num_epochs=num_epochs)
        evaluate(attacker, round_id=0, total_labels=labeled_X.shape[0])

    # Track tá»•ng queries Ä‘á»ƒ Ä‘áº£m báº£o chÃ­nh xÃ¡c
    total_queries_target = query_batch * num_rounds
    total_queries_accumulated = 0
    # Cho phÃ©p lá»‡ch tá»‘i Ä‘a 10% (dÆ° chá»© khÃ´ng Ä‘Æ°á»£c thiáº¿u)
    min_queries_acceptable = int(total_queries_target * 0.9)  # Ãt nháº¥t 90% cá»§a target
    max_queries_acceptable = int(total_queries_target * 1.1)  # Tá»‘i Ä‘a 110% cá»§a target
    
    print(f"\nğŸ“‹ Má»¥c tiÃªu queries: {total_queries_target:,} ({query_batch:,} queries/round Ã— {num_rounds} rounds)")
    print(f"   ğŸ“Š Cho phÃ©p lá»‡ch: {min_queries_acceptable:,} - {max_queries_acceptable:,} queries (90% - 110%)")
    print(f"   âš ï¸  Quan trá»ng: KhÃ´ng Ä‘Æ°á»£c thiáº¿u queries! (tá»‘i thiá»ƒu {min_queries_acceptable:,})")
    
    # Kiá»ƒm tra pool ban Ä‘áº§u cÃ³ Ä‘á»§ khÃ´ng
    if X_pool.shape[0] < total_queries_target:
        print(f"\nâš ï¸  Cáº¢NH BÃO: Pool ban Ä‘áº§u ({X_pool.shape[0]:,}) < Tá»•ng queries dá»± kiáº¿n ({total_queries_target:,})")
        print(f"   ğŸ’¡ Pool sáº½ cáº¡n kiá»‡t trÆ°á»›c khi Ä‘áº¡t Ä‘á»§ queries. Sáº½ cá»‘ gáº¯ng láº¥y tá»‘i Ä‘a cÃ³ thá»ƒ.")
    
    for query_round in range(1, num_rounds + 1):
        # Kiá»ƒm tra xem cÃ²n cáº§n bao nhiÃªu queries ná»¯a
        queries_remaining_needed = total_queries_target - total_queries_accumulated
        
        # Náº¿u Ä‘Ã£ Ä‘áº¡t Ä‘á»§ queries, dá»«ng láº¡i
        if total_queries_accumulated >= total_queries_target:
            print(f"\nâœ… ÄÃ£ Ä‘áº¡t Ä‘á»§ queries dá»± kiáº¿n ({total_queries_target:,}). Dá»«ng active learning.")
            break
        
        # Náº¿u pool cÃ²n láº¡i Ã­t hÆ¡n query_batch, váº«n cá»‘ gáº¯ng láº¥y tá»‘i Ä‘a cÃ³ thá»ƒ
        pool_remaining = X_pool.shape[0]
        queries_to_get_this_round = min(query_batch, pool_remaining, queries_remaining_needed)
        
        if queries_to_get_this_round <= 0:
            print(f"\nâš ï¸  Round {query_round}: KhÃ´ng cÃ²n queries Ä‘á»ƒ láº¥y. Pool: {pool_remaining}, Cáº§n: {queries_remaining_needed}")
            break
        
        if pool_remaining < query_batch:
            print(f"\nâš ï¸  Round {query_round}: Pool cÃ²n láº¡i ({pool_remaining}) < query_batch ({query_batch}).")
            print(f"   ğŸ”„ Sáº½ láº¥y tá»‘i Ä‘a {queries_to_get_this_round} queries tá»« pool cÃ²n láº¡i.")
        
        # GIáº¢I PHÃP 1: DÃ¹ng Entropy + k-medoids thay vÃ¬ random sampling
        # Theo nghiÃªn cá»©u: Entropy Ä‘á»ƒ chá»n Ä‘iá»ƒm cÃ³ Ä‘á»™ báº¥t Ä‘á»‹nh cao,
        # sau Ä‘Ã³ k-medoids Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘a dáº¡ng vÃ  trÃ¡nh láº¥y nhiá»u Ä‘iá»ƒm nhiá»…u
        print(f"\nğŸ”„ Round {query_round}: Äang chá»n queries báº±ng Entropy + k-medoids...")
        
        # Chá»n dá»¯ liá»‡u Ä‘á»ƒ query dá»±a trÃªn attacker type
        pool_for_entropy = X_pool_s if attacker_type in ["keras", "dual"] else X_pool
        
        # QUAN TRá»ŒNG: Vá»›i dualDNN, cáº§n oracle labels cho pool Ä‘á»ƒ entropy sampling
        # Query oracle cho pool náº¿u chÆ°a cÃ³ (chá»‰ cho dualDNN)
        pool_labels_for_entropy = None
        if attacker_type == "dual":
            # Query oracle Ä‘á»ƒ láº¥y labels cho pool (dÃ¹ng cho entropy sampling vá»›i dual=True)
            pool_labels_for_entropy = oracle(pool_for_entropy)
        
        # BÆ°á»›c 1: DÃ¹ng entropy Ä‘á»ƒ chá»n 10000 Ä‘iá»ƒm cÃ³ Ä‘á»™ báº¥t Ä‘á»‹nh cao
        # (k-medoids khÃ´ng scale tá»‘t vá»›i toÃ n bá»™ pool)
        entropy_candidates = min(10000, pool_for_entropy.shape[0])
        dual_flag = (attacker_type == "dual")
        q_idx = entropy_sampling(
            attacker, 
            pool_for_entropy, 
            pool_labels_for_entropy if dual_flag else np.zeros(pool_for_entropy.shape[0]),  # y cáº§n thiáº¿t cho dualDNN
            n_instances=entropy_candidates,
            dual=dual_flag
        )
        X_med = pool_for_entropy[q_idx]
        
        # BÆ°á»›c 2: DÃ¹ng k-medoids Ä‘á»ƒ chá»n queries_to_get_this_round Ä‘iá»ƒm Ä‘áº¡i diá»‡n tá»« cÃ¡c Ä‘iá»ƒm entropy cao
        # QUAN TRá»ŒNG: queries_to_get_this_round Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh á»Ÿ trÃªn (dÃ²ng 583)
        
        # Äáº£m báº£o sá»‘ clusters khÃ´ng lá»›n hÆ¡n sá»‘ samples cÃ³ sáºµn
        num_clusters = min(queries_to_get_this_round, X_med.shape[0])
        
        if num_clusters > 0:
            kmed = KMedoids(n_clusters=num_clusters, init='k-medoids++', random_state=seed)
            kmed.fit(X_med)
            query_idx_in_med = kmed.medoid_indices_
            query_idx = q_idx[query_idx_in_med]
        else:
            # Náº¿u khÃ´ng Ä‘á»§ samples, láº¥y táº¥t cáº£ cÃ³ sáºµn
            query_idx = q_idx[:min(queries_to_get_this_round, len(q_idx))]
        
        print(f"   âœ… ÄÃ£ chá»n {len(query_idx)} queries tá»« {entropy_candidates} entropy candidates (target: {queries_to_get_this_round})")

        # Query oracle
        # - Vá»›i Keras: Query vá»›i dá»¯ liá»‡u Ä‘Ã£ scale (X_pool_s)
        # - Vá»›i LightGBM: Query vá»›i raw data (X_pool), FlexibleLGBTarget sáº½ tá»± Ä‘á»™ng normalize
        X_query_s = pool_for_entropy[query_idx]
        y_query = oracle(X_query_s)

        # Log class distribution Ä‘á»ƒ kiá»ƒm tra imbalance
        query_dist = dict(zip(*np.unique(y_query, return_counts=True)))
        print(f"   ğŸ“Š Query distribution: {query_dist}")
        
        # KIá»‚M TRA VÃ€ CÃ‚N Báº°NG CLASS DISTRIBUTION
        # Theo nghiÃªn cá»©u: Class imbalance cÃ³ thá»ƒ lÃ m model bias vá» class Ä‘a sá»‘
        # Giáº£i phÃ¡p: Äáº£m báº£o má»—i class cÃ³ Ã­t nháº¥t 30% queries (hoáº·c tá»‘i thiá»ƒu 100 samples)
        total_queries = len(y_query)
        if total_queries > 0:
            max_class_ratio = max(query_dist.values()) / total_queries
            min_class_samples = min(query_dist.values()) if len(query_dist) > 1 else 0
            min_required_samples = max(100, int(query_batch * 0.3))  # Tá»‘i thiá»ƒu 30% hoáº·c 100 samples
            
            if max_class_ratio > 0.7 or min_class_samples < min_required_samples:
                print(f"   âš ï¸  Class imbalance: Má»™t class chiáº¿m {max_class_ratio*100:.1f}%, class thiá»ƒu sá»‘ cÃ³ {min_class_samples} samples")
                print(f"   ğŸ’¡ Cáº§n tá»‘i thiá»ƒu {min_required_samples} samples cho má»—i class")
                
                # CÃ¢n báº±ng báº±ng cÃ¡ch láº¥y thÃªm samples tá»« class thiá»ƒu sá»‘
                if len(query_dist) == 2:
                    minority_class = min(query_dist.items(), key=lambda x: x[1])[0]
                    majority_class = max(query_dist.items(), key=lambda x: x[1])[0]
                    minority_count = query_dist[minority_class]
                    
                    if minority_count < min_required_samples:
                        needed_samples = min_required_samples - minority_count
                        print(f"   ğŸ”„ Cáº§n thÃªm {needed_samples} samples tá»« class {minority_class}...")
                        
                        # Query oracle trÃªn toÃ n bá»™ pool cÃ²n láº¡i Ä‘á»ƒ tÃ¬m class thiá»ƒu sá»‘
                        remaining_pool_size = X_pool_s.shape[0]
                        if remaining_pool_size > needed_samples:
                            # TÄƒng sample_size Ä‘á»ƒ tÃ¬m Ä‘á»§ class thiá»ƒu sá»‘ (cÃ³ thá»ƒ pool chá»§ yáº¿u lÃ  class Ä‘a sá»‘)
                            # Æ¯á»›c tÃ­nh: náº¿u class thiá»ƒu sá»‘ chiáº¿m ~10%, cáº§n query ~10x Ä‘á»ƒ tÃ¬m Ä‘á»§
                            sample_size = min(needed_samples * 10, remaining_pool_size)
                            candidate_idx = rng.choice(remaining_pool_size, size=sample_size, replace=False)
                            X_candidates = X_pool_s[candidate_idx]
                            y_candidates = oracle(X_candidates)
                            
                            # Lá»c chá»‰ láº¥y class thiá»ƒu sá»‘
                            minority_mask = y_candidates == minority_class
                            minority_found = np.sum(minority_mask)
                            
                            if minority_found >= needed_samples:
                                # Láº¥y Ä‘á»§ samples tá»« class thiá»ƒu sá»‘
                                minority_indices = candidate_idx[minority_mask][:needed_samples]
                                X_additional = X_pool_s[minority_indices]
                                y_additional = oracle(X_additional)
                                
                                X_query_s = np.vstack([X_query_s, X_additional])
                                y_query = np.concatenate([y_query, y_additional])
                                query_idx = np.concatenate([query_idx, minority_indices])
                                
                                balanced_dist = dict(zip(*np.unique(y_query, return_counts=True)))
                                print(f"   âœ… ÄÃ£ cÃ¢n báº±ng: {balanced_dist}")
                            else:
                                print(f"   âš ï¸  Chá»‰ tÃ¬m tháº¥y {minority_found}/{needed_samples} samples tá»« class {minority_class}")
                                if minority_found > 0:
                                    minority_indices = candidate_idx[minority_mask]
                                    X_additional = X_pool_s[minority_indices]
                                    y_additional = oracle(X_additional)
                                    X_query_s = np.vstack([X_query_s, X_additional])
                                    y_query = np.concatenate([y_query, y_additional])
                                    query_idx = np.concatenate([query_idx, minority_indices])
                                    
                                    final_dist = dict(zip(*np.unique(y_query, return_counts=True)))
                                    final_ratio = min(final_dist.values()) / sum(final_dist.values())
                                    print(f"   âœ… ÄÃ£ thÃªm {minority_found} samples, distribution: {final_dist} (minority ratio: {final_ratio*100:.1f}%)")
                                else:
                                    print(f"   âš ï¸  KhÃ´ng tÃ¬m tháº¥y samples tá»« class {minority_class} trong pool cÃ²n láº¡i")
                                    print(f"   ğŸ’¡ CÃ³ thá»ƒ pool cÃ²n láº¡i chá»§ yáº¿u lÃ  class {majority_class}")
                elif len(query_dist) == 1:
                    print(f"   âš ï¸  Cáº¢NH BÃO: Chá»‰ cÃ³ 1 class trong queries! Model sáº½ khÃ´ng há»c Ä‘Æ°á»£c phÃ¢n biá»‡t 2 classes")
                    # Thá»­ láº¥y thÃªm má»™t sá»‘ random samples Ä‘á»ƒ Ä‘áº£m báº£o cÃ³ cáº£ 2 classes
                    remaining_pool_size = X_pool_s.shape[0]
                    if remaining_pool_size > 0:
                        additional_samples = min(200, remaining_pool_size, query_batch // 2)  # Láº¥y thÃªm 50% hoáº·c tá»‘i Ä‘a 200
                        additional_idx = rng.choice(remaining_pool_size, size=additional_samples, replace=False)
                        X_additional = X_pool_s[additional_idx]
                        y_additional = oracle(X_additional)
                        additional_dist = dict(zip(*np.unique(y_additional, return_counts=True)))
                        print(f"   ğŸ”„ Láº¥y thÃªm {additional_samples} random samples: {additional_dist}")
                        
                        # ThÃªm vÃ o queries náº¿u cÃ³ class má»›i
                        if len(additional_dist) > len(query_dist) or any(c not in query_dist for c in additional_dist):
                            X_query_s = np.vstack([X_query_s, X_additional])
                            y_query = np.concatenate([y_query, y_additional])
                            query_idx = np.concatenate([query_idx, additional_idx])
                            print(f"   âœ… ÄÃ£ thÃªm samples, distribution má»›i: {dict(zip(*np.unique(y_query, return_counts=True)))}")

        # QUAN TRá»ŒNG: Äáº£m báº£o sá»‘ queries chÃ­nh xÃ¡c = queries_to_get_this_round
        # KHÃ”NG BAO GIá»œ Ä‘Æ°á»£c thiáº¿u queries trá»« khi pool thá»±c sá»± cáº¡n kiá»‡t!
        actual_queries = len(y_query)
        
        # TÃ­nh queries cÃ²n cáº§n Ä‘á»ƒ Ä‘áº¡t target
        queries_remaining_needed = total_queries_target - total_queries_accumulated
        
        # Má»¥c tiÃªu queries cho round nÃ y: khÃ´ng vÆ°á»£t quÃ¡ queries_remaining_needed vÃ  khÃ´ng vÆ°á»£t quÃ¡ 110% cá»§a query_batch
        max_queries_this_round = min(int(query_batch * 1.1), queries_remaining_needed) if queries_remaining_needed > 0 else int(query_batch * 1.1)
        min_queries_this_round = queries_to_get_this_round  # Ãt nháº¥t pháº£i Ä‘áº¡t má»¥c tiÃªu cho round nÃ y
        
        # QUAN TRá»ŒNG: Náº¿u thiáº¿u queries, Báº®T BUá»˜C pháº£i bá»• sung tá»« pool
        # Chá»‰ cháº¥p nháº­n thiáº¿u náº¿u pool thá»±c sá»± cáº¡n kiá»‡t
        if actual_queries < min_queries_this_round:
            # QUAN TRá»ŒNG: Náº¿u cÃ³ Ã­t hÆ¡n má»¥c tiÃªu, Báº®T BUá»˜C pháº£i bá»• sung
            needed_samples = min_queries_this_round - actual_queries
            print(f"   âš ï¸  CHá»ˆ CÃ“ {actual_queries}/{min_queries_this_round} queries. Cáº¦N Bá»” SUNG {needed_samples} queries!")
            
            remaining_pool_size = X_pool_s.shape[0]
            if remaining_pool_size >= needed_samples:
                # Láº¥y thÃªm random samples tá»« pool cÃ²n láº¡i
                additional_idx = rng.choice(remaining_pool_size, size=needed_samples, replace=False)
                X_additional = X_pool_s[additional_idx]
                y_additional = oracle(X_additional)
                
                X_query_s = np.vstack([X_query_s, X_additional])
                y_query = np.concatenate([y_query, y_additional])
                query_idx = np.concatenate([query_idx, additional_idx])
                
                print(f"   âœ… ÄÃ£ bá»• sung {needed_samples} queries tá»« pool. Total: {len(y_query)}")
                actual_queries = len(y_query)
            else:
                # Pool khÃ´ng Ä‘á»§, láº¥y táº¥t cáº£ cÃ²n láº¡i
                if remaining_pool_size > 0:
                    X_additional = X_pool_s
                    y_additional = oracle(X_additional)
                    
                    X_query_s = np.vstack([X_query_s, X_additional])
                    y_query = np.concatenate([y_query, y_additional])
                    all_indices = np.arange(X_pool_s.shape[0])
                    query_idx = np.concatenate([query_idx, all_indices])
                    
                    actual_queries = len(y_query)
                    print(f"   âš ï¸  Pool cÃ²n láº¡i chá»‰ cÃ³ {remaining_pool_size} samples. ÄÃ£ láº¥y táº¥t cáº£.")
                    print(f"   ğŸ“Š Total queries trong round nÃ y: {actual_queries} (má»¥c tiÃªu: {min_queries_this_round})")
                    if actual_queries < min_queries_this_round:
                        missing = min_queries_this_round - actual_queries
                        print(f"   âŒ VáºªN THIáº¾U {missing} queries do pool cáº¡n kiá»‡t!")
                else:
                    print(f"   âŒ Lá»–I NGHIÃŠM TRá»ŒNG: Pool Ä‘Ã£ cáº¡n kiá»‡t! Chá»‰ cÃ³ {actual_queries} queries thay vÃ¬ {min_queries_this_round}")
                    print(f"   âŒ Thiáº¿u {min_queries_this_round - actual_queries} queries! Äiá»u nÃ y sáº½ áº£nh hÆ°á»Ÿng nghiÃªm trá»ng Ä‘áº¿n hiá»‡u suáº¥t!")
        
        # Giá»›i háº¡n tá»‘i Ä‘a: khÃ´ng vÆ°á»£t quÃ¡ max_queries_this_round (110% cá»§a query_batch hoáº·c queries cÃ²n cáº§n)
        if actual_queries > max_queries_this_round:
            print(f"   âš ï¸  Class balancing Ä‘Ã£ thÃªm {actual_queries - max_queries_this_round} queries (vÆ°á»£t quÃ¡ 110%).")
            print(f"   ğŸ”„ Giá»›i háº¡n láº¡i vá» {max_queries_this_round} queries.")
            X_query_s = X_query_s[:max_queries_this_round]
            y_query = y_query[:max_queries_this_round]
            query_idx = query_idx[:max_queries_this_round]
            actual_queries = max_queries_this_round
            final_dist = dict(zip(*np.unique(y_query, return_counts=True)))
            print(f"   ğŸ“Š Query distribution sau khi giá»›i háº¡n: {final_dist}")
        
        final_query_count = actual_queries
        
        # QUAN TRá»ŒNG: Verify sá»‘ queries trÆ°á»›c khi thÃªm vÃ o labeled set
        queries_this_round = len(y_query)
        total_queries_accumulated += queries_this_round
        
        # Kiá»ƒm tra xem cÃ³ Ä‘áº¡t má»¥c tiÃªu khÃ´ng
        if queries_this_round >= min_queries_this_round:
            status = "âœ…"
        else:
            status = "âš ï¸"
        
        print(f"   {status} Round {query_round}: ÄÃ£ chá»n {queries_this_round} queries (má»¥c tiÃªu: {min_queries_this_round}, tá»‘i Ä‘a: {max_queries_this_round})")
        print(f"   ğŸ“Š Tá»•ng queries tÃ­ch lÅ©y: {total_queries_accumulated:,}/{total_queries_target:,} ({total_queries_accumulated/total_queries_target*100:.1f}%)")
        
        # QUAN TRá»ŒNG: Verify queries_this_round Ä‘áº¡t má»¥c tiÃªu trÆ°á»›c khi xÃ³a tá»« pool
        # Náº¿u thiáº¿u queries vÃ  pool váº«n cÃ²n, pháº£i cáº£nh bÃ¡o nghiÃªm trá»ng
        if queries_this_round < min_queries_this_round:
            missing = min_queries_this_round - queries_this_round
            pool_remaining_before_delete = X_pool.shape[0]
            print(f"\n   âŒ Lá»–I NGHIÃŠM TRá»ŒNG: Round {query_round} chá»‰ cÃ³ {queries_this_round} queries thay vÃ¬ {min_queries_this_round}!")
            print(f"   âŒ Thiáº¿u {missing} queries! Äiá»u nÃ y sáº½ áº£nh hÆ°á»Ÿng nghiÃªm trá»ng Ä‘áº¿n hiá»‡u suáº¥t!")
            print(f"   ğŸ’¡ Pool cÃ²n láº¡i trÆ°á»›c khi xÃ³a: {pool_remaining_before_delete:,} samples")
            print(f"   ğŸ’¡ Kiá»ƒm tra logic bá»• sung queries hoáº·c pool size ban Ä‘áº§u!")
            # KHÃ”NG raise error vÃ¬ cÃ³ thá»ƒ pool thá»±c sá»± cáº¡n kiá»‡t, nhÆ°ng cáº£nh bÃ¡o rÃµ rÃ ng
        
        labeled_X = np.vstack([labeled_X, X_query_s])
        labeled_y = np.concatenate([labeled_y, y_query])

        # XÃ³a tá»« pool (Ä‘áº£m báº£o query_idx unique)
        query_idx_unique = np.unique(query_idx)
        X_pool = np.delete(X_pool, query_idx_unique, axis=0)
        if attacker_type in ["keras", "dual"]:
            # X_pool_s cÃ³ sáºµn cho Keras vÃ  dualDNN
            X_pool_s = np.delete(X_pool_s, query_idx_unique, axis=0)
            # Vá»›i dualDNN, cÅ©ng cáº§n xÃ³a labels Ä‘Ã£ query khá»i pool_labels_for_entropy
            if attacker_type == "dual" and pool_labels_for_entropy is not None:
                pool_labels_for_entropy = np.delete(pool_labels_for_entropy, query_idx_unique, axis=0)
        else:
            # Vá»›i LightGBM, X_pool_s = X_pool
            X_pool_s = X_pool
        
        print(f"   ğŸ“Š Pool cÃ²n láº¡i: {X_pool.shape[0]:,} samples")

        # QUAN TRá»ŒNG: Re-train tá»« Ä‘áº§u trÃªn toÃ n bá»™ dá»¯ liá»‡u tÃ­ch lÅ©y
        # Theo nghiÃªn cá»©u: Huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u giÃºp model há»c láº¡i phÃ¢n phá»‘i tá»•ng thá»ƒ,
        # giáº£m thiá»ƒu viá»‡c bá»‹ lá»‡ch theo phÃ¢n phá»‘i cá»§a lÃ´ dá»¯ liá»‡u má»›i nháº¥t
        print(f"   ğŸ”„ Re-training model vá»›i {labeled_X.shape[0]:,} labeled samples...")
        
        if attacker_type == "lgb":
            attacker = LGBAttacker(seed=seed)
            attacker.train_model(labeled_X, labeled_y, X_val, y_val, boosting_rounds=1000, early_stopping=60)
        elif attacker_type == "dual":
            attacker = KerasDualAttacker(early_stopping=30, seed=seed)
            # DualDNN train vá»›i (X, y, y_true) - y_true lÃ  oracle labels
            attacker.train_model(labeled_X, labeled_y, labeled_y, X_val_s, y_val, y_val, num_epochs=num_epochs)
        else:
            attacker = KerasAttacker(early_stopping=30, seed=seed, input_shape=(feature_dim,))
            attacker.train_model(labeled_X, labeled_y, X_val_s, y_val, num_epochs=num_epochs)

        evaluate(attacker, round_id=query_round, total_labels=labeled_X.shape[0])
    
    # Kiá»ƒm tra tá»•ng queries cuá»‘i cÃ¹ng
    final_total_queries = total_queries_accumulated
    diff = final_total_queries - total_queries_target
    diff_percent = (diff / total_queries_target * 100) if total_queries_target > 0 else 0
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Tá»”NG Káº¾T QUERIES:")
    print(f"{'='*80}")
    print(f"   Queries dá»± kiáº¿n: {total_queries_target:,} ({query_batch:,} queries/round Ã— {num_rounds} rounds)")
    print(f"   Queries thá»±c táº¿: {final_total_queries:,}")
    print(f"   ChÃªnh lá»‡ch: {diff:+,} queries ({diff_percent:+.2f}%)")
    print(f"   NgÆ°á»¡ng cháº¥p nháº­n: {min_queries_acceptable:,} - {max_queries_acceptable:,} (90% - 110%)")
    
    if final_total_queries == total_queries_target:
        print(f"   âœ… Sá»‘ queries chÃ­nh xÃ¡c 100%!")
    elif final_total_queries >= min_queries_acceptable and final_total_queries <= max_queries_acceptable:
        if diff > 0:
            print(f"   âœ… Sá»‘ queries trong ngÆ°á»¡ng cháº¥p nháº­n (dÆ° {abs(diff):,} queries)")
        else:
            print(f"   âš ï¸  Sá»‘ queries trong ngÆ°á»¡ng cháº¥p nháº­n nhÆ°ng thiáº¿u {abs(diff):,} queries ({abs(diff_percent):.2f}%)")
    elif final_total_queries < min_queries_acceptable:
        print(f"   âŒ Lá»–I NGHIÃŠM TRá»ŒNG: Sá» QUERIES THIáº¾U QUÃ NHIá»€U! ({abs(diff_percent):.2f}% thiáº¿u)")
        print(f"   âŒ Thiáº¿u {abs(diff):,} queries! Äiá»u nÃ y sáº½ áº£nh hÆ°á»Ÿng NGHIÃŠM TRá»ŒNG Ä‘áº¿n hiá»‡u suáº¥t táº¥n cÃ´ng!")
        print(f"   ğŸ’¡ LÃ½ do cÃ³ thá»ƒ: Pool Ä‘Ã£ cáº¡n kiá»‡t trÆ°á»›c khi Ä‘áº¡t Ä‘á»§ queries")
        print(f"   âš ï¸  Cáº§n kiá»ƒm tra láº¡i:")
        print(f"      - Pool size ban Ä‘áº§u cÃ³ Ä‘á»§ khÃ´ng? (cáº§n Ã­t nháº¥t {total_queries_target:,} vá»›i buffer 20%)")
        print(f"      - Logic bá»• sung queries cÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng khÃ´ng?")
        print(f"      - CÃ³ thá»ƒ cáº§n tÄƒng pool size hoáº·c Ä‘iá»u chá»‰nh query_batch/num_rounds")
        # KHÃ”NG raise error vÃ¬ váº«n muá»‘n cÃ³ káº¿t quáº£, nhÆ°ng cáº£nh bÃ¡o rÃµ rÃ ng
    else:
        print(f"   âš ï¸  Sá»‘ queries vÆ°á»£t quÃ¡ 10% (dÆ° {diff:,} queries, {diff_percent:.2f}%)")
    print(f"{'='*80}\n")

    output_dir.mkdir(parents=True, exist_ok=True)
    surrogate_path = output_dir / "surrogate_model"
    attacker.save_model(str(surrogate_path))
    
    # Láº¥y extension phÃ¹ há»£p vá»›i model type
    if attacker_type == "lgb":
        surrogate_model_path = f"{surrogate_path}.txt"
    else:
        # Keras vÃ  dualDNN Ä‘á»u dÃ¹ng .h5
        surrogate_model_path = f"{surrogate_path}.h5"

    joblib_path = output_dir / "robust_scaler.joblib"
    try:
        if scaler is not None:
            import joblib
            joblib.dump(scaler, joblib_path)
        else:
            joblib_path = None
    except Exception:
        joblib_path = None

    df_metrics = pd.DataFrame(metrics_history)
    metrics_csv = output_dir / "extraction_metrics.csv"
    df_metrics.to_csv(metrics_csv, index=False)

    summary = {
        "weights_path": weights_path_abs,  # LÆ°u absolute path Ä‘á»ƒ Ä‘áº£m báº£o consistency
        "model_file_name": model_file_name,  # LÆ°u tÃªn file Ä‘á»ƒ dá»… verify
        "model_type": model_type,
        "normalization_stats_path": normalization_stats_path,
        "attacker_type": attacker_type,
        "surrogate_model_path": surrogate_model_path,
        "scaler_path": str(joblib_path) if joblib_path else None,
        "metrics_csv": str(metrics_csv),
        "metrics": metrics_history,
    }

    summary_path = output_dir / "extraction_summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    return summary


if __name__ == "__main__":
    SUMMARY = run_extraction(
        weights_path=str(PROJECT_ROOT / "src" / "final_model.h5"),
        output_dir=PROJECT_ROOT / "src" / "output",
        seed=42,
    )
    print(json.dumps(SUMMARY["metrics"][-1], indent=2))

