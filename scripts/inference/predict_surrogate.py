#!/usr/bin/env python3
"""
Script Ä‘á»ƒ truy váº¥n cÃ¡c surrogate model dualffnn Ä‘Ã£ Ä‘Æ°á»£c extract.

Usage:
    python scripts/inference/predict_surrogate.py \
        --model output/DUALFFNN-ember-dualDNN-5000/surrogate_model.h5 \
        --output output/predictions/surrogate_dualffnn_5000 \
        --input data/test_samples.csv
"""

import os
import sys
import argparse
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Import ML frameworks
try:
    import tensorflow as tf
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
except ImportError:
    tf = None

try:
    from sklearn.preprocessing import RobustScaler
    import joblib
except ImportError:
    RobustScaler = None
    joblib = None

CLASS_NAMES = ["Benign", "Malware"]


# ========================================
# MODEL LOADERS
# ========================================

def load_surrogate_model(model_path):
    """Load surrogate model tá»« file .h5 (TensorFlow/Keras)"""
    if tf is None:
        raise ImportError("TensorFlow chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. CÃ i Ä‘áº·t báº±ng: pip install tensorflow")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y surrogate model: {model_path}")
    
    # Xá»­ lÃ½ lá»—i version incompatibility vá»›i batch_shape
    # Thá»­ nhiá»u cÃ¡ch load khÃ¡c nhau
    import warnings
    
    # CÃ¡ch 1: Load bÃ¬nh thÆ°á»ng
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            try:
                model = tf.keras.models.load_model(model_path, compile=False, safe_mode=False)
                return model
            except TypeError:
                model = tf.keras.models.load_model(model_path, compile=False)
                return model
    except Exception as e:
        error_str = str(e)
        # CÃ¡ch 2: Load vá»›i custom_objects rá»—ng vÃ  bá» qua warnings
        if 'batch_shape' in error_str or 'Unrecognized keyword' in error_str or 'rms_scaling' in error_str:
            print("âš ï¸  PhÃ¡t hiá»‡n lá»—i tÆ°Æ¡ng thÃ­ch, Ä‘ang thá»­ load vá»›i custom_objects rá»—ng...")
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    try:
                        model = tf.keras.models.load_model(model_path, compile=False, custom_objects={}, safe_mode=False)
                        return model
                    except TypeError:
                        model = tf.keras.models.load_model(model_path, compile=False, custom_objects={})
                        return model
            except Exception as e2:
                # CÃ¡ch 3: Thá»­ load vá»›i custom_objects cÃ³ DTypePolicy
                print("âš ï¸  Äang thá»­ load vá»›i DTypePolicy custom_objects...")
                try:
                    from tensorflow.keras import mixed_precision
                    custom_objects = {
                        'DTypePolicy': mixed_precision.Policy,
                    }
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        try:
                            model = tf.keras.models.load_model(model_path, compile=False, custom_objects=custom_objects, safe_mode=False)
                            return model
                        except TypeError:
                            model = tf.keras.models.load_model(model_path, compile=False, custom_objects=custom_objects)
                            return model
                except Exception as e3:
                    # CÃ¡ch 4: Náº¿u váº«n lá»—i, thá»­ sá»­a file HDF5 táº¡m thá»i
                    print("âš ï¸  Äang thá»­ sá»­a file HDF5 Ä‘á»ƒ bá» qua cÃ¡c keyword khÃ´ng há»£p lá»‡...")
                    try:
                        import h5py
                        import tempfile
                        import shutil
                        import json
                        
                        # Táº¡o file táº¡m
                        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as tmp_file:
                            tmp_path = tmp_file.name
                        
                        # Copy file
                        shutil.copy2(model_path, tmp_path)
                        
                        # Äá»c vÃ  sá»­a file HDF5 Ä‘á»ƒ loáº¡i bá» batch_shape
                        with h5py.File(tmp_path, 'r+') as f:
                            # Sá»­a model_config (JSON string chá»©a config cá»§a model)
                            if 'model_config' in f.attrs:
                                try:
                                    model_config_str = f.attrs['model_config']
                                    if isinstance(model_config_str, bytes):
                                        model_config_str = model_config_str.decode('utf-8')
                                    
                                    model_config = json.loads(model_config_str)
                                    
                                    # Äá»‡ quy xÃ³a cÃ¡c keyword khÃ´ng há»£p lá»‡ trong config
                                    def remove_invalid_keywords(obj):
                                        if isinstance(obj, dict):
                                            # Loáº¡i bá» cÃ¡c keyword khÃ´ng há»£p lá»‡
                                            obj.pop('batch_shape', None)
                                            obj.pop('batch_input_shape', None)
                                            obj.pop('rms_scaling', None)
                                            # Xá»­ lÃ½ cÃ¡c key khÃ¡c
                                            for key, value in list(obj.items()):
                                                if isinstance(value, dict):
                                                    remove_invalid_keywords(value)
                                                elif isinstance(value, list):
                                                    for item in value:
                                                        remove_invalid_keywords(item)
                                        elif isinstance(obj, list):
                                            for item in obj:
                                                remove_invalid_keywords(item)
                                    
                                    remove_invalid_keywords(model_config)
                                    
                                    # Cáº­p nháº­t láº¡i model_config
                                    f.attrs['model_config'] = json.dumps(model_config).encode('utf-8')
                                except Exception as config_error:
                                    print(f"âš ï¸  KhÃ´ng thá»ƒ sá»­a model_config: {config_error}")
                        
                        # Load tá»« file Ä‘Ã£ sá»­a vá»›i custom_objects
                        from tensorflow.keras import mixed_precision
                        custom_objects = {
                            'DTypePolicy': mixed_precision.Policy,
                        }
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            try:
                                model = tf.keras.models.load_model(tmp_path, compile=False, custom_objects=custom_objects, safe_mode=False)
                            except TypeError:
                                model = tf.keras.models.load_model(tmp_path, compile=False, custom_objects=custom_objects)
                        os.unlink(tmp_path)  # XÃ³a file táº¡m
                        return model
                    except Exception as e4:
                        # Náº¿u váº«n lá»—i, raise error vá»›i táº¥t cáº£ cÃ¡c lá»—i
                        raise RuntimeError(
                            f"KhÃ´ng thá»ƒ load surrogate model tá»« {model_path}.\n"
                            f"Lá»—i gá»‘c: {e}\n"
                            f"Lá»—i khi load vá»›i custom_objects rá»—ng: {e2}\n"
                            f"Lá»—i khi load vá»›i DTypePolicy: {e3}\n"
                            f"Lá»—i khi sá»­a file: {e4}\n"
                            f"CÃ³ thá»ƒ do version incompatibility cá»§a TensorFlow.\n"
                            f"Thá»­ cÃ i Ä‘áº·t TensorFlow version tÆ°Æ¡ng thÃ­ch hoáº·c rebuild model."
                        )
        else:
            raise


def load_robust_scaler(scaler_path):
    """Load RobustScaler tá»« file .joblib"""
    if joblib is None:
        raise ImportError("joblib chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. CÃ i Ä‘áº·t báº±ng: pip install joblib")
    
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y robust scaler: {scaler_path}")
    
    scaler = joblib.load(scaler_path)
    return scaler


# ========================================
# DATA PROCESSING
# ========================================

def load_csv_data(csv_path):
    """Load dá»¯ liá»‡u tá»« CSV file"""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file CSV: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    if df.empty:
        raise ValueError("File CSV rá»—ng!")
    
    return df


def clip_scale(scaler, X):
    """Scale data vá»›i RobustScaler vÃ  clip vá» [-5, 5]"""
    if scaler is None:
        raise ValueError("Scaler khÃ´ng Ä‘Æ°á»£c khá»Ÿi táº¡o")
    
    transformed = scaler.transform(X)
    return np.clip(transformed, -5, 5)


# ========================================
# PREDICTION FUNCTIONS
# ========================================

def predict_surrogate(model, X):
    """Predict vá»›i surrogate model (Keras)"""
    # Kiá»ƒm tra xem model cÃ³ multiple inputs khÃ´ng
    input_shape = model.input_shape
    has_multiple_inputs = isinstance(input_shape, list) and len(input_shape) > 1
    
    if has_multiple_inputs:
        # Model cÃ³ multiple inputs - cáº§n táº¡o input thá»© 2
        # ThÆ°á»ng input thá»© 2 lÃ  má»™t giÃ¡ trá»‹ constant hoáº·c metadata
        # Táº¡m thá»i táº¡o input thá»© 2 vá»›i shape (n_samples, 1) filled vá»›i 0
        X_input2 = np.zeros((X.shape[0], 1), dtype=np.float32)
        # Predict vá»›i multiple inputs
        probs_raw = model.predict([X, X_input2], verbose=0)
    else:
        # Single input
        probs_raw = model.predict(X, verbose=0)
    
    # Xá»­ lÃ½ output shape
    if len(probs_raw.shape) == 1:
        # Shape (n_samples,) - binary output
        probs = probs_raw
    elif probs_raw.shape[1] == 1:
        # Shape (n_samples, 1) - single output
        probs = probs_raw[:, 0]
    elif probs_raw.shape[1] == 2:
        # Shape (n_samples, 2) - two outputs, láº¥y class 1 (Malware)
        probs = probs_raw[:, 1]
    else:
        # Fallback: láº¥y output Ä‘áº§u tiÃªn
        probs = probs_raw[:, 0]
    
    # Äáº£m báº£o probs trong khoáº£ng [0, 1]
    probs = np.clip(probs, 0, 1)
    
    # Predictions
    predictions = (probs >= 0.5).astype(int)
    
    return predictions, probs


# ========================================
# OUTPUT FUNCTIONS
# ========================================

def save_predictions_csv(df, predictions, probs, output_path):
    """LÆ°u predictions vÃ o CSV file"""
    # Táº¡o báº£n sao cá»§a dataframe
    df_output = df.copy()
    
    # ThÃªm cá»™t predictions
    df_output['prediction'] = predictions
    df_output['prediction_label'] = df_output['prediction'].map({0: 'Benign', 1: 'Malware'})
    df_output['prediction_prob'] = probs
    
    # LÆ°u file
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_output.to_csv(output_path, index=False)
    
    print(f"âœ… ÄÃ£ lÆ°u predictions vÃ o: {output_path}")
    return df_output


def generate_report(predictions, probs, output_path, model_name, num_samples):
    """Táº¡o report markdown"""
    # TÃ­nh toÃ¡n thá»‘ng kÃª
    benign_count = int(np.sum(predictions == 0))
    malware_count = int(np.sum(predictions == 1))
    benign_pct = (benign_count / num_samples * 100) if num_samples > 0 else 0
    malware_pct = (malware_count / num_samples * 100) if num_samples > 0 else 0
    
    avg_prob = float(np.mean(probs))
    
    # Táº¡o ná»™i dung report
    report = f"""# Surrogate Model Prediction Report

## ThÃ´ng tin mÃ´ hÃ¬nh
- **MÃ´ hÃ¬nh**: {model_name}
- **Thá»i gian**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Thá»‘ng kÃª predictions
- **Tá»•ng sá»‘ samples**: {num_samples:,}
- **Benign**: {benign_count:,} ({benign_pct:.2f}%)
- **Malware**: {malware_count:,} ({malware_pct:.2f}%)

## Thá»‘ng kÃª probabilities
- **Trung bÃ¬nh probability**: {avg_prob:.4f}
- **Min probability**: {float(np.min(probs)):.4f}
- **Max probability**: {float(np.max(probs)):.4f}
- **Std probability**: {float(np.std(probs)):.4f}

## PhÃ¢n bá»‘ predictions
```
Benign:  {'â–ˆ' * (benign_count // max(1, num_samples // 50))}
Malware: {'â–ˆ' * (malware_count // max(1, num_samples // 50))}
```
"""
    
    # LÆ°u file
    report_path = os.path.join(output_path, 'report.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… ÄÃ£ táº¡o report táº¡i: {report_path}")
    return report_path


# ========================================
# MAIN FUNCTION
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='Truy váº¥n cÃ¡c surrogate model dualffnn Ä‘Ã£ Ä‘Æ°á»£c extract',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
VÃ­ dá»¥:
  python scripts/inference/predict_surrogate.py \\
      --model output/DUALFFNN-ember-dualDNN-5000/surrogate_model.h5 \\
      --output output/predictions/surrogate_dualffnn_5000 \\
      --input data/test_samples.csv
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='ÄÆ°á»ng dáº«n Ä‘áº¿n surrogate model (.h5 file)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='ÄÆ°á»ng dáº«n Ä‘áº¿n folder chá»©a file CSV Ä‘áº§u ra vÃ  report markdown'
    )
    
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV Ä‘áº§u vÃ o (chá»‰ cÃ³ features)'
    )
    
    args = parser.parse_args()
    
    # Validate paths
    if not os.path.exists(args.model):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y surrogate model: {args.model}")
        sys.exit(1)
    
    if not os.path.exists(args.input):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file CSV Ä‘áº§u vÃ o: {args.input}")
        sys.exit(1)
    
    # Táº¡o output directory náº¿u chÆ°a cÃ³
    os.makedirs(args.output, exist_ok=True)
    
    # TÃ¬m robust scaler trong cÃ¹ng folder vá»›i model
    model_dir = os.path.dirname(args.model)
    scaler_path = os.path.join(model_dir, 'robust_scaler.joblib')
    
    print("=" * 60)
    print(f"ğŸ” Truy váº¥n Surrogate Model")
    print("=" * 60)
    print(f"ğŸ“ Model: {args.model}")
    print(f"ğŸ“ Scaler: {scaler_path}")
    print(f"ğŸ“ Input: {args.input}")
    print(f"ğŸ“ Output: {args.output}")
    
    # Load robust scaler
    print(f"\nğŸ“Š Äang load robust scaler...")
    try:
        scaler = load_robust_scaler(scaler_path)
        print(f"âœ… ÄÃ£ load robust scaler: {scaler_path}")
    except Exception as e:
        print(f"âŒ Lá»—i khi load robust scaler: {e}")
        sys.exit(1)
    
    # Load surrogate model
    print(f"\nğŸ¤– Äang load surrogate model: {args.model}")
    try:
        model = load_surrogate_model(args.model)
        print(f"âœ… ÄÃ£ load surrogate model thÃ nh cÃ´ng")
        print(f"   Input shape: {model.input_shape}")
        print(f"   Output shape: {model.output_shape}")
    except Exception as e:
        print(f"âŒ Lá»—i khi load surrogate model: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Load CSV data
    print(f"\nğŸ“‚ Äang load dá»¯ liá»‡u tá»« CSV: {args.input}")
    try:
        df = load_csv_data(args.input)
        print(f"âœ… ÄÃ£ load {len(df):,} samples, {len(df.columns)} columns")
    except Exception as e:
        print(f"âŒ Lá»—i khi load CSV: {e}")
        sys.exit(1)
    
    # Extract features (loáº¡i bá» cÃ¡c cá»™t khÃ´ng pháº£i feature)
    # Loáº¡i bá» cÃ¡c cá»™t phá»• biáº¿n khÃ´ng pháº£i feature: label, filename, id, hash, etc.
    exclude_cols = ['Label', 'label', 'target', 'filename', 'file_name', 'id', 'ID', 
                    'hash', 'Hash', 'sha256', 'SHA256', 'md5', 'MD5', 'sha1', 'SHA1']
    
    # Lá»c cÃ¡c cá»™t cÃ³ thá»ƒ convert sang sá»‘
    feature_cols = []
    for col in df.columns:
        if col in exclude_cols:
            continue
        # Thá»­ convert má»™t sample Ä‘á»ƒ kiá»ƒm tra xem cÃ³ pháº£i sá»‘ khÃ´ng
        try:
            pd.to_numeric(df[col].iloc[0], errors='raise')
            feature_cols.append(col)
        except (ValueError, TypeError, IndexError):
            # KhÃ´ng pháº£i sá»‘, bá» qua
            print(f"âš ï¸  Bá» qua cá»™t khÃ´ng pháº£i sá»‘: {col}")
            continue
    
    if len(feature_cols) == 0:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y cá»™t feature nÃ o trong CSV! Vui lÃ²ng kiá»ƒm tra láº¡i file CSV.")
    
    print(f"ğŸ“Š Sá»­ dá»¥ng {len(feature_cols)} cá»™t feature (Ä‘Ã£ loáº¡i bá» {len(df.columns) - len(feature_cols)} cá»™t khÃ´ng pháº£i sá»‘)")
    
    # Convert sang float, xá»­ lÃ½ lá»—i náº¿u cÃ³
    X = df[feature_cols].copy()
    for col in feature_cols:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Fill NaN báº±ng 0 (náº¿u cÃ³)
    X = X.fillna(0)
    X = X.values.astype(np.float32)
    
    # Kiá»ƒm tra sá»‘ lÆ°á»£ng features vá»›i model input
    # model.input_shape cÃ³ thá»ƒ lÃ  tuple, list, hoáº·c list of tuples (multiple inputs)
    input_shape = model.input_shape
    
    # Xá»­ lÃ½ trÆ°á»ng há»£p multiple inputs
    if isinstance(input_shape, list) and len(input_shape) > 0:
        # Náº¿u lÃ  list, láº¥y input Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  main input)
        first_input = input_shape[0]
        if isinstance(first_input, (list, tuple)):
            # Láº¥y sá»‘ features tá»« input shape Ä‘áº§u tiÃªn
            if len(first_input) > 1:
                expected_features = first_input[1] if first_input[1] is not None else first_input[-1]
            else:
                expected_features = first_input[0] if first_input[0] is not None else X.shape[1]
        else:
            # Náº¿u khÃ´ng pháº£i tuple/list, thá»­ láº¥y trá»±c tiáº¿p
            expected_features = first_input if isinstance(first_input, int) else X.shape[1]
    elif isinstance(input_shape, (list, tuple)) and len(input_shape) > 1:
        # Single input vá»›i shape lÃ  tuple/list
        expected_features = input_shape[1] if input_shape[1] is not None else input_shape[-1]
    elif isinstance(input_shape, (list, tuple)) and len(input_shape) == 1:
        expected_features = input_shape[0] if input_shape[0] is not None else X.shape[1]
    else:
        # Fallback: thá»­ láº¥y tá»« model.input
        try:
            if hasattr(model, 'input') and model.input is not None:
                if isinstance(model.input, list):
                    # Multiple inputs
                    expected_features = model.input[0].shape[-1] if model.input[0].shape[-1] is not None else X.shape[1]
                else:
                    expected_features = model.input.shape[-1] if model.input.shape[-1] is not None else X.shape[1]
            else:
                expected_features = X.shape[1]  # Sá»­ dá»¥ng sá»‘ features hiá»‡n táº¡i
        except:
            expected_features = X.shape[1]  # Sá»­ dá»¥ng sá»‘ features hiá»‡n táº¡i
    
    # Äáº£m báº£o expected_features lÃ  sá»‘ nguyÃªn
    if not isinstance(expected_features, (int, np.integer)):
        print(f"âš ï¸  KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ features tá»« model input shape: {input_shape}")
        print(f"   Sáº½ sá»­ dá»¥ng sá»‘ features tá»« CSV: {X.shape[1]}")
        expected_features = X.shape[1]
    
    actual_features = X.shape[1]
    
    if actual_features != expected_features:
        print(f"âš ï¸  Cáº£nh bÃ¡o: Sá»‘ lÆ°á»£ng features khÃ´ng khá»›p!")
        print(f"   CSV cÃ³ {actual_features} features, model mong Ä‘á»£i {expected_features} features")
        
        if actual_features < expected_features:
            # Pad vá»›i zeros
            print(f"âš ï¸  CSV cÃ³ Ã­t features hÆ¡n. Sáº½ pad thÃªm {expected_features - actual_features} features báº±ng 0.")
            X_padded = np.zeros((X.shape[0], expected_features), dtype=np.float32)
            X_padded[:, :actual_features] = X
            X = X_padded
        else:
            print(f"âš ï¸  CSV cÃ³ nhiá»u features hÆ¡n. Sáº½ chá»‰ sá»­ dá»¥ng {expected_features} features Ä‘áº§u tiÃªn.")
            X = X[:, :expected_features]
    
    # Scale vÃ  clip data
    print("\nğŸ”„ Äang scale vÃ  clip dá»¯ liá»‡u...")
    try:
        X_scaled = clip_scale(scaler, X)
        print("âœ… ÄÃ£ scale vÃ  clip xong")
    except Exception as e:
        print(f"âŒ Lá»—i khi scale data: {e}")
        sys.exit(1)
    
    # Predict
    print(f"\nğŸ”® Äang thá»±c hiá»‡n prediction...")
    try:
        predictions, probs = predict_surrogate(model, X_scaled)
        print(f"âœ… ÄÃ£ hoÃ n táº¥t prediction cho {len(predictions):,} samples")
    except Exception as e:
        print(f"âŒ Lá»—i khi predict: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # LÆ°u káº¿t quáº£
    print(f"\nğŸ’¾ Äang lÆ°u káº¿t quáº£...")
    model_name = os.path.basename(os.path.dirname(args.model))
    output_csv_path = os.path.join(args.output, f'predictions_{model_name}.csv')
    df_output = save_predictions_csv(df, predictions, probs, output_csv_path)
    
    # Táº¡o report
    generate_report(predictions, probs, args.output, model_name, len(predictions))
    
    print("\n" + "=" * 60)
    print("âœ… HoÃ n táº¥t!")
    print("=" * 60)
    print(f"ğŸ“ CSV output: {output_csv_path}")
    print(f"ğŸ“„ Report: {os.path.join(args.output, 'report.md')}")


if __name__ == '__main__':
    main()

