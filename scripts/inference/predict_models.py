#!/usr/bin/env python3
"""
Script ƒë·ªÉ truy v·∫•n c√°c m√¥ h√¨nh ML ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán.

Usage:
    python scripts/inference/predict_models.py \
        --input data/test_samples.csv \
        --output output/predictions/ \
        --model lightgbm
"""

import os
import sys
import argparse
import json
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Import ML frameworks
try:
    import lightgbm as lgb
except ImportError:
    lgb = None

try:
    import xgboost as xgb
except ImportError:
    xgb = None

try:
    import tensorflow as tf
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
except ImportError:
    tf = None

try:
    import torch
    import torch.nn as nn
except ImportError:
    torch = None

try:
    from pytorch_tabnet.tab_model import TabNetClassifier
except ImportError:
    TabNetClassifier = None

# ========================================
# CONFIGURATION
# ========================================

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a models
ARTIFACTS_DIR = Path(__file__).parent.parent.parent / "artifacts" / "targets"

# Mapping c√°c m√¥ h√¨nh
MODEL_CONFIGS = {
    "lightgbm": {
        "model_file": "LEE.lgb",
        "stats_file": "LEE_normalization_stats.npz",
    },
    "cnn": {
        "model_file": "CNN.h5",  # S·ª≠ d·ª•ng CNN.h5 thay v√¨ CEE.h5
        "stats_file": "CNN_normalization_stats.npz",
    },
    "xgboost": {
        "model_file": "xgboost_ember.json",
        "stats_file": "xgboost_normalization_stats.npz",
    },
    "tabnet": {
        "model_file": "tabnet_ember.zip",
        "stats_file": "tabnet_normalization_stats.npz",
    },
    "dualffnn": {
        "model_file": "dualffnn_ember.pt",
        "stats_file": "dualffnn_normalization_stats.npz",
    }
}

CLASS_NAMES = ["Benign", "Malware"]


# ========================================
# MODEL LOADERS
# ========================================

def load_normalization_stats(stats_path):
    """Load normalization statistics t·ª´ file .npz"""
    if not os.path.exists(stats_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file normalization stats: {stats_path}")
    
    data = np.load(stats_path, allow_pickle=True)
    feature_means = data['feature_means']
    feature_stds = data['feature_stds']
    
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p std = 0
    feature_stds = np.where(feature_stds == 0, 1.0, feature_stds)
    
    return feature_means.astype(np.float32), feature_stds.astype(np.float32)


def load_lightgbm_model(model_path):
    """Load LightGBM model t·ª´ file .lgb"""
    if lgb is None:
        raise ImportError("LightGBM ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install lightgbm")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
    
    model = lgb.Booster(model_file=str(model_path))
    return model


def load_cnn_model(model_path):
    """Load CNN model t·ª´ file .h5 (TensorFlow/Keras)"""
    if tf is None:
        raise ImportError("TensorFlow ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install tensorflow")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
    
    # X·ª≠ l√Ω l·ªói version incompatibility v·ªõi batch_shape
    try:
        model = tf.keras.models.load_model(model_path, compile=False)
        return model
    except (TypeError, ValueError) as e:
        error_str = str(e)
        if 'batch_shape' in error_str or 'Unrecognized keyword' in error_str:
            print("‚ö†Ô∏è  Ph√°t hi·ªán l·ªói t∆∞∆°ng th√≠ch batch_shape, ƒëang th·ª≠ load v·ªõi custom_objects...")
            try:
                # Th·ª≠ load v·ªõi custom_objects ƒë·ªÉ b·ªè qua DTypePolicy
                from tensorflow.keras import mixed_precision
                custom_objects = {
                    'DTypePolicy': mixed_precision.Policy,
                }
                model = tf.keras.models.load_model(
                    model_path,
                    compile=False,
                    custom_objects=custom_objects
                )
                return model
            except Exception as e2:
                # N·∫øu v·∫´n l·ªói, th·ª≠ s·ª≠a file HDF5 t·∫°m th·ªùi
                print("‚ö†Ô∏è  ƒêang th·ª≠ s·ª≠a file HDF5 ƒë·ªÉ b·ªè qua batch_shape...")
                try:
                    import h5py
                    import tempfile
                    import shutil
                    import json
                    
                    # T·∫°o file t·∫°m
                    with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as tmp_file:
                        tmp_path = tmp_file.name
                    
                    # Copy file
                    shutil.copy2(model_path, tmp_path)
                    
                    # ƒê·ªçc v√† s·ª≠a file HDF5 ƒë·ªÉ lo·∫°i b·ªè batch_shape
                    with h5py.File(tmp_path, 'r+') as f:
                        # S·ª≠a model_config (JSON string ch·ª©a config c·ªßa model)
                        if 'model_config' in f.attrs:
                            try:
                                model_config_str = f.attrs['model_config']
                                if isinstance(model_config_str, bytes):
                                    model_config_str = model_config_str.decode('utf-8')
                                
                                model_config = json.loads(model_config_str)
                                
                                # ƒê·ªá quy x√≥a batch_shape trong config
                                def remove_batch_shape(obj):
                                    if isinstance(obj, dict):
                                        obj.pop('batch_shape', None)
                                        obj.pop('batch_input_shape', None)
                                        for key, value in obj.items():
                                            remove_batch_shape(value)
                                    elif isinstance(obj, list):
                                        for item in obj:
                                            remove_batch_shape(item)
                                
                                remove_batch_shape(model_config)
                                
                                # C·∫≠p nh·∫≠t l·∫°i model_config
                                f.attrs['model_config'] = json.dumps(model_config).encode('utf-8')
                            except Exception as config_error:
                                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ s·ª≠a model_config: {config_error}")
                    
                    # Load t·ª´ file ƒë√£ s·ª≠a v·ªõi custom_objects
                    from tensorflow.keras import mixed_precision
                    custom_objects = {
                        'DTypePolicy': mixed_precision.Policy,
                    }
                    model = tf.keras.models.load_model(tmp_path, compile=False, custom_objects=custom_objects)
                    os.unlink(tmp_path)  # X√≥a file t·∫°m
                    return model
                except Exception as e3:
                    raise RuntimeError(
                        f"Kh√¥ng th·ªÉ load CNN model t·ª´ {model_path}.\n"
                        f"L·ªói g·ªëc: {e}\n"
                        f"L·ªói khi load v·ªõi custom_objects: {e2}\n"
                        f"L·ªói khi s·ª≠a file: {e3}\n"
                        f"C√≥ th·ªÉ do version incompatibility c·ªßa TensorFlow."
                    )
        else:
            raise


def load_xgboost_model(model_path):
    """Load XGBoost model t·ª´ file .json"""
    if xgb is None:
        raise ImportError("XGBoost ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install xgboost")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
    
    model = xgb.Booster()
    model.load_model(str(model_path))
    return model


def load_tabnet_model(model_path):
    """Load TabNet model t·ª´ file .zip"""
    if TabNetClassifier is None:
        raise ImportError("pytorch-tabnet ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install pytorch-tabnet")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
    
    # TabNet load t·ª´ zip file
    model = TabNetClassifier()
    model.load_model(str(model_path))
    return model


def load_dualffnn_model(model_path, stats_path):
    """Load dualffnn model t·ª´ file .pt (PyTorch)"""
    if torch is None:
        raise ImportError("PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install torch")
    
    # Load normalization stats ƒë·ªÉ l·∫•y th√¥ng tin v·ªÅ input dimensions
    stats_data = np.load(stats_path, allow_pickle=True)
    in_dim_1 = int(stats_data['in_dim_1'])
    in_dim_2 = int(stats_data['in_dim_2'])
    
    # ƒê·ªãnh nghƒ©a l·∫°i architecture (t·ª´ notebook) - C·∫¶N ƒê·ªäNH NGHƒ®A TR∆Ø·ªöC KHI LOAD
    class DualFFNN(nn.Module):
        def __init__(self, in_dim_1, in_dim_2, hidden1_branch=256, hidden2_branch=128,
                     hidden_joint=256, out_dim=2, dropout_p=0.2):
            super().__init__()
            self.in_dim_1 = in_dim_1
            self.in_dim_2 = in_dim_2
            
            self.branch1 = nn.Sequential(
                nn.Linear(in_dim_1, hidden1_branch),
                nn.ReLU(),
                nn.Dropout(dropout_p),
                nn.Linear(hidden1_branch, hidden2_branch),
                nn.ReLU(),
            )
            
            self.branch2 = nn.Sequential(
                nn.Linear(in_dim_2, hidden1_branch),
                nn.ReLU(),
                nn.Dropout(dropout_p),
                nn.Linear(hidden1_branch, hidden2_branch),
                nn.ReLU(),
            )
            
            self.joint = nn.Sequential(
                nn.Linear(hidden2_branch * 2, hidden_joint),
                nn.ReLU(),
                nn.Dropout(dropout_p),
                nn.Linear(hidden_joint, out_dim),
            )
        
        def forward(self, x):
            x1 = x[:, :self.in_dim_1]
            x2 = x[:, self.in_dim_1:self.in_dim_1 + self.in_dim_2]
            z1 = self.branch1(x1)
            z2 = self.branch2(x2)
            z = torch.cat([z1, z2], dim=1)
            out = self.joint(z)
            return out
    
    # Ki·ªÉm tra xem c√≥ full model kh√¥ng (∆∞u ti√™n)
    full_model_path = str(model_path).replace('.pt', '_full.pt')
    if os.path.exists(full_model_path):
        # Th·ª≠ load full model (d·ªÖ nh·∫•t)
        # PyTorch 2.6+ y√™u c·∫ßu weights_only=False ƒë·ªÉ load custom classes
        try:
            model = torch.load(full_model_path, map_location='cpu', weights_only=False)
            model.eval()
            return model
        except (AttributeError, RuntimeError) as e:
            # N·∫øu kh√¥ng load ƒë∆∞·ª£c full model (do class kh√¥ng match), load state_dict
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load full model, s·∫Ω load state_dict: {e}")
    
    # Load state_dict t·ª´ checkpoint
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
    
    # Load model checkpoint (state_dict)
    # PyTorch 2.6+ y√™u c·∫ßu weights_only=False ƒë·ªÉ load custom classes
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    # L·∫•y hyperparameters t·ª´ checkpoint ho·∫∑c d√πng defaults
    hidden1_branch = checkpoint.get('hidden1_branch', 256)
    hidden2_branch = checkpoint.get('hidden2_branch', 128)
    hidden_joint = checkpoint.get('hidden_joint', 256)
    dropout_p = checkpoint.get('dropout_p', 0.2)
    
    # T·∫°o model v√† load weights
    model = DualFFNN(
        in_dim_1=in_dim_1,
        in_dim_2=in_dim_2,
        hidden1_branch=hidden1_branch,
        hidden2_branch=hidden2_branch,
        hidden_joint=hidden_joint,
        dropout_p=dropout_p,
    )
    
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()
    
    return model


# ========================================
# DATA PROCESSING
# ========================================

def load_csv_data(csv_path):
    """Load d·ªØ li·ªáu t·ª´ CSV file"""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file CSV: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    if df.empty:
        raise ValueError("File CSV r·ªóng!")
    
    return df


def normalize_features(X, means, stds):
    """Normalize features s·ª≠ d·ª•ng means v√† stds"""
    X = (X - means) / stds
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    return X.astype(np.float32)


# ========================================
# PREDICTION FUNCTIONS
# ========================================

def predict_lightgbm(model, X):
    """Predict v·ªõi LightGBM model"""
    # LightGBM c√≥ th·ªÉ predict tr·ª±c ti·∫øp t·ª´ numpy array
    probs = model.predict(X, num_iteration=model.best_iteration if hasattr(model, 'best_iteration') else None)
    predictions = (probs >= 0.5).astype(int)
    return predictions, probs


def predict_cnn(model, X):
    """Predict v·ªõi CNN model (TensorFlow/Keras)"""
    # CNN ƒë∆∞·ª£c train v·ªõi data ƒë√£ clip v·ªÅ [-10, 10] sau normalize
    # C·∫ßn clip data ƒë·ªÉ ƒë·∫£m b·∫£o consistency v·ªõi training
    CLIP_VALUE = 10.0
    X_clipped = np.clip(X, -CLIP_VALUE, CLIP_VALUE)
    
    # CNN c·∫ßn reshape th√†nh (n_samples, n_features, 1)
    n_features = X_clipped.shape[1]
    X_reshaped = X_clipped.reshape((-1, n_features, 1))
    
    # Predict
    probs = model.predict(X_reshaped, verbose=0)
    # probs c√≥ shape (n_samples, 2) cho binary classification
    if probs.shape[1] == 2:
        probs = probs[:, 1]  # L·∫•y probability c·ªßa class 1 (Malware)
    predictions = (probs >= 0.5).astype(int)
    return predictions, probs


def predict_xgboost(model, X):
    """Predict v·ªõi XGBoost model"""
    # XGBoost c·∫ßn DMatrix
    dmat = xgb.DMatrix(X)
    probs = model.predict(dmat)
    predictions = (probs >= 0.5).astype(int)
    return predictions, probs


def predict_tabnet(model, X):
    """Predict v·ªõi TabNet model"""
    probs = model.predict_proba(X)[:, 1]  # Probability c·ªßa class 1
    predictions = model.predict(X).astype(int)
    return predictions, probs


def predict_dualffnn(model, X, device='cpu'):
    """Predict v·ªõi dualffnn model (PyTorch)"""
    if torch is None:
        raise ImportError("PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    
    model.to(device)
    model.eval()
    
    # Convert numpy to tensor
    X_tensor = torch.from_numpy(X).float().to(device)
    
    # Predict
    with torch.no_grad():
        logits = model(X_tensor)
        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # Probability c·ªßa class 1
        predictions = torch.argmax(logits, dim=1).cpu().numpy()
    
    return predictions, probs


# ========================================
# OUTPUT FUNCTIONS
# ========================================

def save_predictions_csv(df, predictions, probs, output_path):
    """L∆∞u predictions v√†o CSV file"""
    # T·∫°o b·∫£n sao c·ªßa dataframe
    df_output = df.copy()
    
    # Th√™m c·ªôt predictions
    df_output['prediction'] = predictions
    df_output['prediction_label'] = df_output['prediction'].map({0: 'Benign', 1: 'Malware'})
    df_output['prediction_prob'] = probs
    
    # L∆∞u file
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_output.to_csv(output_path, index=False)
    
    print(f"‚úÖ ƒê√£ l∆∞u predictions v√†o: {output_path}")
    return df_output


def generate_report(predictions, probs, output_path, model_name, num_samples):
    """T·∫°o report markdown"""
    # T√≠nh to√°n th·ªëng k√™
    benign_count = int(np.sum(predictions == 0))
    malware_count = int(np.sum(predictions == 1))
    benign_pct = (benign_count / num_samples * 100) if num_samples > 0 else 0
    malware_pct = (malware_count / num_samples * 100) if num_samples > 0 else 0
    
    avg_prob = float(np.mean(probs))
    
    # T·∫°o n·ªôi dung report
    report = f"""# Model Prediction Report

## Th√¥ng tin m√¥ h√¨nh
- **M√¥ h√¨nh**: {model_name.upper()}
- **Th·ªùi gian**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Th·ªëng k√™ predictions
- **T·ªïng s·ªë samples**: {num_samples:,}
- **Benign**: {benign_count:,} ({benign_pct:.2f}%)
- **Malware**: {malware_count:,} ({malware_pct:.2f}%)

## Th·ªëng k√™ probabilities
- **Trung b√¨nh probability**: {avg_prob:.4f}
- **Min probability**: {float(np.min(probs)):.4f}
- **Max probability**: {float(np.max(probs)):.4f}
- **Std probability**: {float(np.std(probs)):.4f}

## Ph√¢n b·ªë predictions
```
Benign:  {'‚ñà' * (benign_count // max(1, num_samples // 50))}
Malware: {'‚ñà' * (malware_count // max(1, num_samples // 50))}
```
"""
    
    # L∆∞u file
    report_path = os.path.join(output_path, 'report.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úÖ ƒê√£ t·∫°o report t·∫°i: {report_path}")
    return report_path


# ========================================
# MAIN FUNCTION
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='Truy v·∫•n c√°c m√¥ h√¨nh ML ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  python scripts/inference/predict_models.py \\
      --input data/test_samples.csv \\
      --output output/predictions/ \\
      --model lightgbm

C√°c m√¥ h√¨nh h·ªó tr·ª£: lightgbm, cnn, xgboost, tabnet, dualffnn
        """
    )
    
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ƒë·∫ßu v√†o (ch·ªâ c√≥ features)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn folder ch·ª©a file CSV ƒë·∫ßu ra v√† report markdown'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        choices=['lightgbm', 'cnn', 'xgboost', 'tabnet', 'dualffnn'],
        help='T√™n m√¥ h√¨nh c·∫ßn s·ª≠ d·ª•ng'
    )
    
    args = parser.parse_args()
    
    # Validate paths
    if not os.path.exists(args.input):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file CSV ƒë·∫ßu v√†o: {args.input}")
        sys.exit(1)
    
    # T·∫°o output directory n·∫øu ch∆∞a c√≥
    os.makedirs(args.output, exist_ok=True)
    
    # L·∫•y config cho model
    model_config = MODEL_CONFIGS[args.model]
    model_path = ARTIFACTS_DIR / model_config['model_file']
    stats_path = ARTIFACTS_DIR / model_config['stats_file']
    
    print("=" * 60)
    print(f"üîç Truy v·∫•n m√¥ h√¨nh: {args.model.upper()}")
    print("=" * 60)
    
    # Load normalization stats
    print("\nüìä ƒêang load normalization statistics...")
    try:
        feature_means, feature_stds = load_normalization_stats(str(stats_path))
        print(f"‚úÖ ƒê√£ load normalization stats: {stats_path}")
    except Exception as e:
        print(f"‚ùå L·ªói khi load normalization stats: {e}")
        sys.exit(1)
    
    # Load model
    print(f"\nü§ñ ƒêang load m√¥ h√¨nh: {model_path}")
    try:
        if args.model == 'lightgbm':
            model = load_lightgbm_model(str(model_path))
        elif args.model == 'cnn':
            model = load_cnn_model(str(model_path))
        elif args.model == 'xgboost':
            model = load_xgboost_model(str(model_path))
        elif args.model == 'tabnet':
            model = load_tabnet_model(str(model_path))
        elif args.model == 'dualffnn':
            model = load_dualffnn_model(str(model_path), str(stats_path))
        print(f"‚úÖ ƒê√£ load m√¥ h√¨nh th√†nh c√¥ng")
    except Exception as e:
        print(f"‚ùå L·ªói khi load m√¥ h√¨nh: {e}")
        sys.exit(1)
    
    # Load CSV data
    print(f"\nüìÇ ƒêang load d·ªØ li·ªáu t·ª´ CSV: {args.input}")
    try:
        df = load_csv_data(args.input)
        print(f"‚úÖ ƒê√£ load {len(df):,} samples, {len(df.columns)} features")
    except Exception as e:
        print(f"‚ùå L·ªói khi load CSV: {e}")
        sys.exit(1)
    
    # Extract features (lo·∫°i b·ªè c√°c c·ªôt kh√¥ng ph·∫£i feature)
    # Lo·∫°i b·ªè c√°c c·ªôt ph·ªï bi·∫øn kh√¥ng ph·∫£i feature: label, filename, id, hash, etc.
    exclude_cols = ['Label', 'label', 'target', 'filename', 'file_name', 'id', 'ID', 
                    'hash', 'Hash', 'sha256', 'SHA256', 'md5', 'MD5', 'sha1', 'SHA1']
    
    # L·ªçc c√°c c·ªôt c√≥ th·ªÉ convert sang s·ªë
    feature_cols = []
    for col in df.columns:
        if col in exclude_cols:
            continue
        # Th·ª≠ convert m·ªôt sample ƒë·ªÉ ki·ªÉm tra xem c√≥ ph·∫£i s·ªë kh√¥ng
        try:
            pd.to_numeric(df[col].iloc[0], errors='raise')
            feature_cols.append(col)
        except (ValueError, TypeError, IndexError):
            # Kh√¥ng ph·∫£i s·ªë, b·ªè qua
            print(f"‚ö†Ô∏è  B·ªè qua c·ªôt kh√¥ng ph·∫£i s·ªë: {col}")
            continue
    
    if len(feature_cols) == 0:
        raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt feature n√†o trong CSV! Vui l√≤ng ki·ªÉm tra l·∫°i file CSV.")
    
    print(f"üìä S·ª≠ d·ª•ng {len(feature_cols)} c·ªôt feature (ƒë√£ lo·∫°i b·ªè {len(df.columns) - len(feature_cols)} c·ªôt kh√¥ng ph·∫£i s·ªë)")
    
    # Convert sang float, x·ª≠ l√Ω l·ªói n·∫øu c√≥
    X = df[feature_cols].copy()
    for col in feature_cols:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Fill NaN b·∫±ng 0 (n·∫øu c√≥)
    X = X.fillna(0)
    X = X.values.astype(np.float32)
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng features
    expected_features = len(feature_means)
    actual_features = X.shape[1]
    
    if actual_features != expected_features:
        print(f"‚ö†Ô∏è  C·∫£nh b√°o: S·ªë l∆∞·ª£ng features kh√¥ng kh·ªõp!")
        print(f"   CSV c√≥ {actual_features} features, model mong ƒë·ª£i {expected_features} features")
        
        if actual_features < expected_features:
            print(f"‚ùå Kh√¥ng ƒë·ªß features! C·∫ßn th√™m {expected_features - actual_features} features.")
            sys.exit(1)
        else:
            print(f"‚ö†Ô∏è  CSV c√≥ nhi·ªÅu features h∆°n. S·∫Ω ch·ªâ s·ª≠ d·ª•ng {expected_features} features ƒë·∫ßu ti√™n.")
            X = X[:, :expected_features]
    
    # Normalize features
    print("\nüîÑ ƒêang normalize features...")
    X_normalized = normalize_features(X, feature_means, feature_stds)
    print("‚úÖ ƒê√£ normalize xong")
    
    # Predict
    print(f"\nüîÆ ƒêang th·ª±c hi·ªán prediction...")
    try:
        if args.model == 'lightgbm':
            predictions, probs = predict_lightgbm(model, X_normalized)
        elif args.model == 'cnn':
            predictions, probs = predict_cnn(model, X_normalized)
        elif args.model == 'xgboost':
            predictions, probs = predict_xgboost(model, X_normalized)
        elif args.model == 'tabnet':
            predictions, probs = predict_tabnet(model, X_normalized)
        elif args.model == 'dualffnn':
            device = 'cuda' if torch and torch.cuda.is_available() else 'cpu'
            predictions, probs = predict_dualffnn(model, X_normalized, device)
        
        print(f"‚úÖ ƒê√£ ho√†n t·∫•t prediction cho {len(predictions):,} samples")
    except Exception as e:
        print(f"‚ùå L·ªói khi predict: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # L∆∞u k·∫øt qu·∫£
    print(f"\nüíæ ƒêang l∆∞u k·∫øt qu·∫£...")
    output_csv_path = os.path.join(args.output, f'predictions_{args.model}.csv')
    df_output = save_predictions_csv(df, predictions, probs, output_csv_path)
    
    # T·∫°o report
    generate_report(predictions, probs, args.output, args.model, len(predictions))
    
    print("\n" + "=" * 60)
    print("‚úÖ Ho√†n t·∫•t!")
    print("=" * 60)
    print(f"üìÅ CSV output: {output_csv_path}")
    print(f"üìÑ Report: {os.path.join(args.output, 'report.md')}")


if __name__ == '__main__':
    main()

