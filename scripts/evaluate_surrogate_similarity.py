"""
Script ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target model v√† c√°c surrogate models
s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ train_ember_2018_v2_features_label_minus1.parquet
"""
import json
import os
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.targets import KerasCNNTarget
from src.attackers import KerasAttacker


def get_feature_columns(parquet_path: str, label_col: str = "Label") -> list:
    """L·∫•y danh s√°ch feature columns t·ª´ parquet file."""
    pq_file = pq.ParquetFile(parquet_path)
    return [name for name in pq_file.schema.names if name != label_col]


def load_test_data(parquet_path: str, feature_cols: list, max_samples: int = 10000):
    """Load d·ªØ li·ªáu test t·ª´ parquet file (b·ªè qua nh√£n -1)."""
    pq_file = pq.ParquetFile(parquet_path)
    all_X = []
    rows_loaded = 0
    
    print(f"üîÑ ƒêang load d·ªØ li·ªáu t·ª´ {parquet_path}...")
    
    for batch in pq_file.iter_batches(batch_size=5000, columns=feature_cols + ["Label"]):
        if rows_loaded >= max_samples:
            break
            
        batch_df = batch.to_pandas()
        
        # L·∫•y t·∫•t c·∫£ samples (kh√¥ng quan t√¢m nh√£n v√¨ s·∫Ω query target model)
        X = batch_df[feature_cols].values.astype(np.float32)
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        all_X.append(X)
        rows_loaded += len(X)
        
        if rows_loaded % 5000 == 0:
            print(f"  ƒê√£ load {rows_loaded:,}/{max_samples:,} samples...")
        
        if rows_loaded >= max_samples:
            break
    
    if all_X:
        X_concat = np.concatenate(all_X, axis=0)
        if len(X_concat) > max_samples:
            X_concat = X_concat[:max_samples]
        print(f"‚úÖ ƒê√£ load {len(X_concat):,} samples")
        return X_concat
    else:
        return np.empty((0, len(feature_cols)), dtype=np.float32)


def load_surrogate_model(model_path: str, scaler_path: str, feature_dim: int = 2381):
    """Load surrogate model v√† scaler."""
    import joblib
    import tensorflow as tf
    
    os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
    os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")
    
    # Load scaler
    scaler = joblib.load(scaler_path)
    
    # Load model v·ªõi compile=False ƒë·ªÉ tr√°nh l·ªói compatibility gi·ªØa c√°c version Keras
    try:
        model = tf.keras.models.load_model(model_path, compile=False)
    except Exception as e:
        # N·∫øu v·∫´n l·ªói, th·ª≠ load v·ªõi safe_mode=False
        try:
            model = tf.keras.models.load_model(model_path, compile=False, safe_mode=False)
        except Exception as e2:
            # N·∫øu v·∫´n l·ªói, th·ª≠ load weights th·ªß c√¥ng
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load model v·ªõi compile=False, th·ª≠ c√°ch kh√°c: {e2}")
            raise e2
    
    def predict(X):
        # Scale data
        X_scaled = scaler.transform(X)
        X_scaled = np.clip(X_scaled, -5, 5)
        # Predict
        probs = np.squeeze(model.predict(X_scaled, verbose=0), axis=-1)
        # N·∫øu model output l√† 2D (softmax), l·∫•y class 1
        if probs.ndim > 1 and probs.shape[-1] == 2:
            probs = probs[:, 1]
        return (probs >= 0.5).astype(int), probs
    
    return predict


def evaluate_model_similarity(
    target_model,
    surrogate_predict,
    X_test,
    y_target,
    model_name: str
):
    """ƒê√°nh gi√° ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa target v√† surrogate model."""
    print(f"\nüîÑ ƒêang ƒë√°nh gi√° {model_name}...")
    
    # Predict v·ªõi surrogate
    y_surrogate, probs_surrogate = surrogate_predict(X_test)
    
    # T√≠nh metrics
    accuracy = accuracy_score(y_target, y_surrogate)
    agreement = (y_target == y_surrogate).mean()
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_target, y_surrogate, average="binary", zero_division=0
    )
    
    try:
        auc = roc_auc_score(y_target, probs_surrogate)
    except ValueError:
        auc = float("nan")
    
    # Confusion matrix
    cm = confusion_matrix(y_target, y_surrogate)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    # Ph√¢n b·ªë predictions
    target_dist = dict(zip(*np.unique(y_target, return_counts=True)))
    surrogate_dist = dict(zip(*np.unique(y_surrogate, return_counts=True)))
    
    metrics = {
        "model_name": model_name,
        "accuracy": float(accuracy),
        "agreement": float(agreement),
        "auc": float(auc) if not np.isnan(auc) else None,
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": {
            "tn": int(tn),
            "fp": int(fp),
            "fn": int(fn),
            "tp": int(tp)
        },
        "target_distribution": {int(k): int(v) for k, v in target_dist.items()},
        "surrogate_distribution": {int(k): int(v) for k, v in surrogate_dist.items()},
    }
    
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Agreement: {agreement:.4f}")
    print(f"  AUC: {auc:.4f}" if not np.isnan(auc) else "  AUC: NaN")
    
    return metrics


def main():
    # ƒê∆∞·ªùng d·∫´n
    test_parquet = str(PROJECT_ROOT / "data" / "train_ember_2018_v2_features_label_minus1.parquet")
    target_model_path = str(PROJECT_ROOT / "src" / "final_model.h5")
    output_dir = PROJECT_ROOT / "logs" / "evaluation"
    
    # C√°c surrogate models
    surrogate_configs = [
        {
            "name": "surrogate_model.h5 (attack_run)",
            "model_path": PROJECT_ROOT / "output" / "attack_run" / "surrogate_model.h5",
            "scaler_path": PROJECT_ROOT / "output" / "attack_run" / "robust_scaler.joblib",
        },
        {
            "name": "surrogate_model.h5 (attack_run_5000)",
            "model_path": PROJECT_ROOT / "output" / "attack_run_5000" / "surrogate_model.h5",
            "scaler_path": PROJECT_ROOT / "output" / "attack_run_5000" / "robust_scaler.joblib",
        },
    ]
    
    print("=" * 80)
    print("ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS")
    print("=" * 80)
    
    # Load feature columns
    feature_cols = get_feature_columns(test_parquet)
    print(f"\nFeature columns: {len(feature_cols)}")
    
    # Load test data
    X_test = load_test_data(test_parquet, feature_cols, max_samples=10000)
    
    if len(X_test) == 0:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ test!")
        return
    
    # Load target model
    print(f"\nüîÑ ƒêang load target model...")
    os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
    os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")
    
    target_model = KerasCNNTarget(target_model_path, feature_dim=len(feature_cols))
    
    # Query target model ƒë·ªÉ l·∫•y nh√£n th·ª±c t·∫ø
    print(f"\nüîÑ ƒêang query target model ƒë·ªÉ l·∫•y nh√£n...")
    y_target = target_model(X_test)
    print(f"‚úÖ ƒê√£ l·∫•y nh√£n t·ª´ target model")
    print(f"  Ph√¢n b·ªë nh√£n: {dict(zip(*np.unique(y_target, return_counts=True)))}")
    
    # ƒê√°nh gi√° t·ª´ng surrogate model
    all_results = []
    
    for config in surrogate_configs:
        if not config["model_path"].exists() or not config["scaler_path"].exists():
            print(f"\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y model ho·∫∑c scaler cho {config['name']}")
            continue
        
        try:
            surrogate_predict = load_surrogate_model(
                str(config["model_path"]),
                str(config["scaler_path"]),
                feature_dim=len(feature_cols)
            )
            
            metrics = evaluate_model_similarity(
                target_model,
                surrogate_predict,
                X_test,
                y_target,
                config["name"]
            )
            
            all_results.append(metrics)
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi ƒë√°nh gi√° {config['name']}: {e}")
            import traceback
            traceback.print_exc()
    
    # T·∫°o report
    print(f"\n{'='*80}")
    print("üìä T·∫†O B√ÅO C√ÅO")
    print(f"{'='*80}\n")
    
    report_path = output_dir / "surrogate_similarity_report.txt"
    report_md_path = output_dir / "surrogate_similarity_report.md"
    json_path = output_dir / "surrogate_similarity_results.json"
    
    # Text report
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("B√ÅO C√ÅO ƒê√ÅNH GI√Å ƒê·ªò T∆Ø∆†NG ƒê·ªíNG GI·ªÆA TARGET V√Ä SURROGATE MODELS\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("TH√îNG TIN D·ªÆ LI·ªÜU TEST:\n")
        f.write("-" * 80 + "\n")
        f.write(f"File: {test_parquet}\n")
        f.write(f"S·ªë samples: {len(X_test):,}\n")
        f.write(f"Ph√¢n b·ªë nh√£n t·ª´ target model: {dict(zip(*np.unique(y_target, return_counts=True)))}\n\n")
        
        f.write("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:\n")
        f.write("-" * 80 + "\n\n")
        
        for result in all_results:
            f.write(f"{result['model_name'].upper().replace('_', ' ')}:\n")
            f.write(f"  Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
            f.write(f"  Agreement: {result['agreement']:.4f} ({result['agreement']*100:.2f}%)\n")
            if result['auc'] is not None:
                f.write(f"  AUC: {result['auc']:.4f}\n")
            else:
                f.write(f"  AUC: NaN\n")
            f.write(f"  Precision: {result['precision']:.4f}\n")
            f.write(f"  Recall: {result['recall']:.4f}\n")
            f.write(f"  F1-score: {result['f1_score']:.4f}\n")
            f.write(f"  Confusion Matrix:\n")
            f.write(f"    TN: {result['confusion_matrix']['tn']}, FP: {result['confusion_matrix']['fp']}\n")
            f.write(f"    FN: {result['confusion_matrix']['fn']}, TP: {result['confusion_matrix']['tp']}\n")
            f.write(f"  Target distribution: {result['target_distribution']}\n")
            f.write(f"  Surrogate distribution: {result['surrogate_distribution']}\n")
            f.write("\n")
        
        # So s√°nh
        f.write("\n" + "=" * 80 + "\n")
        f.write("SO S√ÅNH C√ÅC SURROGATE MODELS:\n")
        f.write("=" * 80 + "\n\n")
        
        if all_results:
            best_acc = max(all_results, key=lambda x: x['accuracy'])
            best_agreement = max(all_results, key=lambda x: x['agreement'])
            best_auc = max([r for r in all_results if r['auc'] is not None], key=lambda x: x['auc'], default=None)
            
            f.write(f"Best Accuracy: {best_acc['model_name']} ({best_acc['accuracy']:.4f})\n")
            f.write(f"Best Agreement: {best_agreement['model_name']} ({best_agreement['agreement']:.4f})\n")
            if best_auc:
                f.write(f"Best AUC: {best_auc['model_name']} ({best_auc['auc']:.4f})\n")
    
    # Markdown report
    with open(report_md_path, "w", encoding="utf-8") as f:
        f.write("# B√°o C√°o ƒê√°nh Gi√° ƒê·ªô T∆∞∆°ng ƒê·ªìng Gi·ªØa Target v√† Surrogate Models\n\n")
        
        f.write("## Th√¥ng Tin D·ªØ Li·ªáu Test\n\n")
        f.write(f"- **File**: `{test_parquet}`\n")
        f.write(f"- **S·ªë samples**: {len(X_test):,}\n")
        f.write(f"- **Ph√¢n b·ªë nh√£n t·ª´ target model**: {dict(zip(*np.unique(y_target, return_counts=True)))}\n\n")
        
        f.write("## K·∫øt Qu·∫£ ƒê√°nh Gi√°\n\n")
        f.write("| Model | Accuracy | Agreement | AUC | Precision | Recall | F1 |\n")
        f.write("|-------|----------|-----------|-----|-----------|--------|----|\n")
        
        for result in all_results:
            auc_str = f"{result['auc']:.4f}" if result['auc'] is not None else "N/A"
            f.write(f"| {result['model_name']} | {result['accuracy']:.4f} | "
                   f"{result['agreement']:.4f} | {auc_str} | {result['precision']:.4f} | "
                   f"{result['recall']:.4f} | {result['f1_score']:.4f} |\n")
        
        f.write("\n## Chi Ti·∫øt T·ª´ng Model\n\n")
        
        for result in all_results:
            f.write(f"### {result['model_name'].replace('_', ' ').title()}\n\n")
            f.write(f"- **Accuracy**: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
            f.write(f"- **Agreement**: {result['agreement']:.4f} ({result['agreement']*100:.2f}%)\n")
            if result['auc'] is not None:
                f.write(f"- **AUC**: {result['auc']:.4f}\n")
            else:
                f.write(f"- **AUC**: NaN\n")
            f.write(f"- **Precision**: {result['precision']:.4f}\n")
            f.write(f"- **Recall**: {result['recall']:.4f}\n")
            f.write(f"- **F1-score**: {result['f1_score']:.4f}\n\n")
            
            f.write("**Confusion Matrix:**\n\n")
            f.write(f"| | Predicted 0 | Predicted 1 |\n")
            f.write(f"|------|------------|-------------|\n")
            f.write(f"| Actual 0 | {result['confusion_matrix']['tn']} | {result['confusion_matrix']['fp']} |\n")
            f.write(f"| Actual 1 | {result['confusion_matrix']['fn']} | {result['confusion_matrix']['tp']} |\n\n")
            
            f.write(f"- **Target distribution**: {result['target_distribution']}\n")
            f.write(f"- **Surrogate distribution**: {result['surrogate_distribution']}\n\n")
    
    # JSON results
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "test_info": {
                "file": test_parquet,
                "num_samples": int(len(X_test)),
                "target_distribution": {int(k): int(v) for k, v in dict(zip(*np.unique(y_target, return_counts=True))).items()}
            },
            "results": all_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ ƒê√£ t·∫°o report:")
    print(f"   - Text report: {report_path}")
    print(f"   - Markdown report: {report_md_path}")
    print(f"   - JSON results: {json_path}")
    
    # In t√≥m t·∫Øt
    print(f"\n{'='*80}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢:")
    print(f"{'='*80}\n")
    
    if all_results:
        df = pd.DataFrame(all_results)
        print(df[["model_name", "accuracy", "agreement", "auc", "f1_score"]].to_string(index=False))
    
    return all_results


if __name__ == "__main__":
    main()

