"""
Flexible target model loader - T·ª± ƒë·ªông ph√°t hi·ªán v√† load ƒë√∫ng architecture

T√≠nh nƒÉng:
- T·ª± ƒë·ªông ph√°t hi·ªán architecture t·ª´ weights file
- X·ª≠ l√Ω input size mismatch b·∫±ng preprocessing layer
- H·ªó tr·ª£ nhi·ªÅu lo·∫°i model kh√°c nhau (DNN, CNN, LightGBM, etc.)
- Ph√π h·ª£p v·ªõi model extraction attack - ch·ªâ c·∫ßn query target model ƒë·ªÉ l·∫•y labels

L∆∞u √Ω:
- N·∫øu input size kh√¥ng kh·ªõp, preprocessing layer s·∫Ω ƒë∆∞·ª£c th√™m v√†o
- Preprocessing layer s·ª≠ d·ª•ng random projection (c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%)
- Trong model extraction attack, ƒëi·ªÅu n√†y v·∫´n ƒë·∫£m b·∫£o logic ƒë√∫ng v√¨ ta ch·ªâ c·∫ßn labels t·ª´ target model
"""
import os
import numpy as np
import h5py
from pathlib import Path
import lightgbm as lgb


class FlexibleKerasTarget:
    """
    Wrapper linh ho·∫°t ƒë·ªÉ load Keras model v·ªõi b·∫•t k·ª≥ architecture n√†o.
    T·ª± ƒë·ªông ph√°t hi·ªán v√† th·ª≠ c√°c c√°ch load kh√°c nhau.
    
    X·ª≠ l√Ω feature dimension mismatch: T·ª± ƒë·ªông c·∫Øt b·ªè ƒë·∫∑c tr∆∞ng th·ª´a n·∫øu input 
    c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n model y√™u c·∫ßu (Interface Compliance).
    """
    
    def __init__(self, weights_path, feature_dim=2381, threshold=0.5, name="flexible-keras-target"):
        self.model_endpoint = weights_path
        self.model_threshold = threshold
        self.name = name
        self.feature_dim = feature_dim  # Feature dim c·ªßa attacker dataset
        
        os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
        os.environ.setdefault("TF_USE_LEGACY_KERAS", "1")
        
        self._model = self._load_model_flexible()
        self._input_shape = self._detect_input_shape()
        # L·∫•y s·ªë ƒë·∫∑c tr∆∞ng y√™u c·∫ßu th·ª±c t·∫ø c·ªßa model
        self._required_feature_dim = self._get_actual_required_feature_dim()
    
    def _load_model_flexible(self):
        """
        Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ load model:
        1. Load nh∆∞ full model (n·∫øu c√≥ architecture trong file)
        2. Th·ª≠ c√°c architecture ph·ªï bi·∫øn
        3. Detect t·ª´ weights file
        """
        import tensorflow as tf
        
        # C√°ch 1: Th·ª≠ load nh∆∞ full model
        try:
            model = tf.keras.models.load_model(self.model_endpoint, compile=False)
            # Ki·ªÉm tra input shape c·ªßa model
            model_input_shape = model.input_shape[1:] if model.input_shape else None
            if model_input_shape and len(model_input_shape) > 0:
                model_input_size = model_input_shape[0] if isinstance(model_input_shape[0], int) else None
                # N·∫øu input size kh√¥ng kh·ªõp, c·∫ßn th√™m preprocessing layer
                if model_input_size and model_input_size != self.feature_dim:
                    print(f"‚ö†Ô∏è  Full model input size ({model_input_size}) != feature_dim ({self.feature_dim}), adding preprocessing layer")
                    model = self._add_preprocessing_layer(model, model_input_size)
            print(f"‚úÖ Loaded as full model with {len(model.layers)} layers")
            return model
        except Exception as e:
            print(f"‚ö†Ô∏è  Cannot load as full model: {type(e).__name__}")
        
        # C√°ch 2: Th·ª≠ load v·ªõi safe_mode=False
        try:
            model = tf.keras.models.load_model(
                self.model_endpoint, 
                compile=False, 
                safe_mode=False
            )
            # Ki·ªÉm tra input shape c·ªßa model
            model_input_shape = model.input_shape[1:] if model.input_shape else None
            if model_input_shape and len(model_input_shape) > 0:
                model_input_size = model_input_shape[0] if isinstance(model_input_shape[0], int) else None
                # N·∫øu input size kh√¥ng kh·ªõp, c·∫ßn th√™m preprocessing layer
                if model_input_size and model_input_size != self.feature_dim:
                    print(f"‚ö†Ô∏è  Model input size ({model_input_size}) != feature_dim ({self.feature_dim}), adding preprocessing layer")
                    model = self._add_preprocessing_layer(model, model_input_size)
            print(f"‚úÖ Loaded with safe_mode=False, {len(model.layers)} layers")
            return model
        except Exception as e:
            print(f"‚ö†Ô∏è  Cannot load with safe_mode=False: {type(e).__name__}")
        
        # C√°ch 3: Detect architecture t·ª´ weights file v√† build ƒë·ªông
        print("üîÑ Attempting to detect architecture from weights file...")
        architecture_info = self._detect_architecture_from_weights()
        
        # Th·ª≠ build model ƒë·ªông t·ª´ weights info
        try:
            model = self._build_model_from_weights(architecture_info)
            if model:
                # Load weights v·ªõi by_name=True v√† skip_mismatch=True
                # ƒêi·ªÅu n√†y cho ph√©p skip c√°c layers kh√¥ng c√≥ trong weights file (nh∆∞ preprocessing layer)
                try:
                    model.load_weights(self.model_endpoint, by_name=True, skip_mismatch=True)
                    print(f"‚úÖ Successfully built and loaded model from weights ({len(model.layers)} layers)")
                    # Ki·ªÉm tra xem c√≥ preprocessing layer kh√¥ng
                    if any('preprocessing' in layer.name for layer in model.layers):
                        print(f"   ‚ÑπÔ∏è  Model has preprocessing layer for input size adaptation")
                    return model
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error loading weights: {type(e).__name__}: {str(e)[:100]}")
                    # N·∫øu kh√¥ng load ƒë∆∞·ª£c, v·∫´n tr·∫£ v·ªÅ model (c√≥ th·ªÉ weights s·∫Ω ƒë∆∞·ª£c load sau)
                    return model
        except Exception as e:
            print(f"‚ö†Ô∏è  Cannot build from weights info: {type(e).__name__}: {str(e)[:100]}")
        
        # C√°ch 4: Th·ª≠ c√°c architecture ph·ªï bi·∫øn
        architectures = self._get_common_architectures()
        
        for arch_name, build_func in architectures.items():
            try:
                model = build_func()
                # Th·ª≠ load weights
                model.load_weights(self.model_endpoint)
                print(f"‚úÖ Successfully loaded with {arch_name} architecture ({len(model.layers)} layers)")
                return model
            except Exception as e:
                continue
        
        # N·∫øu t·∫•t c·∫£ ƒë·ªÅu fail, raise error
        raise ValueError(
            f"Cannot load model from {self.model_endpoint}. "
            f"Tried full model load and common architectures. "
            f"Please check the file or provide correct architecture."
        )
    
    def _detect_architecture_from_weights(self):
        """Ph√¢n t√≠ch weights file ƒë·ªÉ ƒëo√°n architecture"""
        info = {
            "has_conv": False,
            "has_dense": False,
            "layer_count": 0,
            "layer_names": [],
            "dense_layers": []  # List of (layer_name, output_size)
        }
        
        try:
            with h5py.File(self.model_endpoint, 'r') as f:
                if 'model_weights' in f:
                    weights_group = f['model_weights']
                    for layer_name in weights_group.keys():
                        if layer_name == 'top_level_model_weights':
                            continue
                        info["layer_names"].append(layer_name)
                        info["layer_count"] += 1
                        
                        if 'conv' in layer_name.lower():
                            info["has_conv"] = True
                        
                        if 'dense' in layer_name.lower():
                            info["has_dense"] = True
                            try:
                                layer = weights_group[layer_name]
                                # T√¨m kernel trong nested structure (c√≥ th·ªÉ l√† sequential/dense/kernel)
                                kernel = None
                                
                                # Th·ª≠ c√°c path kh√°c nhau
                                if 'kernel' in layer:
                                    kernel = layer['kernel']
                                elif 'sequential' in layer:
                                    seq = layer['sequential']
                                    # T√¨m dense layer trong sequential
                                    for seq_key in seq.keys():
                                        if 'dense' in seq_key.lower() and isinstance(seq[seq_key], h5py.Group):
                                            dense_in_seq = seq[seq_key]
                                            if 'kernel' in dense_in_seq:
                                                kernel = dense_in_seq['kernel']
                                                break
                                    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ tr·ª±c ti·∫øp
                                    if kernel is None and 'kernel' in seq:
                                        kernel = seq['kernel']
                                
                                if kernel is not None:
                                    kernel_shape = tuple(kernel.shape)
                                    # Kernel shape: (input_size, output_size) cho Dense layer
                                    if len(kernel_shape) == 2:
                                        output_size = kernel_shape[1]
                                    elif len(kernel_shape) == 1:
                                        output_size = kernel_shape[0]
                                    else:
                                        output_size = kernel_shape[-1]
                                    info["dense_layers"].append((layer_name, int(output_size)))
                            except Exception as e:
                                print(f"‚ö†Ô∏è  Error reading {layer_name}: {e}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error detecting architecture: {e}")
        
        return info
    
    def _build_model_from_weights(self, architecture_info):
        """Build model ƒë·ªông d·ª±a tr√™n th√¥ng tin t·ª´ weights file"""
        import tensorflow as tf
        from tensorflow.keras import Sequential
        from tensorflow.keras.layers import BatchNormalization, Dense, Dropout
        
        if not architecture_info["has_dense"] or not architecture_info["dense_layers"]:
            return None
        
        # S·∫Øp x·∫øp dense layers theo th·ª© t·ª±
        dense_layers = sorted(architecture_info["dense_layers"], 
                             key=lambda x: int(x[0].replace('dense', '').replace('_', '0') or '0'))
        
        if len(dense_layers) < 2:
            return None
        
        # Build model d·ª±a tr√™n dense layers t√¨m ƒë∆∞·ª£c
        layers = []
        is_first = True
        
        # L·∫•y input size th·ª±c t·∫ø c·ªßa model t·ª´ weights file
        actual_input_size = None
        if dense_layers:
            # ƒê·ªçc kernel shape c·ªßa layer ƒë·∫ßu ti√™n ƒë·ªÉ bi·∫øt input size th·ª±c t·∫ø
            try:
                with h5py.File(self.model_endpoint, 'r') as f:
                    first_layer_name = dense_layers[0][0]
                    layer = f['model_weights'][first_layer_name]
                    if 'sequential' in layer:
                        seq = layer['sequential']
                        for seq_key in seq.keys():
                            if 'dense' in seq_key.lower() and isinstance(seq[seq_key], h5py.Group):
                                dense_in_seq = seq[seq_key]
                                if 'kernel' in dense_in_seq:
                                    kernel_shape = dense_in_seq['kernel'].shape
                                    actual_input_size = kernel_shape[0]  # Input size l√† dimension ƒë·∫ßu ti√™n
                                    break
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading input size: {e}")
        
        # N·∫øu input size kh√¥ng kh·ªõp, c·∫ßn th√™m preprocessing layer
        needs_preprocessing = actual_input_size and actual_input_size != self.feature_dim
        
        if needs_preprocessing:
            # Th√™m preprocessing layer ƒë·ªÉ map t·ª´ feature_dim xu·ªëng actual_input_size
            # Layer n√†y KH√îNG c√≥ trong weights file, s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n
            # Nh∆∞ng trong model extraction attack, ta ch·ªâ c·∫ßn query target model,
            # kh√¥ng c·∫ßn train preprocessing layer n√†y
            layers.append(Dense(actual_input_size, activation='linear', 
                              input_shape=(self.feature_dim,), 
                              name='preprocessing_mapping',
                              trainable=False))  # Kh√¥ng train, ch·ªâ d√πng ƒë·ªÉ map input
        
        # Build c√°c layers t·ª´ weights file
        for i, (layer_name, output_size) in enumerate(dense_layers):
            if is_first:
                # Layer ƒë·∫ßu ti√™n: input size ph·∫£i match v·ªõi actual_input_size
                input_size_for_layer = actual_input_size if actual_input_size else self.feature_dim
                if needs_preprocessing:
                    # N·∫øu c√≥ preprocessing, layer ƒë·∫ßu ti√™n nh·∫≠n input t·ª´ preprocessing
                    layers.append(Dense(output_size, activation='relu', name=layer_name))
                else:
                    # Kh√¥ng c√≥ preprocessing, layer ƒë·∫ßu ti√™n nh·∫≠n input tr·ª±c ti·∫øp
                    layers.append(Dense(output_size, activation='relu', 
                                      input_shape=(self.feature_dim,), 
                                      name=layer_name))
                is_first = False
            else:
                # Layer cu·ªëi c√πng c√≥ th·ªÉ l√† output layer
                if i == len(dense_layers) - 1:
                    activation = 'sigmoid' if output_size == 1 else 'softmax'
                else:
                    activation = 'relu'
                layers.append(Dense(output_size, activation=activation, name=layer_name))
            
            # Th√™m BatchNormalization v√† Dropout sau m·ªói Dense (tr·ª´ layer cu·ªëi)
            if i < len(dense_layers) - 1:
                # T√™n BN v√† Dropout ph·∫£i match v·ªõi weights file
                if i == 0:
                    bn_name = 'batch_normalization'
                    dropout_name = 'dropout'
                else:
                    bn_name = f'batch_normalization_{i}'
                    dropout_name = f'dropout_{i}'
                layers.append(BatchNormalization(name=bn_name))
                layers.append(Dropout(0.3, name=dropout_name))
        
        model = Sequential(layers)
        
        # N·∫øu c√≥ preprocessing layer, kh·ªüi t·∫°o weights th√¥ng minh h∆°n
        if needs_preprocessing:
            preprocessing_layer = model.get_layer('preprocessing_mapping')
            import numpy as np
            
            # Kh·ªüi t·∫°o v·ªõi random projection (Gaussian random matrix)
            # ƒê√¢y l√† m·ªôt c√°ch ti·∫øp c·∫≠n h·ª£p l√Ω khi kh√¥ng bi·∫øt mapping ch√≠nh x√°c
            # Trong model extraction attack, ta ch·ªâ c·∫ßn query target model,
            # preprocessing layer n√†y s·∫Ω ƒë∆∞·ª£c "h·ªçc" ng·∫ßm th√¥ng qua queries
            weights = preprocessing_layer.get_weights()
            if len(weights) > 0:
                # S·ª≠ d·ª•ng random projection v·ªõi scaling ph√π h·ª£p
                # Random projection gi·ªØ ƒë∆∞·ª£c m·ªôt ph·∫ßn th√¥ng tin t·ª´ input
                kernel = np.random.randn(*weights[0].shape).astype(np.float32)
                # Scale ƒë·ªÉ output c√≥ variance t∆∞∆°ng ƒë∆∞∆°ng
                kernel = kernel / np.sqrt(weights[0].shape[0])
                bias = np.zeros(weights[1].shape, dtype=np.float32)
                preprocessing_layer.set_weights([kernel, bias])
                
                print(f"   ‚ÑπÔ∏è  Initialized preprocessing layer: {self.feature_dim} -> {actual_input_size}")
        
        return model
    
    def _get_common_architectures(self):
        """Tr·∫£ v·ªÅ dictionary c√°c h√†m build architecture ph·ªï bi·∫øn"""
        import tensorflow as tf
        from tensorflow.keras import Sequential
        from tensorflow.keras.layers import (
            BatchNormalization, Conv1D, Dense, Dropout, 
            Flatten, MaxPooling1D, LayerNormalization
        )
        from tensorflow.keras.regularizers import l2
        
        architectures = {}
        
        # Architecture 1: CNN (nh∆∞ trong final_model.ipynb)
        def build_cnn():
            return Sequential([
                Conv1D(64, 5, strides=2, padding='same', 
                      input_shape=(self.feature_dim, 1), activation='relu'),
                BatchNormalization(),
                MaxPooling1D(pool_size=2),
                Dropout(0.3),
                Conv1D(64, 3, padding='same', activation='relu'),
                BatchNormalization(),
                Conv1D(32, 3, padding='same', activation='relu'),
                BatchNormalization(),
                MaxPooling1D(pool_size=2),
                Dropout(0.4),
                Flatten(),
                Dense(256, activation='relu', kernel_regularizer=l2(0.01)),
                BatchNormalization(),
                Dropout(0.5),
                Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
                BatchNormalization(),
                Dropout(0.5),
                Dense(2, activation='softmax', dtype='float32'),
            ])
        architectures['CNN'] = build_cnn
        
        # Architecture 2: DNN (nh∆∞ create_dnn)
        def build_dnn():
            initializer = tf.keras.initializers.GlorotNormal(seed=42)
            inputs = tf.keras.Input(shape=(self.feature_dim,))
            x = Dense(2381, activation='elu', kernel_initializer=initializer)(inputs)
            x = LayerNormalization()(x)
            x = Dropout(0.3)(x)
            x = Dense(1024, activation='elu', kernel_initializer=initializer)(x)
            x = LayerNormalization()(x)
            x = Dropout(0.3)(x)
            x = Dense(512, activation='elu', kernel_initializer=initializer)(x)
            x = LayerNormalization()(x)
            x = Dropout(0.3)(x)
            x = Dense(128, activation='elu', kernel_initializer=initializer)(x)
            x = LayerNormalization()(x)
            x = Dropout(0.3)(x)
            outputs = Dense(1, activation="sigmoid")(x)
            return tf.keras.Model(inputs, outputs)
        architectures['DNN'] = build_dnn
        
        # Architecture 3: Simple DNN (nh∆∞ model trong file - 10 layers)
        # Pattern: Dense -> BN -> Dropout -> Dense -> BN -> Dropout -> Dense -> BN -> Dropout -> Dense
        def build_simple_dnn():
            return Sequential([
                Dense(2381, activation='elu', input_shape=(self.feature_dim,)),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1024, activation='elu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(512, activation='elu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1, activation='sigmoid'),
            ])
        architectures['Simple_DNN'] = build_simple_dnn
        
        # Architecture 4: DNN v·ªõi 4 Dense layers (d·ª±a tr√™n weights file structure)
        # Th·ª≠ v·ªõi c√°c k√≠ch th∆∞·ªõc kh√°c nhau
        def build_dnn_4layer_v1():
            return Sequential([
                Dense(2381, activation='relu', input_shape=(self.feature_dim,)),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1024, activation='relu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(512, activation='relu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1, activation='sigmoid'),
            ])
        architectures['DNN_4Layer_v1'] = build_dnn_4layer_v1
        
        def build_dnn_4layer_v2():
            # Th·ª≠ v·ªõi c√°c k√≠ch th∆∞·ªõc kh√°c
            return Sequential([
                Dense(2048, activation='relu', input_shape=(self.feature_dim,)),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1024, activation='relu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(512, activation='relu'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1, activation='sigmoid'),
            ])
        architectures['DNN_4Layer_v2'] = build_dnn_4layer_v2
        
        def build_dnn_4layer_v3():
            # Th·ª≠ v·ªõi activation kh√°c
            return Sequential([
                Dense(2381, activation='tanh', input_shape=(self.feature_dim,)),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1024, activation='tanh'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(512, activation='tanh'),
                BatchNormalization(),
                Dropout(0.3),
                Dense(1, activation='sigmoid'),
            ])
        architectures['DNN_4Layer_v3'] = build_dnn_4layer_v3
        
        return architectures
    
    def _add_preprocessing_layer(self, model, target_input_size):
        """Th√™m preprocessing layer v√†o model ƒë√£ load ƒë·ªÉ x·ª≠ l√Ω input size mismatch"""
        import tensorflow as tf
        from tensorflow.keras import Sequential, Model
        from tensorflow.keras.layers import Dense, Input
        
        # T·∫°o preprocessing layer
        preprocessing_input = Input(shape=(self.feature_dim,), name='preprocessing_input')
        preprocessing_layer = Dense(
            target_input_size, 
            activation='linear',
            name='preprocessing_mapping',
            trainable=False
        )(preprocessing_input)
        
        # K·∫øt n·ªëi v·ªõi model hi·ªán t·∫°i
        # L·∫•y output c·ªßa preprocessing layer l√†m input cho model g·ªëc
        model_output = model(preprocessing_layer)
        
        # T·∫°o model m·ªõi v·ªõi preprocessing layer
        new_model = Model(inputs=preprocessing_input, outputs=model_output, name='model_with_preprocessing')
        
        # Kh·ªüi t·∫°o preprocessing layer v·ªõi random projection
        preprocessing_layer_obj = new_model.get_layer('preprocessing_mapping')
        weights = preprocessing_layer_obj.get_weights()
        if len(weights) > 0:
            kernel = np.random.randn(*weights[0].shape).astype(np.float32)
            kernel = kernel / np.sqrt(weights[0].shape[0])
            bias = np.zeros(weights[1].shape, dtype=np.float32)
            preprocessing_layer_obj.set_weights([kernel, bias])
        
        return new_model
    
    def _detect_input_shape(self):
        """Ph√°t hi·ªán input shape t·ª´ model"""
        if hasattr(self._model, 'input_shape') and self._model.input_shape:
            return self._model.input_shape[1:]  # B·ªè qua batch dimension
        elif hasattr(self._model, 'inputs') and self._model.inputs:
            return tuple(self._model.inputs[0].shape[1:])
        else:
            # Default: gi·∫£ s·ª≠ l√† DNN (1D input)
            return (self.feature_dim,)
    
    def _has_preprocessing_layer(self):
        """Ki·ªÉm tra xem model c√≥ preprocessing layer kh√¥ng"""
        if hasattr(self._model, 'layers'):
            for layer in self._model.layers:
                if 'preprocessing' in layer.name.lower() or 'mapping' in layer.name.lower():
                    return True
        return False
    
    def _get_actual_required_feature_dim(self):
        """
        L·∫•y s·ªë ƒë·∫∑c tr∆∞ng th·ª±c t·∫ø m√† model y√™u c·∫ßu.
        
        N·∫øu model c√≥ preprocessing layer:
        - Model ƒë√£ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ nh·∫≠n input v·ªõi feature_dim c·ªßa attacker
        - Preprocessing layer s·∫Ω map t·ª´ feature_dim c·ªßa attacker sang feature_dim c·ªßa model th·ª±c t·∫ø
        - Trong tr∆∞·ªùng h·ª£p n√†y, kh√¥ng c·∫ßn c·∫Øt ƒë·∫∑c tr∆∞ng (preprocessing layer ƒë√£ x·ª≠ l√Ω)
        - Tr·∫£ v·ªÅ None ƒë·ªÉ b√°o hi·ªáu kh√¥ng c·∫ßn c·∫Øt ƒë·∫∑c tr∆∞ng
        
        N·∫øu kh√¥ng c√≥ preprocessing layer:
        - L·∫•y t·ª´ input shape c·ªßa model (ƒë√¢y l√† s·ªë ƒë·∫∑c tr∆∞ng model th·ª±c s·ª± y√™u c·∫ßu)
        - C·∫ßn c·∫Øt ƒë·∫∑c tr∆∞ng n·∫øu attacker c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n
        """
        # Ki·ªÉm tra xem c√≥ preprocessing layer kh√¥ng
        if self._has_preprocessing_layer():
            # N·∫øu c√≥ preprocessing layer, model ƒë√£ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ nh·∫≠n input v·ªõi feature_dim c·ªßa attacker
            # Kh√¥ng c·∫ßn c·∫Øt ƒë·∫∑c tr∆∞ng v√¨ preprocessing layer ƒë√£ x·ª≠ l√Ω vi·ªác mapping
            return None
        
        # N·∫øu kh√¥ng c√≥ preprocessing layer, l·∫•y t·ª´ input shape c·ªßa model
        input_shape = self._input_shape
        if len(input_shape) == 1:
            # DNN: (features,)
            return int(input_shape[0])
        elif len(input_shape) == 2 and input_shape[-1] == 1:
            # CNN: (features, 1)
            return int(input_shape[0])
        else:
            # Default: d√πng feature_dim ƒë∆∞·ª£c truy·ªÅn v√†o
            return self.feature_dim
    
    def get_required_feature_dim(self):
        """
        Tr·∫£ v·ªÅ s·ªë ƒë·∫∑c tr∆∞ng m√† target model y√™u c·∫ßu th·ª±c t·∫ø.
        """
        return self._required_feature_dim
    
    def _align_features(self, X):
        """
        ƒê·ªìng b·ªô h√≥a s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng c·ªßa input v·ªõi y√™u c·∫ßu c·ªßa target model.
        N·∫øu X c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n, c·∫Øt b·ªè c√°c ƒë·∫∑c tr∆∞ng th·ª´a ·ªü cu·ªëi.
        N·∫øu X c√≥ √≠t ƒë·∫∑c tr∆∞ng h∆°n, raise ValueError.
        
        Args:
            X: Input features array (n_samples, n_features)
            
        Returns:
            X_aligned: Input ƒë√£ ƒë∆∞·ª£c ƒë·ªìng b·ªô h√≥a
        """
        required_dim = self._required_feature_dim
        if required_dim is None:
            return X
        
        X = np.asarray(X)
        if X.ndim == 1:
            X = X.reshape(1, -1)
        
        actual_dim = X.shape[1]
        
        if actual_dim == required_dim:
            return X
        elif actual_dim > required_dim:
            # C·∫Øt b·ªè ƒë·∫∑c tr∆∞ng th·ª´a ·ªü cu·ªëi
            print(f"‚ö†Ô∏è  Input has {actual_dim} features, target model requires {required_dim}. "
                  f"Trimming {actual_dim - required_dim} features.")
            return X[:, :required_dim]
        else:
            # Kh√¥ng ƒë·ªß ƒë·∫∑c tr∆∞ng - raise error
            raise ValueError(
                f"Input has {actual_dim} features, but target model requires {required_dim}. "
                f"Cannot pad features - please provide correct feature set."
            )
    
    def _prepare_input(self, X):
        """Chu·∫©n b·ªã input ph√π h·ª£p v·ªõi model architecture"""
        # ƒê·ªìng b·ªô h√≥a s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng tr∆∞·ªõc khi chu·∫©n b·ªã input
        X = self._align_features(X)
        X = np.asarray(X, dtype=np.float32)
        if X.ndim == 1:
            X = X.reshape(1, -1)
        
        # N·∫øu model c·∫ßn 3D input (CNN), th√™m channel dimension
        if len(self._input_shape) == 2 and self._input_shape[-1] == 1:
            # CNN model: (features, 1)
            return np.expand_dims(X, axis=-1)
        else:
            # DNN model: (features,)
            return X
    
    def predict_proba(self, X, batch_size=512):
        """Predict probabilities"""
        X = self._prepare_input(X)
        probs = self._model.predict(X, batch_size=batch_size, verbose=0)
        
        # X·ª≠ l√Ω output shape kh√°c nhau
        if probs.ndim > 1:
            if probs.shape[-1] == 2:
                # Softmax output: l·∫•y class 1
                return probs[:, 1] if probs.ndim == 2 else probs[..., 1]
            elif probs.shape[-1] == 1:
                # Sigmoid output: squeeze
                return np.squeeze(probs, axis=-1)
        
        return np.squeeze(probs)
    
    def __call__(self, X, batch_size=512):
        """Predict binary labels"""
        probs = self.predict_proba(X, batch_size=batch_size)
        return (probs >= self.model_threshold).astype(int)


class FlexibleLGBTarget:
    """
    Wrapper linh ho·∫°t ƒë·ªÉ load LightGBM model (.lgb, .txt, .pkl, .d5) v·ªõi normalization stats.
    
    H·ªó tr·ª£:
    - Load model t·ª´ file .lgb (LightGBM native format)
    - Load normalization statistics t·ª´ file .npz
    - Normalize features tr∆∞·ªõc khi predict (gi·ªëng nh∆∞ code trong user's example)
    - X·ª≠ l√Ω feature alignment n·∫øu c·∫ßn
    
    X·ª≠ l√Ω feature dimension mismatch: T·ª± ƒë·ªông c·∫Øt b·ªè ƒë·∫∑c tr∆∞ng th·ª´a n·∫øu input 
    c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n model y√™u c·∫ßu (Interface Compliance).
    """
    
    def __init__(
        self, 
        model_path, 
        normalization_stats_path=None,
        threshold=0.5, 
        name="flexible-lgb-target",
        feature_dim=None
    ):
        """
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n t·ªõi file model .lgb, .txt, .pkl, ho·∫∑c .d5
            normalization_stats_path: ƒê∆∞·ªùng d·∫´n t·ªõi file .npz ch·ª©a normalization stats.
                                     N·∫øu None, s·∫Ω kh√¥ng normalize features.
            threshold: Threshold ƒë·ªÉ chuy·ªÉn probabilities th√†nh binary labels
            name: T√™n c·ªßa target model
            feature_dim: S·ªë ƒë·∫∑c tr∆∞ng c·ªßa attacker dataset. N·∫øu None, s·∫Ω l·∫•y t·ª´ model.
        """
        self.model_endpoint = model_path
        self.model_threshold = threshold
        self.name = name
        self.feature_dim = feature_dim
        
        # Load model
        self.model = self._load_model()
        
        # L·∫•y s·ªë ƒë·∫∑c tr∆∞ng y√™u c·∫ßu t·ª´ model
        self._required_feature_dim = self.model.num_feature()
        
        # N·∫øu feature_dim kh√¥ng ƒë∆∞·ª£c cung c·∫•p, d√πng t·ª´ model
        if self.feature_dim is None:
            self.feature_dim = self._required_feature_dim
        
        # Load normalization stats (n·∫øu c√≥)
        self.feature_means = None
        self.feature_stds = None
        self.feature_cols = None
        self.use_normalization = False
        
        if normalization_stats_path is not None:
            self._load_normalization_stats(normalization_stats_path)
    
    def _load_model(self):
        """Load LightGBM model t·ª´ file"""
        try:
            # C√°ch 1: Load t·ª´ file .lgb, .txt, ho·∫∑c .d5 (LightGBM native format)
            model = lgb.Booster(model_file=self.model_endpoint)
            print(f"‚úÖ Loaded LightGBM model from {self.model_endpoint}")
            print(f"   Model features: {model.num_feature()}")
            return model
        except Exception as e:
            # C√°ch 2: Th·ª≠ load t·ª´ pickle file
            try:
                import pickle
                with open(self.model_endpoint, 'rb') as f:
                    model = pickle.load(f)
                if isinstance(model, lgb.Booster):
                    print(f"‚úÖ Loaded LightGBM model from pickle file {self.model_endpoint}")
                    print(f"   Model features: {model.num_feature()}")
                    return model
                else:
                    raise ValueError(f"File {self.model_endpoint} kh√¥ng ph·∫£i LightGBM Booster")
            except Exception as e2:
                raise ValueError(
                    f"Cannot load LightGBM model from {self.model_endpoint}. "
                    f"Error: {type(e).__name__}: {str(e)}"
                )
    
    def _load_normalization_stats(self, stats_path):
        """Load normalization statistics t·ª´ file .npz"""
        try:
            stats = np.load(stats_path, allow_pickle=True)
            
            if 'feature_means' in stats:
                self.feature_means = stats['feature_means']
            else:
                raise ValueError(f"File {stats_path} kh√¥ng ch·ª©a 'feature_means'")
            
            if 'feature_stds' in stats:
                self.feature_stds = stats['feature_stds']
            else:
                raise ValueError(f"File {stats_path} kh√¥ng ch·ª©a 'feature_stds'")
            
            if 'feature_cols' in stats:
                self.feature_cols = stats['feature_cols'].tolist() if hasattr(stats['feature_cols'], 'tolist') else stats['feature_cols']
            else:
                self.feature_cols = None
            
            self.use_normalization = True
            print(f"‚úÖ Loaded normalization stats from {stats_path}")
            print(f"   Feature means shape: {self.feature_means.shape}")
            print(f"   Feature stds shape: {self.feature_stds.shape}")
            
            # Ki·ªÉm tra compatibility
            if self.feature_cols is not None:
                print(f"   Feature columns: {len(self.feature_cols)}")
                if len(self.feature_cols) != self._required_feature_dim:
                    print(f"   ‚ö†Ô∏è  Warning: feature_cols ({len(self.feature_cols)}) != model features ({self._required_feature_dim})")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Cannot load normalization stats from {stats_path}: {type(e).__name__}: {str(e)}")
            print(f"   Will use features without normalization")
            self.use_normalization = False
    
    def get_required_feature_dim(self):
        """Tr·∫£ v·ªÅ s·ªë ƒë·∫∑c tr∆∞ng m√† LightGBM model y√™u c·∫ßu"""
        return self._required_feature_dim
    
    def _align_features(self, X):
        """
        ƒê·ªìng b·ªô h√≥a s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng c·ªßa input v·ªõi y√™u c·∫ßu c·ªßa target model.
        N·∫øu X c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n, c·∫Øt b·ªè c√°c ƒë·∫∑c tr∆∞ng th·ª´a ·ªü cu·ªëi.
        N·∫øu X c√≥ √≠t ƒë·∫∑c tr∆∞ng h∆°n, raise ValueError.
        """
        X = np.asarray(X, dtype=np.float32)
        if X.ndim == 1:
            X = X.reshape(1, -1)
        
        actual_dim = X.shape[1]
        
        if actual_dim == self._required_feature_dim:
            return X
        elif actual_dim > self._required_feature_dim:
            # C·∫Øt b·ªè ƒë·∫∑c tr∆∞ng th·ª´a ·ªü cu·ªëi
            print(f"‚ö†Ô∏è  Input has {actual_dim} features, target model requires {self._required_feature_dim}. "
                  f"Trimming {actual_dim - self._required_feature_dim} features.")
            return X[:, :self._required_feature_dim]
        else:
            # Kh√¥ng ƒë·ªß ƒë·∫∑c tr∆∞ng - raise error
            raise ValueError(
                f"Input has {actual_dim} features, but target model requires {self._required_feature_dim}. "
                f"Cannot pad features - please provide correct feature set."
            )
    
    def _normalize_features(self, X):
        """
        Normalize features gi·ªëng nh∆∞ code c·ªßa ng∆∞·ªùi d√πng:
        - (features_array - feature_means) / feature_stds
        - X·ª≠ l√Ω NaN v√† infinity
        
        L∆∞u √Ω: N·∫øu normalization stats c√≥ nhi·ªÅu features h∆°n model y√™u c·∫ßu,
        ch·ªâ normalize s·ªë features m√† model c·∫ßn (t·ª´ ƒë·∫ßu).
        """
        if not self.use_normalization:
            return X
        
        # ƒê·∫£m b·∫£o X ƒë√£ align v·ªõi model requirements tr∆∞·ªõc (s·ªë features model c·∫ßn)
        # ƒêi·ªÅu n√†y quan tr·ªçng v√¨ model c√≥ th·ªÉ ch·ªâ c·∫ßn subset c·ªßa features
        X_aligned = self._align_features(X)  # C·∫Øt xu·ªëng s·ªë features model c·∫ßn
        
        # N·∫øu normalization stats c√≥ nhi·ªÅu features h∆°n model c·∫ßn,
        # ch·ªâ l·∫•y s·ªë features ƒë·∫ßu ti√™n t·ª´ stats t∆∞∆°ng ·ª©ng v·ªõi s·ªë features model c·∫ßn
        if self.feature_means.shape[0] > self._required_feature_dim:
            # Normalization stats c√≥ nhi·ªÅu features h∆°n model c·∫ßn
            # Ch·ªâ normalize v·ªõi stats c·ªßa s·ªë features ƒë·∫ßu ti√™n
            feature_means_used = self.feature_means[:self._required_feature_dim]
            feature_stds_used = self.feature_stds[:self._required_feature_dim]
        elif self.feature_means.shape[0] == self._required_feature_dim:
            # Normalization stats kh·ªõp v·ªõi s·ªë features model c·∫ßn
            feature_means_used = self.feature_means
            feature_stds_used = self.feature_stds
        else:
            # Normalization stats c√≥ √≠t features h∆°n model c·∫ßn - kh√¥ng n√™n x·∫£y ra
            raise ValueError(
                f"Normalization stats ch·ªâ c√≥ {self.feature_means.shape[0]} features, "
                f"nh∆∞ng model c·∫ßn {self._required_feature_dim} features. "
                f"Vui l√≤ng ki·ªÉm tra l·∫°i file normalization stats."
            )
        
        # Normalize v·ªõi stats ƒë√£ ƒë∆∞·ª£c ch·ªçn
        features_normalized = (X_aligned - feature_means_used) / feature_stds_used
        
        # X·ª≠ l√Ω NaN v√† infinity (gi·ªëng code c·ªßa ng∆∞·ªùi d√πng)
        features_normalized = np.nan_to_num(features_normalized, nan=0.0, posinf=0.0, neginf=0.0)
        
        return features_normalized
    
    def predict_proba(self, X):
        """
        Predict probabilities gi·ªëng nh∆∞ code c·ªßa ng∆∞·ªùi d√πng.
        
        Args:
            X: Input features (n_samples, n_features) ho·∫∑c dict v·ªõi feature names
            
        Returns:
            probabilities: Array of probabilities (n_samples,)
        """
        # N·∫øu X l√† dict (nh∆∞ code c·ªßa ng∆∞·ªùi d√πng), chuy·ªÉn th√†nh array
        if isinstance(X, dict):
            if self.feature_cols is None:
                raise ValueError("Cannot convert dict to array without feature_cols in normalization stats")
            
            # Chuy·ªÉn ƒë·ªïi features dict th√†nh array theo ƒë√∫ng th·ª© t·ª± feature_cols
            features_array = np.array(
                [X.get(col, 0.0) for col in self.feature_cols], 
                dtype=np.float32
            ).reshape(1, -1)
        else:
            features_array = np.asarray(X, dtype=np.float32)
            if features_array.ndim == 1:
                features_array = features_array.reshape(1, -1)
        
        # QUAN TR·ªåNG: X·ª≠ l√Ω normalization v√† alignment
        # Logic: Model c·∫ßn 108 features, nh∆∞ng normalization stats c√≥ th·ªÉ c√≥ 2381 features
        # Gi·∫£i ph√°p: Normalize v·ªõi s·ªë features model c·∫ßn (108 ƒë·∫ßu ti√™n t·ª´ stats)
        #            r·ªìi c·∫Øt b·ªè features th·ª´a t·ª´ input
        
        if self.use_normalization:
            # B∆∞·ªõc 1: Align input v·ªõi s·ªë features model c·∫ßn (c·∫Øt b·ªè features th·ª´a)
            # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o ta ch·ªâ normalize v·ªõi s·ªë features m√† model th·ª±c s·ª± c·∫ßn
            features_array_aligned = self._align_features(features_array)  # C·∫Øt xu·ªëng 108 features
            
            # B∆∞·ªõc 2: Normalize v·ªõi stats t∆∞∆°ng ·ª©ng
            # N·∫øu stats c√≥ nhi·ªÅu features h∆°n model c·∫ßn, ch·ªâ l·∫•y s·ªë features ƒë·∫ßu ti√™n
            if self.feature_means.shape[0] >= self._required_feature_dim:
                # Stats c√≥ ƒë·ªß ho·∫∑c nhi·ªÅu h∆°n - ch·ªâ l·∫•y s·ªë features model c·∫ßn
                feature_means_used = self.feature_means[:self._required_feature_dim]
                feature_stds_used = self.feature_stds[:self._required_feature_dim]
            else:
                # Stats c√≥ √≠t h∆°n model c·∫ßn - d√πng to√†n b·ªô stats
                feature_means_used = self.feature_means
                feature_stds_used = self.feature_stds
                # C·∫Øt features_array ƒë·ªÉ kh·ªõp v·ªõi stats
                if features_array_aligned.shape[1] > feature_means_used.shape[0]:
                    features_array_aligned = features_array_aligned[:, :feature_means_used.shape[0]]
            
            # Normalize
            features_normalized = (features_array_aligned - feature_means_used) / feature_stds_used
            features_normalized = np.nan_to_num(features_normalized, nan=0.0, posinf=0.0, neginf=0.0)
            
            features_array = features_normalized
        else:
            # Kh√¥ng c√≥ normalization - ch·ªâ align
            features_array = self._align_features(features_array)
        
        # Reshape cho LightGBM (c·∫ßn shape (n_samples, n_features))
        # LightGBM predict t·ª± ƒë·ªông x·ª≠ l√Ω (1, n_features) ho·∫∑c (n_samples, n_features)
        
        # Predict
        # X·ª≠ l√Ω num_iteration gi·ªëng code m·∫´u c·ªßa ng∆∞·ªùi d√πng:
        # - N·∫øu model c√≥ best_iteration v√† best_iteration > 0, d√πng best_iteration
        # - N·∫øu best_iteration = -1 ho·∫∑c kh√¥ng c√≥, d√πng None (t·∫•t c·∫£ trees)
        num_iteration = None
        if hasattr(self.model, 'best_iteration') and self.model.best_iteration is not None:
            if self.model.best_iteration > 0:
                num_iteration = self.model.best_iteration
            # N·∫øu best_iteration = -1, d√πng None (t·∫•t c·∫£ trees)
            # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi model kh√¥ng c√≥ best_iteration ƒë∆∞·ª£c l∆∞u
        
        prediction_prob = self.model.predict(features_array, num_iteration=num_iteration)
        
        # ƒê·∫£m b·∫£o output l√† 1D array
        if prediction_prob.ndim > 1:
            prediction_prob = np.squeeze(prediction_prob)
        
        return prediction_prob
    
    def __call__(self, X):
        """
        Predict binary labels.
        
        Args:
            X: Input features (n_samples, n_features) ho·∫∑c dict v·ªõi feature names
            
        Returns:
            labels: Binary labels (n_samples,)
        """
        probs = self.predict_proba(X)
        return (probs >= self.model_threshold).astype(int)

