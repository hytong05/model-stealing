import lightgbm as lgb
from ..models.dnn import create_dnn, create_dnn2, create_cnn
import tensorflow as tf
import joblib
import abc
import numpy as np

# This is optional and can also be called from the command line
try:
    from sklearnex import patch_sklearn

    patch_sklearn()
except ImportError:
    patch_sklearn = None
from sklearn import svm

# XGBoost and TabNet imports
try:
    import xgboost as xgb
except ImportError:
    xgb = None

try:
    from pytorch_tabnet.tab_model import TabNetClassifier
    import torch
except ImportError:
    TabNetClassifier = None
    torch = None

class AbstractAttacker(abc.ABC):
    @abc.abstractmethod
    def __init__(self):
        pass

    @abc.abstractmethod
    def __call__(self, X):
        raise NotImplementedError

    @abc.abstractmethod
    def train_model(self, X, y, X_val, y_val):
        raise NotImplementedError
    
    @abc.abstractmethod
    def save_model(self):
        raise NotImplementedError


class LGBAttacker(AbstractAttacker):
    def __init__(self, seed=42):
        # C·∫≠p nh·∫≠t hyperparameters ƒë·ªÉ kh·ªõp v·ªõi target model (LEE.lgb)
        # Target model s·ª≠ d·ª•ng: num_leaves=15, learning_rate=0.01, max_depth=7, 
        # lambda_l1=0.1, lambda_l2=0.1, min_data_in_leaf=30
        self.lgb_params = {
            "boosting_type" : "gbdt",
            "objective" : "binary",
            "learning_rate" : 0.01,  # Gi·∫£m t·ª´ 0.05 xu·ªëng 0.01 ƒë·ªÉ kh·ªõp target
            "num_leaves": 15,  # Gi·∫£m t·ª´ 2048 xu·ªëng 15 ƒë·ªÉ tr√°nh overfitting v√† kh·ªõp target
            "max_depth" : 7,  # Gi·∫£m t·ª´ 15 xu·ªëng 7 ƒë·ªÉ kh·ªõp target
            "min_data_in_leaf": 30,  # Kh·ªõp v·ªõi min_data_in_leaf c·ªßa target
            "lambda_l1": 0.1,  # Th√™m L1 regularization ƒë·ªÉ kh·ªõp target
            "lambda_l2": 0.1,  # Th√™m L2 regularization ƒë·ªÉ kh·ªõp target
            "feature_fraction": 0.8,  # Th√™m feature_fraction ƒë·ªÉ kh·ªõp target
            "bagging_fraction": 0.8,  # Th√™m bagging_fraction ƒë·ªÉ kh·ªõp target
            "bagging_freq": 5,  # Th√™m bagging_freq ƒë·ªÉ kh·ªõp target
            "force_row_wise": True,  # Th√™m force_row_wise ƒë·ªÉ kh·ªõp target
            "verbose": -1,
            "seed": seed
        }
        self.model = None

    def train_model(self, X, y, X_val, y_val, boosting_rounds=2000, early_stopping=100):
        # T√≠nh scale_pos_weight ƒë·ªÉ x·ª≠ l√Ω class imbalance
        train_label_counts = np.bincount(y)
        num_negative = train_label_counts[0] if len(train_label_counts) > 0 else 0
        num_positive = train_label_counts[1] if len(train_label_counts) > 1 else 0
        
        if num_positive > 0 and num_negative > 0:
            scale_pos_weight = num_negative / num_positive
            self.lgb_params['scale_pos_weight'] = scale_pos_weight
            print(f"   üìä Class distribution: {num_negative} negative, {num_positive} positive")
            print(f"   üìä scale_pos_weight = {scale_pos_weight:.4f}")
        
        train_data = lgb.Dataset(X, label=y)
        self.val_data = lgb.Dataset(X_val, y_val)
        # LightGBM m·ªõi d√πng callbacks cho early stopping v√† logging
        callbacks = [
            lgb.log_evaluation(period=0),  # period=0 ƒë·ªÉ kh√¥ng log
            lgb.early_stopping(stopping_rounds=early_stopping)  # Early stopping
        ]
        self.model = lgb.train(
            self.lgb_params, 
            train_data,
            num_boost_round=boosting_rounds,
            valid_sets=[self.val_data],
            callbacks=callbacks
        )                
    
    def __call__(self, X):
        return self.model.predict(X)

    def save_model(self, path):
        self.model.save_model(path+".txt")


class KerasAttacker(AbstractAttacker):
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381,)):

        self.model = create_dnn(seed=seed, input_shape=input_shape, mc=mc)
        self.checkpoint_filepath = '/tmp/checkpoint.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)

        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, X_val, y_val, num_epochs):
        
        self.model.fit(X, y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=(X_val, y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])   

        # Load the best weights after training
        self.model.load_weights(self.checkpoint_filepath)       
    
    def __call__(self, X):        
        return self.model.predict(X)

    def save_model(self, path):
        self.model.save(path+".h5")


class CNNAttacker(AbstractAttacker):
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381, 1)):
        """
        CNN Attacker s·ª≠ d·ª•ng architecture CNN ƒë∆°n gi·∫£n.
        
        Args:
            early_stopping: Patience cho early stopping
            seed: Random seed
            mc: Monte Carlo dropout flag
            input_shape: Input shape (features, channels) - default: (2381, 1) cho EMBER
        """
        self.input_shape = input_shape
        self.model = create_cnn(seed=seed, input_shape=input_shape, mc=mc)
        self.checkpoint_filepath = '/tmp/checkpoint_cnn.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)

        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, X_val, y_val, num_epochs):
        """
        Train CNN model. Input X s·∫Ω ƒë∆∞·ª£c reshape th√†nh (n_samples, n_features, 1) n·∫øu c·∫ßn.
        """
        # Reshape X n·∫øu c·∫ßn (n·∫øu X l√† 2D, c·∫ßn reshape th√†nh 3D cho Conv1D)
        if len(X.shape) == 2:
            # X c√≥ shape (n_samples, n_features) -> reshape th√†nh (n_samples, n_features, 1)
            X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))
        else:
            X_reshaped = X
            
        if len(X_val.shape) == 2:
            X_val_reshaped = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
        else:
            X_val_reshaped = X_val
        
        self.model.fit(X_reshaped, y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=(X_val_reshaped, y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])   

        # Load the best weights after training
        self.model.load_weights(self.checkpoint_filepath)       
    
    def __call__(self, X):
        """
        Predict v·ªõi CNN model. Input X s·∫Ω ƒë∆∞·ª£c reshape n·∫øu c·∫ßn.
        """
        # Reshape X n·∫øu c·∫ßn
        if len(X.shape) == 2:
            X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))
        else:
            X_reshaped = X
        return self.model.predict(X_reshaped)

    def save_model(self, path):
        self.model.save(path+".h5")

class KerasDualAttacker(AbstractAttacker):
    def __init__(self, early_stopping=30, seed=42, mc=False, input_shape=(2381,)):

        self.model = create_dnn2(seed=seed, mc=mc, input_shape=input_shape)

        # Keras y√™u c·∫ßu filepath ph·∫£i k·∫øt th√∫c b·∫±ng .weights.h5 khi save_weights_only=True
        self.checkpoint_filepath = '/tmp/checkpoint2.weights.h5'
        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            monitor='val_accuracy',
            mode='max',
            save_best_only=True)

        self.early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping)

    def train_model(self, X, y, y_true, X_val, y_val, y_val_true, num_epochs):
        
        self.model.fit((X, y_true), y,
            batch_size=128, 
            epochs=num_epochs, 
            validation_data=((X_val, y_val_true), y_val),
            callbacks=[self.model_checkpoint_callback, self.early_stopping])  
        
        self.model.load_weights(self.checkpoint_filepath)          
    
    def __call__(self, X, y_true):
        return self.model.predict((X, y_true))

    def save_model(self, path):
        self.model.save(path+".h5")


class SVMAttacker(AbstractAttacker):
    def __init__(self, seed=42, max_iter=1000):
        self.model = svm.SVC(C=10., 
                    kernel='linear',
                    max_iter=max_iter, 
                    random_state=seed, 
                    probability=True)

    def train_model(self, X, y):
        self.model.fit(X, y)
        print(f"SVM was fitted properly: {self.model.fit_status_}")

    def __call__(self, X):
        return self.model.predict_proba(X)[:, 1]

    def save_model(self, path):
        joblib.dump(self.model, path+".joblib")


class KNNAttacker(AbstractAttacker):
    def __init__(self, seed=42, n_neighbors=5, weights='uniform', metric='euclidean'):
        """
        KNN Attacker s·ª≠ d·ª•ng sklearn KNeighborsClassifier.
        
        Args:
            seed: Random seed (kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp trong KNN, nh∆∞ng gi·ªØ cho consistency)
            n_neighbors: S·ªë neighbors (default: 5)
            weights: C√°ch t√≠nh tr·ªçng s·ªë - 'uniform' ho·∫∑c 'distance' (default: 'uniform')
            metric: Metric ƒë·ªÉ t√≠nh kho·∫£ng c√°ch - 'euclidean', 'manhattan', etc. (default: 'euclidean')
        """
        from sklearn.neighbors import KNeighborsClassifier
        self.model = KNeighborsClassifier(
            n_neighbors=n_neighbors,
            weights=weights,
            metric=metric,
            n_jobs=-1  # S·ª≠ d·ª•ng t·∫•t c·∫£ CPU cores
        )
        self.seed = seed  # L∆∞u l·∫°i cho reference

    def train_model(self, X, y, X_val=None, y_val=None):
        """
        Train KNN model. KNN kh√¥ng c·∫ßn validation set cho training (kh√¥ng c√≥ training phase).
        Validation set c√≥ th·ªÉ ƒë∆∞·ª£c d√πng ƒë·ªÉ ch·ªçn hyperparameters, nh∆∞ng ·ªü ƒë√¢y ƒë∆°n gi·∫£n h√≥a.
        """
        self.model.fit(X, y)
        print(f"KNN was fitted with {len(X)} samples")

    def __call__(self, X):
        """
        Predict probabilities v·ªõi KNN model.
        """
        # KNN tr·∫£ v·ªÅ probabilities cho c·∫£ 2 classes, ch·ªâ c·∫ßn class 1 (malware)
        return self.model.predict_proba(X)[:, 1]

    def save_model(self, path):
        """
        Save KNN model b·∫±ng joblib.
        """
        joblib.dump(self.model, path+".pkl")


class XGBoostAttacker(AbstractAttacker):
    def __init__(self, seed=42):
        """
        XGBoost Attacker s·ª≠ d·ª•ng XGBoost library.
        
        Args:
            seed: Random seed
        """
        if xgb is None:
            raise ImportError("xgboost package is required. Install it with: pip install xgboost")
        
        self.xgb_params = {
            "objective": "binary:logistic",
            "eval_metric": "auc",
            "tree_method": "hist",  # GPU or CPU hist
            "max_depth": 8,
            "eta": 0.1,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "random_state": seed,
            "verbosity": 0
        }
        self.model = None
        self.seed = seed

    def train_model(self, X, y, X_val, y_val, boosting_rounds=200, early_stopping=20):
        """
        Train XGBoost model.
        
        Args:
            X: Training features
            y: Training labels
            X_val: Validation features
            y_val: Validation labels
            boosting_rounds: Maximum number of boosting rounds
            early_stopping: Early stopping rounds
        """
        # T√≠nh scale_pos_weight ƒë·ªÉ x·ª≠ l√Ω class imbalance
        train_label_counts = np.bincount(y.astype(int))
        num_negative = train_label_counts[0] if len(train_label_counts) > 0 else 0
        num_positive = train_label_counts[1] if len(train_label_counts) > 1 else 0
        
        if num_positive > 0 and num_negative > 0:
            scale_pos_weight = num_negative / num_positive
            self.xgb_params['scale_pos_weight'] = scale_pos_weight
            print(f"   üìä Class distribution: {num_negative} negative, {num_positive} positive")
            print(f"   üìä scale_pos_weight = {scale_pos_weight:.4f}")
        
        # T·∫°o DMatrix cho train v√† validation
        dtrain = xgb.DMatrix(X, label=y)
        dval = xgb.DMatrix(X_val, label=y_val)
        
        watchlist = [(dtrain, "train"), (dval, "valid")]
        
        self.model = xgb.train(
            params=self.xgb_params,
            dtrain=dtrain,
            num_boost_round=boosting_rounds,
            evals=watchlist,
            early_stopping_rounds=early_stopping,
            verbose_eval=False
        )
    
    def __call__(self, X):
        """
        Predict probabilities v·ªõi XGBoost model.
        """
        dtest = xgb.DMatrix(X)
        # XGBoost predict tr·∫£ v·ªÅ probability c·ªßa class 1 (malware)
        return self.model.predict(dtest)

    def save_model(self, path):
        """
        Save XGBoost model d∆∞·ªõi d·∫°ng .json (gi·ªëng target model).
        """
        self.model.save_model(path + ".json")


class TabNetAttacker(AbstractAttacker):
    def __init__(self, seed=42, device_name=None):
        """
        TabNet Attacker s·ª≠ d·ª•ng pytorch_tabnet library.
        
        Args:
            seed: Random seed
            device_name: Device name ('cuda' or 'cpu'), None ƒë·ªÉ auto-detect
        """
        if TabNetClassifier is None:
            raise ImportError("pytorch_tabnet package is required. Install it with: pip install pytorch-tabnet")
        
        if device_name is None:
            if torch is not None and torch.cuda.is_available():
                device_name = "cuda"
            else:
                device_name = "cpu"
        
        self.device_name = device_name
        self.seed = seed
        
        # TabNet hyperparameters t·ª´ notebook
        self.tabnet_params = {
            "n_d": 24,
            "n_a": 24,
            "n_steps": 3,
            "gamma": 1.5,
            "n_independent": 1,
            "n_shared": 1,
            "momentum": 0.02,
        }
        
        self.model = None

    def train_model(self, X, y, X_val, y_val, max_epochs=100, patience=10, batch_size=1024):
        """
        Train TabNet model.
        
        Args:
            X: Training features
            y: Training labels
            X_val: Validation features
            y_val: Validation labels
            max_epochs: Maximum number of epochs
            patience: Early stopping patience
            batch_size: Batch size for training
        """
        # Kh·ªüi t·∫°o TabNetClassifier
        self.model = TabNetClassifier(
            n_d=self.tabnet_params["n_d"],
            n_a=self.tabnet_params["n_a"],
            n_steps=self.tabnet_params["n_steps"],
            gamma=self.tabnet_params["gamma"],
            n_independent=self.tabnet_params["n_independent"],
            n_shared=self.tabnet_params["n_shared"],
            momentum=self.tabnet_params["momentum"],
            device_name=self.device_name,
        )
        
        # TabNet s·ª≠ d·ª•ng numpy arrays tr·ª±c ti·∫øp
        # Fit model v·ªõi early stopping.
        # L∆ØU √ù: V·ªõi m·ªôt s·ªë tr∆∞·ªùng h·ª£p c·ª±c l·ªách class (nh∆∞ SOMLAP d∆∞·ªõi oracle TABNET),
        # pytorch-tabnet + sklearn metrics (AUC/logloss) c√≥ th·ªÉ l·ªói n·∫øu y_val ch·ªâ c√≥ 1 class.
        # ƒê·ªÉ tr√°nh crash:
        #  - Th·ª≠ train v·ªõi eval_set + eval_metric="logloss"
        #  - N·∫øu l·ªói (single-class validation), fallback train KH√îNG eval_set/metric
        try:
            self.model.fit(
                X, y,
                eval_set=[(X_val, y_val)],
                eval_name=["valid"],
                eval_metric=["logloss"],
                max_epochs=max_epochs,
                patience=patience,
                batch_size=batch_size,
                virtual_batch_size=batch_size // 4,
                num_workers=0,
                drop_last=False,
            )
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: TabNet fit with validation metrics failed: {type(e).__name__}: {e}")
            print("   üí° Fallback: train TabNet WITHOUT eval_set / eval_metric (no early stopping).")
            self.model.fit(
                X, y,
                max_epochs=max_epochs,
                patience=patience,
                batch_size=batch_size,
                virtual_batch_size=batch_size // 4,
                num_workers=0,
                drop_last=False,
            )
    
    def __call__(self, X):
        """
        Predict probabilities v·ªõi TabNet model.
        """
        # TabNet predict_proba tr·∫£ v·ªÅ probabilities cho c·∫£ 2 classes
        # L·∫•y probability c·ªßa class 1 (malware)
        proba = self.model.predict_proba(X)
        if proba.ndim == 2 and proba.shape[1] == 2:
            return proba[:, 1]
        else:
            # N·∫øu ch·ªâ c√≥ 1 class, tr·∫£ v·ªÅ tr·ª±c ti·∫øp
            return proba.flatten()

    def save_model(self, path):
        """
        Save TabNet model b·∫±ng save_model() method (t·∫°o .zip file).
        """
        self.model.save_model(path)
